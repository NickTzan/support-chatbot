{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import SeleniumURLLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-70b-8192\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pdf files\n",
    "loader = DirectoryLoader(path='pdf', glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']\n",
    "\n",
    "# load the vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the provided context to answer the user's question.\n",
    "If you don't know the answer, respond with \"I do not know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604a6f7b20f1467388d7c88ad1137f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebe1dd4f84645e484e67e8257ece0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max retries exceeded for MultiContextEvolution(generator_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), docstore=InMemoryDocumentStore(splitter=<langchain_text_splitters.base.TokenTextSplitter object at 0x000001C4341095A0>, nodes=[Node(page_content='Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 0, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3f4a67f5-5bc7-4af1-8a9e-4e5e024876de', wins=19), Node(page_content='Estimates for the optimal action values can be learned\\nusing Q-learning (Watkins, 1989), a form of temporal dif-\\nference learning (Sutton, 1988). Most interesting problems\\nare too large to learn all action values in all states sepa-\\nrately. Instead, we can learn a parameterized value function\\nQ(s,a;θt). The standard Q-learning update for the param-\\neters after taking action Atin stateStand observing the\\nimmediate reward Rt+1and resulting state St+1is then\\nθt+1=θt+α(YQ\\nt−Q(St,At;θt))∇θtQ(St,At;θt).(1)\\nwhereαis a scalar step size and the target YQ\\ntis deﬁned as\\nYQ\\nt≡Rt+1+γmax\\naQ(St+1,a;θt). (2)\\nThis update resembles stochastic gradient descent, updating\\nthe current value Q(St,At;θt)towards a target value YQ\\nt.\\nDeep Q Networks\\nA deep Q network (DQN) is a multi-layered neural network\\nthat for a given state soutputs a vector of action values\\nQ(s,·;θ), where θare the parameters of the network. For\\nann-dimensional state space and an action space contain-\\ningmactions, the neural network is a function from Rnto\\nRm. Two important ingredients of the DQN algorithm as\\nproposed by Mnih et al. (2015) are the use of a target net-\\nwork, and the use of experience replay. The target network,\\nwith parameters θ−, is the same as the online network ex-\\ncept that its parameters are copied every τsteps from the\\nonline network, so that then θ−\\nt=θt, and kept ﬁxed on all\\nother steps. The target used by DQN is then\\nYDQN\\nt≡Rt+1+γmax\\naQ(St+1,a;θ−\\nt). (3)\\nFor the experience replay (Lin, 1992), observed transitions\\nare stored for some time and sampled uniformly from this\\nmemory bank to update the network. Both the target network\\nand the experience replay dramatically improve the perfor-\\nmance of the algorithm (Mnih et al., 2015).\\nDouble Q-learning\\nThe max operator in standard Q-learning and DQN, in (2)\\nand (3), uses the same values both to select and to evalu-\\nate an action. This makes it more likely to select overesti-\\nmated values, resulting in overoptimistic value estimates. To\\nprevent this, we can decouple the selection from the evalua-\\ntion. This is the idea behind Double Q-learning (van Hasselt,\\n2010).\\nIn the original Double Q-learning algorithm, two value\\nfunctions are learned by assigning each experience ran-\\ndomly to update one of the two value functions, such that\\nthere are two sets of weights, θandθ′. For each update, one\\nset of weights is used to determine the greedy policy and the\\nother to determine its value. For a clear comparison, we can\\nﬁrst untangle the selection and evaluation in Q-learning and\\nrewrite its target (2) as\\nYQ\\nt=Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θt).The Double Q-learning error can then be written as\\nYDoubleQ\\nt≡Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θ′\\nt).\\n(4)\\nNotice that the selection of the action, in the argmax , is\\nstill due to the online weights θt. This means that, as in Q-\\nlearning, we are still estimating the value of the greedy pol-\\nicy according to the current values, as deﬁned by θt. How-\\never, we use the second set of weights θ′\\ntto fairly evaluate\\nthe value of this policy. This second set of weights can be\\nupdated symmetrically by switching the roles of θandθ′.\\nOveroptimism due to estimation errors\\nQ-learning’s overestimations were ﬁrst investigated by\\nThrun and Schwartz (1993), who showed that if the action\\nvalues contain random errors uniformly distributed in an in', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 1, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='bec2012e-c935-4494-84e6-4c83a1304cb3', wins=17), Node(page_content=' this\\ncauses higher estimation errors for unseen states, resulting\\nin higher overestimations. This is important because ﬂexi-\\nble parametric function approximators are often employed\\nin reinforcement learning (see, e.g., Tesauro 1995; Sallans\\nand Hinton 2004; Riedmiller 2005; Mnih et al. 2015).\\nIn contrast to van Hasselt (2010) we did not use a sta-\\ntistical argument to ﬁnd overestimations, the process to ob-\\ntain Figure 2 is fully deterministic. In contrast to Thrun and\\nSchwartz (1993), we did not rely on inﬂexible function ap-\\nproximation with irreducible asymptotic errors; the bottom\\nrow shows that a function that is ﬂexible enough to cover all\\nsamples leads to high overestimations. This indicates that\\nthe overestimations can occur quite generally.\\nIn the examples above, overestimations occur even when\\nassuming we have samples of the true action value at cer-\\ntain states. The value estimates can further deteriorate if we\\nbootstrap off of action values that are already overoptimistic,\\nsince this causes overestimations to propagate throughout\\nour estimates. Although uniformly overestimating values\\nmight not hurt the resulting policy, in practice overestima-\\ntion errors will differ for different states and actions. Over-\\nestimation combined with bootstrapping then has the perni-\\ncious effect of propagating the wrong relative information\\nabout which states are more valuable than others, directly\\naffecting the quality of the learned policies.\\nThe overestimations should not be confused with opti-\\nmism in the face of uncertainty (Sutton, 1990; Agrawal,\\n1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and\\nTennenholtz, 2003; Szita and L ˝orincz, 2008; Strehl et al.,\\n2009), where an exploration bonus is given to states or\\nactions with uncertain values. Conversely, the overestima-\\ntions discussed here occur only after updating, resulting in\\noveroptimism in the face of apparent certainty. This was al-\\nready observed by Thrun and Schwartz (1993), who noted\\nthat, in contrast to optimism in the face of uncertainty, these\\noverestimations actually can impede learning an optimal\\npolicy. We will see this negative effect on policy quality con-\\nﬁrmed later in the experiments as well: when we reduce the\\noverestimations using Double Q-learning, the policies im-\\nprove.\\n2We arbitrarily used the samples of action ai+5(fori≤5)\\norai−5(fori > 5) as the second set of samples for the double\\nestimator of action ai.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 2, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='58b9d738-ba0c-403d-8f3a-2a5f4971ba3f', wins=14), Node(page_content='DQN Double DQN Double DQN (tuned)\\nMedian 47.5% 88.4% 116.7%\\nMean 122.0% 273.1% 475.2%\\nTable 2: Summary of normalized performance up to 30 minutes\\nof play on 49 games with human starts. Results for DQN are from\\nNair et al. (2015).\\nto allow for a controlled experiment focused just on re-\\nducing overestimations. The learned policies are evaluated\\nfor 5 mins of emulator time (18,000 frames) with an ϵ-\\ngreedy policy where ϵ= 0.05. The scores are averaged over\\n100 episodes. The only difference between Double DQN\\nand DQN is the target, using YDoubleDQN\\nt rather thanYDQN.\\nThis evaluation is somewhat adversarial, as the used hyper-\\nparameters were tuned for DQN but not for Double DQN.\\nTo obtain summary statistics across games, we normalize\\nthe score for each game as follows:\\nscore normalized =score agent−score random\\nscore human−score random. (5)\\nThe ‘random’ and ‘human’ scores are the same as used by\\nMnih et al. (2015), and are given in the appendix.\\nTable 1, under no ops , shows that on the whole Double\\nDQN clearly improves over DQN. A detailed comparison\\n(in appendix) shows that there are several games in which\\nDouble DQN greatly improves upon DQN. Noteworthy ex-\\namples include Road Runner (from 233% to 617%), Asterix\\n(from 70% to 180%), Zaxxon (from 54% to 111%), and\\nDouble Dunk (from 17% to 397%).\\nThe Gorila algorithm (Nair et al., 2015), which is a mas-\\nsively distributed version of DQN, is not included in the ta-\\nble because the architecture and infrastructure is sufﬁciently\\ndifferent to make a direct comparison unclear. For complete-\\nness, we note that Gorila obtained median and mean normal-\\nized scores of 96% and 495%, respectively.\\nRobustness to Human starts\\nOne concern with the previous evaluation is that in deter-\\nministic games with a unique starting point the learner could\\npotentially learn to remember sequences of actions with-\\nout much need to generalize. While successful, the solution\\nwould not be particularly robust. By testing the agents from\\nvarious starting points, we can test whether the found so-\\nlutions generalize well, and as such provide a challenging\\ntestbed for the learned polices (Nair et al., 2015).\\nWe obtained 100 starting points sampled for each game\\nfrom a human expert’s trajectory, as proposed by Nair et al.\\n(2015). We start an evaluation episode from each of these\\nstarting points and run the emulator for up to 108,000 frames\\n(30 mins at 60Hz including the trajectory before the starting\\npoint). Each agent is only evaluated on the rewards accumu-\\nlated after the starting point.\\nFor this evaluation we include a tuned version of Double\\nDQN. Some tuning is appropriate because the hyperparame-\\nters were tuned for DQN, which is a different algorithm. For\\nthe tuned version of Double DQN, we increased the num-\\nber of frames between each two copies of the target network\\nfrom 10,000 to 30,000, to reduce overestimations further be-\\ncause immediately after each switch DQN and Double DQN\\n0% 100% 200% 300% 400% 500%1000%1500%2000%2500%5000%7500%\\nNormalized score\\nHuman\\n∗∗Solaris∗∗Private EyeGravitarVentureMontezuma’s RevengeAsteroids∗∗Pitfall∗∗Ms. PacmanAmidar∗∗Yars Revenge∗∗AlienCentipedeBowling∗∗Skiing∗∗FrostbiteChopper CommandSeaquest∗∗Berzerk∗∗H.E.R.O.TutankhamIce HockeyBattle ZoneRiver Raid∗∗Surround∗∗Q*BertTennisFishing DerbyZaxxonPongFreewayBeam RiderBank HeistTime PilotName This GameWizard of WorKung-Fu MasterEnduroJames BondSpace InvadersUp and Down∗∗Phoenix∗∗∗∗Defender∗∗AsterixKangarooCrazy ClimberKrullRoad RunnerStar GunnerBoxingGopherRobotankDouble DunkAssaultBreakoutDemon AttackAtlantisVideo', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 5, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3a3d16f1-c204-4708-a36f-2299250ba53a', wins=17), Node(page_content='human, or even surpassing these.\\nDouble DQN appears more robust to this more challeng-\\ning evaluation, suggesting that appropriate generalizations\\noccur and that the found solutions do not exploit the deter-\\nminism of the environments. This is appealing, as it indi-\\ncates progress towards ﬁnding general solutions rather than\\na deterministic sequence of steps that would be less robust.\\nDiscussion\\nThis paper has ﬁve contributions. First, we have shown why\\nQ-learning can be overoptimistic in large-scale problems,\\neven if these are deterministic, due to the inherent estima-\\ntion errors of learning. Second, by analyzing the value es-\\ntimates on Atari games we have shown that these overesti-\\nmations are more common and severe in practice than pre-\\nviously acknowledged. Third, we have shown that Double\\nQ-learning can be used at scale to successfully reduce this\\noveroptimism, resulting in more stable and reliable learning.\\nFourth, we have proposed a speciﬁc implementation called\\nDouble DQN, that uses the existing architecture and deep\\nneural network of the DQN algorithm without requiring ad-\\nditional networks or parameters. Finally, we have shown that\\nDouble DQN ﬁnds better policies, obtaining new state-of-\\nthe-art results on the Atari 2600 domain.\\nAcknowledgments\\nWe would like to thank Tom Schaul, V olodymyr Mnih, Marc\\nBellemare, Thomas Degris, Georg Ostrovski, and Richard\\nSutton for helpful comments, and everyone at Google Deep-\\nMind for a constructive research environment.\\nReferences\\nR. Agrawal. Sample mean based index policies with O(log n) re-\\ngret for the multi-armed bandit problem. Advances in Applied\\nProbability , pages 1054–1078, 1995.\\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the\\nmultiarmed bandit problem. Machine learning , 47(2-3):235–\\n256, 2002.\\nL. Baird. Residual algorithms: Reinforcement learning with func-\\ntion approximation. In Machine Learning: Proceedings of the\\nTwelfth International Conference , pages 30–37, 1995.\\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The ar-\\ncade learning environment: An evaluation platform for general\\nagents. J. Artif. Intell. Res. (JAIR) , 47:253–279, 2013.\\nR. I. Brafman and M. Tennenholtz. R-max-a general polynomial\\ntime algorithm for near-optimal reinforcement learning. The\\nJournal of Machine Learning Research , 3:213–231, 2003.\\nK. Fukushima. Neocognitron: A hierarchical neural network ca-\\npable of visual pattern recognition. Neural networks , 1(2):119–\\n130, 1988.\\nL. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement\\nlearning: A survey. Journal of Artiﬁcial Intelligence Research ,\\n4:237–285, 1996.\\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based\\nlearning applied to document recognition. Proceedings of the\\nIEEE , 86(11):2278–2324, 1998.\\nL. Lin. Self-improving reactive agents based on reinforcement\\nlearning, planning and teaching. Machine learning , 8(3):293–\\n321, 1992.H. R. Maei. Gradient temporal-difference learning algorithms .\\nPhD thesis, University of Alberta, 2011.\\nV . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-\\nvski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,\\nD. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-\\nlevel control through deep reinforcement learning. Nature , 518\\n(7540):529–533, 2015.\\nA. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.\\nMaria, V . Panneershelvam, M. Suleyman, C. Beattie, S. Pe-', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 6, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='abb026b9-5154-4765-b3ba-0680e0f9af45', wins=20), Node(page_content='. Consider a state sin which all the true optimal action\\nvalues are equal at Q∗(s,a) =V∗(s). Suppose that the estimation\\nerrorsQt(s,a)−Q∗(s,a)are independently distributed uniformly\\nrandomly in [−1,1]. Then,\\nE[\\nmax\\naQt(s,a)−V∗(s)]\\n=m−1\\nm+ 1\\nProof. Deﬁneϵa=Qt(s,a)−Q∗(s,a); this is a uniform ran-\\ndom variable in [−1,1]. The probability that max aQt(s,a)≤x\\nfor somexis equal to the probability that ϵa≤xfor allasimul-\\ntaneously. Because the estimation errors are independent, we can\\nderive\\nP(max\\naϵa≤x) =P(X1≤x∧X2≤x∧...∧Xm≤x)\\n=m∏\\na=1P(ϵa≤x).\\nThe function P(ϵa≤x)is the cumulative distribution function\\n(CDF) ofϵa, which here is simply deﬁned as\\nP(ϵa≤x) =\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1\\n1+x\\n2ifx∈(−1,1)\\n1 ifx≥1\\nThis implies that\\nP(max\\naϵa≤x) =m∏\\na=1P(ϵa≤x)\\n=\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1(1+x\\n2)mifx∈(−1,1)\\n1 ifx≥1\\nThis gives us the CDF of the random variable max aϵa. Its expec-\\ntation can be written as an integral\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx,\\nwherefmaxis the probability density function of this variable, de-\\nﬁned as the derivative of the CDF: fmax(x) =d\\ndxP(max aϵa≤\\nx), so that for x∈[−1,1]we havefmax(x) =m\\n2(1+x\\n2)m−1.\\nEvaluating the integral yields\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx\\n=[(x+ 1\\n2)mmx−1\\nm+ 1]1\\n−1\\n=m−1\\nm+ 1.\\nExperimental Details for the Atari 2600\\nDomain\\nWe selected the 49 games to match the list used by Mnih et al.\\n(2015), see Tables below for the full list. Each agent step is com-\\nposed of four frames (the last selected action is repeated during\\nthese frames) and reward values (obtained from the Arcade Learn-\\ning Environment (Bellemare et al., 2013)) are clipped between -1\\nand 1.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 7, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='205d470b-7014-478d-8130-1ec0f0be88a6', wins=19), Node(page_content='Network Architecture\\nThe convolution network used in the experiment is exactly the one\\nproposed by proposed by Mnih et al. (2015), we only provide de-\\ntails here for completeness. The input to the network is a 84x84x4\\ntensor containing a rescaled, and gray-scale, version of the last four\\nframes. The ﬁrst convolution layer convolves the input with 32 ﬁl-\\nters of size 8 (stride 4), the second layer has 64 layers of size 4\\n(stride 2), the ﬁnal convolution layer has 64 ﬁlters of size 3 (stride\\n1). This is followed by a fully-connected hidden layer of 512 units.\\nAll these layers are separated by Rectiﬁer Linear Units (ReLu). Fi-\\nnally, a fully-connected linear layer projects to the output of the\\nnetwork, i.e., the Q-values. The optimization employed to train the\\nnetwork is RMSProp (with momentum parameter 0.95).\\nHyper-parameters\\nIn all experiments, the discount was set to γ= 0.99, and the learn-\\ning rate toα= 0.00025 . The number of steps between target net-\\nwork updates was τ= 10,000. Training is done over 50M steps\\n(i.e., 200M frames). The agent is evaluated every 1M steps, and\\nthe best policy across these evaluations is kept as the output of the\\nlearning process. The size of the experience replay memory is 1M\\ntuples. The memory gets sampled to update the network every 4\\nsteps with minibatches of size 32. The simple exploration policy\\nused is anϵ-greedy policy with the ϵdecreasing linearly from 1 to\\n0.1over 1M steps.\\nSupplementary Results in the Atari 2600\\nDomain\\nThe Tables below provide further detailed results for our experi-\\nments in the Atari domain.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 8, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='9d2b58d5-997a-4b85-81c9-50ffb321603e', wins=11)], node_embeddings_list=[[-0.04956690967082977, 0.05030025169253349, -0.03161194548010826, -0.03288557380437851, 0.01843535155057907, 0.0030159135349094868, -0.04557950794696808, 0.003113736631348729, -0.03918442875146866, 0.044702835381031036, -0.046046625822782516, -0.023291850462555885, -0.04208846762776375, 0.04081600159406662, 0.0008428394212387502, -0.01272555347532034, 0.03858095780014992, -0.031005043536424637, -0.0021355892531573772, -0.0263868048787117, -0.019500315189361572, -0.06270435452461243, -0.004712102934718132, 0.03614285588264465, -0.008492016233503819, -0.008154899813234806, -0.022909270599484444, -0.029226038604974747, 0.023909516632556915, -0.04845501855015755, 0.016177255660295486, -0.01821986399590969, 0.03537508100271225, 0.07697879523038864, 2.149998636014061e-06, -0.024586323648691177, 0.0026486176066100597, -0.030079323798418045, 0.027112800627946854, 0.04406825825572014, -0.007023315876722336, 0.024078145623207092, -0.03997643664479256, -0.0002518778492230922, -0.01077247317880392, -0.041674334555864334, 0.04684838652610779, 0.027832284569740295, 0.040335237979888916, 0.01932237483561039, 0.025564471259713173, -0.01299844030290842, 0.007462881039828062, -0.00503364996984601, 0.014621518552303314, -0.08512124419212341, 0.033202532678842545, 0.038979317992925644, 0.023208150640130043, -0.07128992676734924, -0.019696498289704323, -0.008343297056853771, 0.039767321199178696, -0.004311467986553907, -0.010576926171779633, 0.034841056913137436, 0.03286511078476906, 0.03844849765300751, -0.009584137238562107, 0.02926979959011078, -0.11251521110534668, -0.003184487111866474, 0.014170180074870586, -0.018793893977999687, -0.01477824430912733, -0.05464312806725502, -0.00925841648131609, 0.004600195679813623, -0.010893245227634907, 0.0027479573618620634, -0.012223675847053528, 0.0073741739615798, 0.03547237440943718, -0.019527563825249672, -0.02712169475853443, 0.036739230155944824, -0.039569608867168427, -0.03296245262026787, 0.026563992723822594, 0.047920167446136475, -0.02188410796225071, -0.023699888959527016, -0.0228212121874094, 0.008442508056759834, -0.08602551370859146, -0.0032441981602460146, 0.017753038555383682, 0.01990649476647377, -0.053375206887722015, -0.03753526508808136, 0.04794248938560486, 0.00438083428889513, 0.0416092611849308, 0.032957226037979126, -0.023404056206345558, 0.0643588975071907, -0.07362734526395798, 0.016156217083334923, -0.014290009625256062, 0.057421308010816574, -0.019295191392302513, 0.022706398740410805, -0.043158549815416336, 0.06357996165752411, -0.010380125604569912, -0.027038097381591797, -0.05441286787390709, -0.01633065938949585, -0.06266791373491287, 0.028577269986271858, -0.025486286729574203, 0.05074427276849747, -0.01839800365269184, 0.03272087499499321, -0.06857135891914368, 0.005552635062485933, -0.06000499427318573, 0.035123344510793686, 0.027371956035494804, 0.013303566724061966, 0.0016372428508475423, -0.02401612512767315, -0.03464997187256813, -0.011425425298511982, 0.01611497439444065, 0.1212221309542656, -0.016620660200715065, 0.0573403425514698, -0.008569801226258278, 0.024873845279216766, 0.02887134440243244, -0.005614537745714188, 0.03259080648422241, -0.04940129816532135, 0.0037878442090004683, -0.0533355176448822, 0.016369109973311424, 0.006827794015407562, -0.00017759716138243675, -0.001276011927984655, 0.024550406262278557, 0.07139012217521667, 0.02180689573287964, -0.018303725868463516, 0.03832290694117546, 0.005785456858575344, -0.08122086524963379, 0.06939934939146042, 0.0708138644695282, 0.017690518870949745, -0.011429138481616974, 0.016395462676882744, 0.0366867296397686, 0.02167961373925209, -0.0002495442458894104, -0.015720680356025696, 0.012048636563122272, -0.056377165019512177, 0.018716612830758095, 0.09623261541128159, 0.025793397799134254, 0.0020899081137031317, -0.07367810606956482, 0.020922085270285606, 0.0321451835334301, -0.009429696947336197, 0.035609904676675797, 0.04683776572346687, 0.01547489408403635, -0.0019847729708999395, 0.003428798634558916, -0.032974973320961, -0.03405817225575447, 0.05185338854789734, 0.057354845106601715, -0.005111761391162872, -0.03181115910410881, 0.018379351124167442, -0.025368699803948402, -0.08989953994750977, 0.07761101424694061, -0.03392950817942619, -0.0010409398237243295, -0.005600721575319767, 0.01664034090936184, 0.011814948171377182, -0.08045651763677597, -0.07757017761468887, 0.007214331068098545, 0.08210252225399017, 0.00714770145714283, -0.0576181598007679, 0.032748397439718246, -0.013818069361150265, 0.0137260053306818, -0.08512803912162781, -0.009280887432396412, -0.04084613546729088, -0.06721779704093933, -0.024022048339247704, 0.025394370779395103, -0.03132794797420502, 0.02795223332941532, -0.0012632743455469608, -0.03879129886627197, -0.03045129030942917, -0.010286913253366947, 0.0739719569683075, 0.008769582025706768, 0.018274089321494102, -0.03925643488764763, 0.06059976667165756, -0.012500639073550701, -0.010989345610141754, -0.02826921083033085, -0.08887050300836563, 0.06163368374109268, -0.0034426336642354727, 0.017172494903206825, -0.03198494762182236, -0.000173825173988007, -0.005194282624870539, -0.006883100140839815, 0.021072372794151306, -0.017542852088809013, -0.04622218757867813, -0.0786595270037651, -0.002477356232702732, -0.0058600823394954205, -0.03545485436916351, 0.040372252464294434, 0.0399077869951725, -0.004569454118609428, -0.0010490784188732505, 0.025372540578246117, -0.017541198059916496, -0.026828192174434662, 0.026768334209918976, -0.023631805554032326, -0.007192598190158606, -0.003604464465752244, -6.169293919811025e-05, 0.043534401804208755, 0.0029465872794389725, -0.028786279261112213, 0.012549839913845062, -0.03656994551420212, -0.0390135757625103, -0.07188323140144348, 0.011403464712202549, 0.0004116089839953929, -0.0030171708203852177, 0.004657383542507887, 0.04503307119011879, -0.008829002268612385, -0.0074689555913209915, 0.03309345990419388, 0.02473643608391285, 0.02424710802733898, -0.037099551409482956, -0.005151750985532999, -0.0011958505492657423, -0.023298479616642, 0.007246533874422312, -0.019917519763112068, -0.003190400777384639, -0.009359658695757389, 0.01866842620074749, -0.007855579257011414, 0.04516812786459923, 0.04992367699742317, 0.03316927328705788, -0.008716070093214512, -0.03759509697556496, -0.010453482158482075, 0.006495311390608549, -0.029378529638051987, -0.012247872538864613, -0.007959171198308468, -0.00512451259419322, -0.012904105708003044, 0.03132719546556473, -0.01787458173930645, -0.012055667117238045, 0.015984466299414635, -0.030895309522747993, -0.026987392455339432, 0.09747907519340515, -0.017459619790315628, 0.023969298228621483, 0.03587264195084572, 0.0352080762386322, -0.037214674055576324, 0.07119585573673248, -0.02100016176700592, -0.046305280178785324, -0.019978828728199005, 0.019801659509539604, 0.03852659463882446, 0.08900782465934753, -0.06891528517007828, 0.03329094126820564, -0.031006095930933952, 0.05408318340778351, 0.010144073516130447, -0.07400228083133698, -0.009552961215376854, 0.008767182938754559, -0.03414269909262657, -0.0047276816330850124, -0.04157065600156784, 0.0026749989483505487, 0.06187150999903679, 0.07103555649518967, 0.009358062408864498, 0.017061466351151466, 0.005598212592303753, 0.05317479372024536, 0.014188873581588268, 0.03242873400449753, 0.018814990296959877, -0.0616469606757164, -0.019207686185836792, -0.050389986485242844, -0.034628454595804214, -0.010077671147882938, -0.03193289041519165, 0.011392203159630299, -0.0039245192892849445, -0.02092493698000908, -0.005906561855226755, -0.05125012621283531, -0.012407233007252216, 0.03443283215165138, -0.017866019159555435, 0.01250829454511404, -0.027765337377786636, -0.009294365532696247, -0.024218088015913963, -0.052021682262420654, 0.04773879051208496, 0.022135594859719276, -0.02410902827978134, -0.017023401334881783, -0.026226451620459557, 0.011308490298688412, 0.020139852538704872, -0.006132879760116339, 0.03974349796772003, -0.06352242082357407, 0.07311461120843887, -0.01773030310869217, 0.03676407039165497, 0.006706724874675274, -0.00113665999379009, 0.014348911121487617, -0.026536082848906517, -0.0308107640594244, -0.034079134464263916, 0.052283644676208496, 0.04096300154924393, -0.03515404462814331, -0.04475938901305199, -0.01596837118268013, -0.020565446466207504, -0.01847333461046219, 0.019493091851472855, 0.008841834962368011, 0.02457306534051895, -0.01257703360170126, 0.0632074698805809, -0.05528101325035095, -0.020462218672037125, -0.00772258872166276, 0.014005635865032673, -5.446333307190798e-05, -0.024307800456881523, 0.013770432211458683, -0.07240741699934006, 0.0011185075854882598, 0.0038715829141438007, -0.04053213447332382, -0.05016007646918297, 0.03667386621236801, -0.04187147319316864, -0.04028069227933884, 0.014091952703893185, 0.030159082263708115, 0.018237927928566933, -0.025925692170858383, 0.04533470794558525, 0.01699031889438629, 0.05208983272314072, -0.012026390992105007, -0.06956001371145248, 0.011980989016592503, 0.05501585826277733, 0.017857961356639862, -0.03165769577026367, -0.005820938386023045, -0.042281366884708405, 0.02048397809267044, 0.006348191760480404, 0.07693615555763245, 0.01606963574886322, -0.011864441446959972, 0.008255308493971825, 0.026007264852523804, 0.010685653425753117, 0.04990028217434883, 0.02351139485836029, 0.031110204756259918, 0.04332878813147545, -0.012153967283666134, -0.03794131428003311, 0.025629857555031776, -0.018776461482048035, 0.012083655223250389, -0.0005816958728246391, -0.025191569700837135, 0.0008335255552083254, -0.07716438919305801, -0.02297556772828102, -0.02814149670302868, -0.088962122797966, 1.7185151591547765e-05, -0.020434973761439323, 0.016032952815294266, 0.005323451012372971, -0.03690676391124725, 0.0011500546243041754, -0.036443114280700684, 0.018618030473589897, -0.0176237840205431, -0.013270661234855652, -0.006224134471267462, 0.06981797516345978, -0.0005877199582755566, -0.048915762454271317, -0.002769813872873783, 0.029081476852297783, -0.008111154660582542, 0.05358076095581055, -0.006325608119368553, -0.049744926393032074, -0.0161903016269207, -0.02019103616476059, -0.004875012673437595, 0.009113934822380543, 0.023439906537532806, 0.07227034121751785, 0.008071894757449627, 0.036758240312337875, -0.005599034950137138, -0.013699065893888474, 0.041046466678380966, 0.05880441516637802, 0.022640852257609367, -0.032338712364435196, 0.07543806731700897, 0.0014054124476388097, 0.00021080787701066583, 0.028200365602970123, -0.06495729833841324, -0.0009477616986259818, -0.007826036773622036, 0.026593919843435287, -0.04710095375776291, 0.030852418392896652, -0.018303919583559036, -0.05774788185954094, 0.04364979639649391, 0.013156036846339703, -0.018749352544546127, 0.0034918866585940123, 0.010379191488027573, 0.02955382876098156, 0.010919143445789814, -0.04005775600671768, 0.0030696613248437643, 0.04186369106173515, 0.02955406717956066, 0.0031217450741678476, -0.004175861831754446, 0.10254082828760147, -0.03302936255931854, -0.013329079374670982, 0.026858331635594368, 0.05360277369618416, 0.0335363894701004, -0.006293140351772308, -0.0051373643800616264, 0.028665807098150253, -0.02126343548297882, 0.03197021037340164, 0.04389636591076851, 0.03102790378034115, -0.015325527638196945, 0.0203153807669878, -0.01759224198758602, -0.014048167504370213, -0.01878613419830799, -0.054337628185749054, -0.08147112280130386, 0.006435368675738573, 0.030237050727009773, 0.09055858850479126, 0.0062395185232162476, -0.04611249268054962, -0.017497709020972252, 0.002797875553369522, -0.0017475589411333203, -0.054203033447265625, 0.008608764968812466, -0.0035609116312116385, 0.030582701787352562, 0.028722237795591354, -0.00027848099125549197, -0.01907588727772236, 0.01941657066345215, 0.020659247413277626, 0.0591428242623806, -0.038499992340803146, 0.018652664497494698, -0.006281610112637281, -0.04479268938302994, 0.06129003316164017, 0.04852548614144325, -0.027460653334856033, -0.05722035467624664, 0.020719166845083237, -0.004362539388239384, 0.0004621725529432297, 0.07286541163921356, 0.02440125308930874, -0.05857822299003601, 0.02733508124947548, -0.040851153433322906, 0.001233183196745813, -0.010925082489848137, -0.044086117297410965, -0.019097426906228065, -0.028419246897101402, 0.0009672532323747873, 0.018251528963446617, -5.2393660333690824e-33, 0.005733703728765249, 0.0327339842915535, 0.013665182515978813, -0.013834282755851746, -0.03953896835446358, 0.012471248395740986, -0.008237186819314957, -0.06624552607536316, 0.012249778024852276, 0.01691672019660473, -0.022888822481036186, 0.006525000091642141, 0.005824986379593611, -0.01810799352824688, 0.009818199090659618, -0.0665813460946083, 0.03861081972718239, -0.03042105957865715, -0.037292104214429855, 0.04174313321709633, -0.009261527098715305, 0.03208363801240921, 0.04066682606935501, 0.014413449913263321, -0.030452514067292213, 0.021000316366553307, -0.030024368315935135, 0.01764131709933281, -0.015565648674964905, 0.035078324377536774, -0.013481101021170616, -0.018075278028845787, -0.045937031507492065, -0.08943227678537369, 0.008996015414595604, -0.00177675555460155, -0.053804680705070496, -0.008313515223562717, -0.05668061971664429, 0.018642116338014603, -0.008951383642852306, -0.0412890799343586, 0.03725530207157135, 0.053809188306331635, -0.08073937147855759, 0.005439059343189001, 0.004016018472611904, 0.008666821755468845, -0.0027602296322584152, -0.08467312902212143, -0.007249248679727316, -0.001326903817243874, 0.016480321064591408, -0.02427643910050392, 0.0422847680747509, 0.06903411448001862, -0.017198549583554268, 0.007142256945371628, -0.032724928110837936, -0.01114514097571373, 0.007348882034420967, 0.007590506225824356, -0.0009148311801254749, 0.08791342377662659, 0.08708048611879349, 0.03475499898195267, 0.04031061381101608, -0.0031139147467911243, -0.04015132784843445, 0.009228046983480453, -0.007614428643137217, 0.012225717306137085, 0.030473385006189346, -0.01602003164589405, 0.08681568503379822, 0.004674796015024185, -0.036929965019226074, 0.035452283918857574, 0.013656810857355595, 0.07004045695066452, 0.014242538250982761, -0.0032166766468435526, -0.018858937546610832, 0.0038062664680182934, -0.009649226441979408, -0.015989605337381363, 0.015520875342190266, -0.01825098507106304, -0.009923762641847134, 0.006572339218109846, 0.03542119637131691, 0.019002918154001236, -0.000930476700887084, -0.03828636556863785, 0.05465613678097725, -0.01938931830227375, 0.0027082867454737425, 0.03808492049574852, -0.014031925238668919, 0.005083836615085602, -0.034158360213041306, -0.06560160964727402, -0.015721118077635765, -0.02117462269961834, -0.005181961227208376, 0.02705833502113819, -0.02057328261435032, 0.011634482070803642, -0.0025265796575695276, 0.01984385959804058, -0.008347034454345703, 0.020092995837330818, 0.07017629593610764, -0.02701658196747303, 0.044486694037914276, -0.04864874854683876, 0.016991257667541504, 0.007083837408572435, -0.00965020339936018, 0.024995123967528343, 0.00997256487607956, -0.018661057576537132, -0.039940718561410904, 0.034596625715494156, -0.013127707876265049, -0.027812914922833443, -0.1137981191277504, 0.03329765051603317, -0.0954248309135437, -0.08871246874332428, -0.028346283361315727, 0.002901082392781973, 2.830523726515821e-07, -0.06526628881692886, 0.02176315151154995, 0.030600890517234802, 0.010186759755015373, 0.006753704976290464, 0.03764456883072853, -0.01513525377959013, 0.015110800042748451, -0.014872671104967594, 0.02616477757692337, 0.0769086629152298, 0.042842019349336624, 0.004197615198791027, -0.005126357544213533, -0.05055604875087738, -0.032553959637880325, -0.05483115836977959, 0.05333319306373596, 0.0027884861920028925, -0.023650875315070152, -0.029190247878432274, 0.04392814636230469, 0.0030022342689335346, -0.017225341871380806, 0.07329633831977844, -0.00618797168135643, 0.02890566550195217, -0.01015797071158886, 0.04330960661172867, 0.005862101446837187, 0.016835445538163185, 0.018966244533658028, -0.03767523169517517, -0.05417787656188011, -0.020708922296762466, 0.042562227696180344, -0.006089566275477409, 0.04329114034771919, 0.006681707687675953, -0.032713744789361954, -0.04041823372244835, 0.03448858857154846, 0.07394971698522568, -0.012641963548958302, 0.04563938453793526, -0.056690141558647156, 0.02270055189728737, -0.006796339526772499, -0.04671710729598999, -0.006431576795876026, 0.02362627349793911, 0.0031040781177580357, -0.01609502173960209, -0.05438423529267311, -0.023288927972316742, 0.028326116502285004, 0.03590812906622887, -0.03162478283047676, 0.015953650698065758, 0.08908253163099289, 0.028270892798900604, -0.029228312894701958, -0.044985976070165634, 0.022145522758364677, 0.02303995192050934, -0.05817921459674835, -0.024420034140348434, 1.8514450433127441e-34, 0.01697324961423874, -0.06399133801460266, 0.009780898690223694, -0.038277916610240936, -0.012614056468009949, 0.004733836278319359, 0.03986188396811485, 0.010966276749968529, 0.0971110463142395, 0.037209633737802505, -0.03184156119823456], [-0.04602940008044243, 0.019681455567479134, -0.01960614137351513, -0.041058249771595, -0.024524137377738953, -0.009800859726965427, -0.053674984723329544, -0.001083041075617075, -0.038948751986026764, 0.036183033138513565, -0.02878202497959137, -0.003416056977584958, -0.02670563943684101, 0.05225274711847305, 0.016610287129878998, -0.04243627190589905, 0.04263217747211456, 0.001855393871665001, -0.043818097561597824, -0.0031126905232667923, -0.014428502880036831, -0.0765562579035759, -0.0329836830496788, 0.03188700973987579, 0.009657010436058044, -0.029219139367341995, -0.07348824292421341, -0.040029339492321014, 0.05639645829796791, -0.0542156845331192, 0.021953236311674118, -0.03277580440044403, 0.025720426812767982, 0.062254466116428375, 1.995331558646285e-06, -0.0006518788286484778, 0.00286106183193624, -0.06626146286725998, -0.0029985804576426744, -0.023997310549020767, -0.035406019538640976, 0.04237125441431999, -0.03611861914396286, 0.024609211832284927, -0.027215858921408653, -0.007935874164104462, 0.05315399542450905, 0.021626750007271767, 0.04986194893717766, 0.06225023418664932, 0.01645629294216633, 0.010575380176305771, 0.004307347349822521, -0.027056006714701653, 0.014077995903789997, -0.052946221083402634, 0.0444154366850853, 0.008943727239966393, 0.019241059198975563, -0.03595568612217903, -0.012328031472861767, -0.02452894113957882, 0.03279603645205498, -0.014652584679424763, 0.0004571707977447659, 0.0383283905684948, 0.08476120978593826, 0.038770660758018494, -0.0005641364841721952, 0.012863718904554844, -0.07837529480457306, 0.012828600592911243, 0.017830919474363327, -0.015522832982242107, -0.03162357956171036, -0.036969754844903946, 0.01997237466275692, -0.005835445132106543, 0.0002132725203409791, 0.00550698721781373, -0.004788583610206842, 0.024582456797361374, -0.00012291091843508184, 0.020692814141511917, -4.3039166484959424e-05, 0.02308695763349533, -0.0320698618888855, -0.022110287100076675, 0.009809519164264202, -0.015153222717344761, -0.0047020879574120045, -0.025256728753447533, -0.0022235240321606398, 0.03901924565434456, -0.04081115126609802, -0.008353509940207005, 0.03582862764596939, -0.005574747454375029, -0.059680767357349396, -0.022990845143795013, 0.05063652619719505, 0.021203162148594856, 0.035883959382772446, 0.01282994169741869, -0.015058581717312336, 0.07177269458770752, -0.04124896228313446, -0.005817510187625885, -0.0062251016497612, 0.09383376687765121, -0.08054083585739136, 0.015647802501916885, -0.04257864132523537, 0.0686333104968071, 0.022665422409772873, -0.003193650860339403, -0.07468482851982117, 0.02502225525677204, -0.07945992797613144, 0.017783762887120247, -0.03340540826320648, 0.011141810566186905, -0.04612677916884422, 0.0201744195073843, -0.0852399691939354, -0.010669015347957611, -0.03079591877758503, 0.028458978980779648, -0.000528671604115516, 0.0388391837477684, -0.0009701951057650149, -0.015328388661146164, 0.023009728640317917, -0.01386967021971941, -0.008696314878761768, 0.09185249358415604, 0.0091159762814641, 0.021785562857985497, -0.027549097314476967, 0.008595976047217846, 0.03823503106832504, 0.017480013892054558, 0.04315800964832306, -0.023751812055706978, -0.010298914276063442, -0.06416524946689606, -0.014956229366362095, -0.0014385979156941175, -0.00721023790538311, -0.01303795538842678, 0.011816177517175674, 0.04091491177678108, 0.08507107198238373, -0.016331162303686142, 0.05409306287765503, -0.03148699924349785, -0.028595605865120888, 0.0665159523487091, 0.022775964811444283, 0.05177345126867294, -0.026938829571008682, 0.02166542410850525, 0.057304296642541885, 0.027249839156866074, 0.002089919988065958, 0.041894931346178055, -0.027614543214440346, -0.07695085555315018, 0.028845172375440598, 0.07652207463979721, 0.014765172265470028, -0.012056687846779823, -0.10233362019062042, 0.023894578218460083, 0.04293539747595787, 0.02459724433720112, 0.03871751204133034, -0.009708656929433346, 0.014595244079828262, -0.008089609444141388, 0.0040893093682825565, -0.03657882660627365, -0.024931160733103752, 0.08238127827644348, 0.04457470029592514, -0.010702704079449177, -0.06502632051706314, -0.018393751233816147, -0.035324957221746445, -0.07015643268823624, 0.03992270678281784, -0.026652518659830093, -0.00013389080413617194, 0.0005359531496651471, 0.02364419959485531, 0.003294995753094554, -0.06692832708358765, -0.045120175927877426, -0.0019163774559274316, 0.08804183453321457, 0.01307318639010191, -0.02056790143251419, 0.03137074038386345, 0.0033537012059241533, 0.00904708169400692, -0.049341920763254166, -0.03688247874379158, -0.059433627873659134, -0.053112588822841644, -0.017550239339470863, -0.0019460693001747131, -0.05369085446000099, 0.030076105147600174, 0.016851436346769333, -0.021779805421829224, -0.014342419803142548, -0.009926711209118366, 0.05401250347495079, 0.005858398973941803, 0.006299550179392099, -0.05091239511966705, 0.09661075472831726, -0.04077210649847984, 0.004449560306966305, -0.019048145040869713, -0.08047869801521301, 0.055917888879776, 0.014876343309879303, 0.030240491032600403, -0.042289186269044876, 0.025909438729286194, 0.007453594822436571, -0.02634337730705738, 0.02434758096933365, -0.016128839924931526, 0.0019147670827805996, -0.032775551080703735, -0.022353263571858406, 0.026587553322315216, 0.0011221464956179261, 0.042130790650844574, 0.033209528774023056, -0.019049022346735, -0.04885156825184822, 0.01569056324660778, -0.0161771010607481, -0.012068300507962704, 0.020987562835216522, -0.02902081236243248, 0.023097718134522438, -0.039645738899707794, 0.002449053805321455, 0.029034214094281197, -0.013682007789611816, -0.02087218686938286, -0.001321876421570778, 0.026316357776522636, -0.04658430069684982, -0.09131598472595215, -0.0012722787214443088, -0.0007682505529373884, -0.013765347190201283, 0.012151332572102547, 0.04108329489827156, -0.005688758566975594, 0.02105339802801609, 0.0028948113322257996, 0.03003797121345997, -0.006759669631719589, -0.008663102984428406, 0.006622841116040945, 0.017464127391576767, -0.017433954402804375, 0.016117705032229424, 0.0002162268356187269, -0.018862545490264893, 0.019642531871795654, 0.011794455349445343, -0.012825780548155308, 0.03448706865310669, 0.06624026596546173, 0.007592715322971344, -0.011674323119223118, -0.0201448742300272, 0.0016345412004739046, 0.01859896443784237, -0.026412300765514374, -0.008071922697126865, -0.004856553860008717, -0.03176780045032501, -0.0025751774664968252, 0.040319349616765976, -0.012317630462348461, -0.06004273146390915, 0.0022603149991482496, -0.02646208368241787, -0.00977018941193819, 0.10935483872890472, 0.023298021405935287, 0.042651597410440445, 0.023032108321785927, 0.03794274479150772, -0.03812096640467644, 0.05647982656955719, -0.08861745893955231, -0.03913350775837898, -0.004278955515474081, 0.008731056936085224, 0.06331274658441544, 0.08724428713321686, -0.06037725508213043, 0.032011039555072784, -0.058076631277799606, 0.030788619071245193, -0.014911027625203133, -0.04815947264432907, -0.023485150188207626, 0.013657977804541588, -0.06530507653951645, -0.024613628163933754, -0.041831593960523605, -0.012917022220790386, 0.04309142753481865, 0.09952415525913239, 0.012239747680723667, 0.0367879793047905, 0.007234178949147463, 0.04910415783524513, 0.034616902470588684, 0.03700118139386177, 0.036733802407979965, -0.05498407036066055, -0.019112730398774147, -0.029112908989191055, -0.033980417996644974, 0.0269315205514431, -0.007303233258426189, -0.019758740440011024, 0.004847004543989897, 0.02577020972967148, 0.0011480983812361956, -0.0292969923466444, -0.022320620715618134, 0.07103142887353897, -0.0034729796461760998, -0.009500831365585327, -0.06877782195806503, 0.012503680773079395, 0.032257478684186935, -0.01555885374546051, 0.035319503396749496, 0.019451478496193886, -0.04343036934733391, -0.003211656119674444, -0.011866230517625809, -0.01819217950105667, 0.02306266501545906, -0.00821365974843502, -0.010353307239711285, -0.044539473950862885, 0.048458509147167206, 0.005513358395546675, 0.08308698236942291, -0.014520314522087574, -0.016254115849733353, 0.01380881480872631, 0.0077892872504889965, -0.0528918020427227, -0.009477020241320133, 0.04231807217001915, 0.011585652828216553, -0.017661670222878456, -0.039951518177986145, -0.0017175410175696015, -0.01628199964761734, -0.043794918805360794, 1.9369386791368015e-05, -0.0041297608986496925, -0.034917913377285004, -0.026459725573658943, 0.049180466681718826, -0.03500933572649956, 0.011757249012589455, -0.02465430088341236, 0.02349701151251793, -0.02278805524110794, -0.025604814291000366, 0.017050711438059807, -0.035130102187395096, -0.0008509363979101181, 0.009446199983358383, -0.02618779055774212, -0.016177181154489517, 0.03142474964261055, -0.022792791947722435, -0.05391676723957062, -0.005998841021209955, 0.03888319060206413, 0.0308273546397686, -0.024521538987755775, 0.04123725742101669, 0.03449301794171333, 0.03626091778278351, 0.019219420850276947, -0.033324696123600006, -0.00025046474183909595, 0.057688143104314804, 0.03535791113972664, -0.03843776881694794, -0.032816726714372635, -0.025301510468125343, -0.029963400214910507, 0.004558613058179617, 0.05727077275514603, -0.02073764055967331, -0.005509511101990938, 0.006869136355817318, 0.011799278669059277, 0.04455806314945221, 0.0583316870033741, -0.012979074381291866, 0.006040597800165415, 0.026065340265631676, -0.049356188625097275, 0.00751268956810236, 0.02426019497215748, -0.029152195900678635, 0.023738577961921692, -0.01519553642719984, -0.05867673084139824, -0.01582433469593525, -0.07011420279741287, -0.005288171581923962, -0.026859186589717865, -0.08540823310613632, -0.02868887223303318, -0.023039795458316803, 0.0052734422497451305, -0.012938829138875008, -0.08338119834661484, 0.007192990742623806, -0.023456497117877007, 0.08372245728969574, -0.011389967985451221, -0.025728536769747734, 0.03341474384069443, 0.06822092831134796, 0.0037476203870028257, -0.04225587099790573, -0.019894130527973175, 0.013165379874408245, -0.02008705586194992, 0.044420067220926285, 0.030025692656636238, -0.02888169139623642, -0.0194022785872221, -0.049791496247053146, -0.02672993205487728, -0.022959569469094276, 0.04073236510157585, 0.04699478670954704, 0.00780224334448576, 0.023828895762562752, -0.019276242703199387, -0.005926098674535751, 0.0585966557264328, -0.04282625764608383, 0.04346286132931709, -0.02168111689388752, 0.0884394720196724, 0.05691337585449219, -0.02498811110854149, 0.028323356062173843, -0.05266127362847328, 0.0035364541690796614, 0.0019847266376018524, 0.008881895802915096, -0.02804938703775406, 0.010103408247232437, 0.016057705506682396, -0.08106963336467743, 0.01795961707830429, 0.04217352718114853, 0.0014261903706938028, -0.025277936831116676, -0.009371409192681313, 0.033685408532619476, 0.01556418463587761, -0.037654146552085876, 0.0181695818901062, 0.04837977886199951, 0.03598187863826752, -0.001200062339194119, -0.06728966534137726, 0.06344150751829147, -0.03647095710039139, -0.0018649754347279668, 0.011347044259309769, 0.028250638395547867, 0.02410617098212242, 0.011435574851930141, 0.01964891143143177, 0.016698043793439865, -0.0045118373818695545, 0.012055855244398117, 0.010056738741695881, 0.029463764280080795, 0.0061956713907420635, 0.05458185449242592, 0.02089911513030529, -0.03157007321715355, -0.024171726778149605, -0.03667059913277626, -0.048755571246147156, 0.0007694684900343418, 0.021876508370041847, 0.09597074240446091, 0.003373742802068591, -0.05351104959845543, -0.058054473251104355, 0.018599728122353554, 0.006976572796702385, -0.01712702587246895, -0.017562326043844223, 0.005126739852130413, 0.02546202391386032, 0.053606655448675156, 0.026991721242666245, 0.005119386129081249, 0.032660190016031265, 0.020684650167822838, 0.069789357483387, -0.00024005422892514616, 0.025410374626517296, 0.004855573642998934, 0.007356131449341774, -0.004015728831291199, 0.04783535376191139, -0.008128517307341099, -0.04621489346027374, 0.0236258115619421, -0.004638721235096455, 0.023090112954378128, 0.04951778054237366, 0.0003433989768382162, -0.05149313062429428, 0.01835394836962223, -0.039474938064813614, 0.015596739016473293, -0.0020844517275691032, -0.04481068626046181, -0.023735450580716133, 0.01597130484879017, -0.0010111687006428838, 0.014116736128926277, -4.901004025246718e-33, 0.050033558160066605, 0.00028399215079844, 0.019039804115891457, 0.018251383677124977, -0.05604281276464462, 0.005719100125133991, -0.021947011351585388, -0.04956495761871338, 0.00988087523728609, 0.022839099168777466, -0.0181568656116724, -0.0024700204376131296, 0.006571561098098755, -0.0469309538602829, 0.041962429881095886, -0.043392881751060486, 0.0234834011644125, -0.04694611579179764, -0.037009090185165405, 0.023143334314227104, 0.006097509991377592, -0.0019156860653311014, 0.033400293439626694, 0.055879827588796616, -0.029405320063233376, 0.013223716989159584, -0.03790085390210152, 0.05501920357346535, 0.009400187991559505, 0.019529130309820175, -0.014559642411768436, -0.06306499242782593, -0.028914034366607666, -0.09487207233905792, 0.004429107066243887, -0.0436757430434227, -0.03578140214085579, -0.032648880034685135, -0.07756228744983673, -0.0038205741439014673, -0.0587485171854496, -0.022703154012560844, 0.008390485309064388, 0.023849939927458763, -0.061665479093790054, 0.03459755331277847, -0.012686219997704029, 0.03539512678980827, -0.018267765641212463, -0.08438608795404434, 0.004319644067436457, 0.014122606255114079, 0.011363237164914608, -0.018555885180830956, 0.031770192086696625, 0.05501324683427811, -0.010287393815815449, -0.044954389333724976, -0.03315334394574165, 0.001004053745418787, -0.009141652844846249, 0.0040029422380030155, -0.01924002543091774, 0.08699921518564224, 0.10551462322473526, 0.007459345273673534, 0.03589785844087601, -0.003540214616805315, -0.007676652632653713, 0.030127644538879395, -0.021302105858922005, -0.0005716259474866092, 0.018115565180778503, -0.026238683611154556, 0.01723499409854412, -0.023927414789795876, -0.02131136879324913, -0.001809597248211503, 0.027887774631381035, 0.074457548558712, 0.007526191882789135, 0.03639652207493782, 0.007873148657381535, -0.0003066732606384903, -0.008958281949162483, 0.011665359139442444, 0.03033193200826645, -0.011341088451445103, 0.0016581870149821043, 0.043567389249801636, 0.014514234848320484, -0.013509965501725674, 0.020052360370755196, 0.002971164882183075, 0.05773961916565895, -0.0024333626497536898, 0.004786472301930189, 0.047340888530015945, -0.0003410572826396674, 0.014247787185013294, -0.06555934995412827, -0.04000825062394142, -0.03358869254589081, -0.04521181806921959, -0.01774045266211033, 0.025608599185943604, -0.03934728726744652, 0.049795858561992645, -0.009505280293524265, 0.017376337200403214, -0.02929360419511795, 0.004380695056170225, 0.04947692155838013, -0.020488014444708824, 0.066704660654068, -0.04785614833235741, -0.0027911625802516937, 0.028084052726626396, -0.00693945586681366, 0.00037420689477585256, -0.007783351931720972, 0.004320951644331217, -0.020971832796931267, 0.06497731059789658, -0.025127729400992393, -0.008675976656377316, -0.07704660296440125, 0.031703125685453415, -0.07230343669652939, -0.06546515971422195, 0.018708808347582817, 0.021228047087788582, 2.6887295234701014e-07, -0.0869716927409172, -0.004951111972332001, 0.04515857249498367, 0.00010575795749900863, 0.031524527817964554, 0.02884824201464653, -0.018823472782969475, 0.008946888148784637, 0.015526085160672665, 0.0347447507083416, 0.05272384732961655, 0.031126949936151505, -0.01948072947561741, -0.028264200314879417, -0.06714026629924774, -0.03662567958235741, -0.014276860281825066, 0.025982700288295746, -0.011305496096611023, -0.05907505750656128, -0.057015545666217804, 0.022389262914657593, 0.037301771342754364, 0.02272132597863674, 0.05359581112861633, -0.008734564296901226, 0.03703984245657921, 0.033505335450172424, 0.010622753761708736, -0.007954922504723072, 0.04930391535162926, -0.008187573403120041, -0.03145184367895126, -0.0747668519616127, -0.02282373607158661, 0.056993816047906876, -0.053749896585941315, 0.042432185262441635, -0.011032359674572945, -0.04135744646191597, -0.05608076974749565, 0.041487276554107666, 0.05491331219673157, -0.009110108017921448, 0.026954613626003265, -0.0748908668756485, 0.016929866746068, -0.007470616605132818, -0.005000015255063772, -0.01285680290311575, 0.013988200575113297, 0.021273203194141388, -0.031738653779029846, -0.01844053529202938, -0.018261631950736046, 0.05701855942606926, 0.005185623653233051, -0.034335896372795105, 0.022240785881876945, 0.051696229726076126, 0.029254335910081863, -0.024512171745300293, -0.015442402102053165, 0.03582252189517021, 0.0318482369184494, -0.04858313500881195, -0.03470783308148384, 1.8780085753708133e-34, -0.02041430026292801, -0.04303703084588051, 0.004850721452385187, 0.006619875319302082, -0.02228010818362236, -0.0007569359731860459, 0.017196116968989372, 0.017975999042391777, 0.1088654026389122, 0.026586713269352913, -0.039730504155159], [-0.056119829416275024, 0.004070463590323925, -0.00810107309371233, -0.057916417717933655, -0.01801663637161255, 0.013189977966248989, -0.013766703195869923, 0.010134114883840084, -0.0669793114066124, 0.0694354847073555, -0.009308619424700737, -0.023326892405748367, 0.018644623458385468, 0.01605956070125103, 0.019885577261447906, -0.009801189415156841, 0.0405200757086277, -0.0740460753440857, -0.02583947218954563, -0.03536833077669144, -0.04107652232050896, -0.0467359721660614, -0.02477450668811798, 0.04431719332933426, 0.010152729228138924, -0.029223607853055, 0.00846900139003992, 0.010284104384481907, 0.031108906492590904, -0.07932496070861816, -0.012197890318930149, -0.024241596460342407, 0.018748639151453972, 0.024763604626059532, 2.4250052774732467e-06, -0.00924533698707819, -0.012729472480714321, -0.02418270707130432, 0.005720448214560747, -0.0013882111525163054, -0.002644249703735113, -0.006221385672688484, 0.01379091665148735, 0.022806208580732346, 0.027608202770352364, -0.020988987758755684, 0.0005647523212246597, 0.08784552663564682, -0.05526835471391678, 0.03416489064693451, 0.019786855205893517, 0.06018036976456642, -0.032432444393634796, -0.038670606911182404, 0.02624424546957016, -0.03865635767579079, 0.03153758868575096, -0.0007026400999166071, 0.005453045945614576, -0.037251152098178864, -0.057641882449388504, -0.017510520294308662, 0.021797992289066315, 0.001819341559894383, -0.06769255548715591, 0.03311031311750412, 0.07460426539182663, 0.010974244214594364, -0.003503411542624235, -0.006619899533689022, -0.013017464429140091, 0.011775078251957893, -0.03193671256303787, -0.033229224383831024, 0.01687614433467388, 0.0844452753663063, -0.01899121142923832, 0.0018720589578151703, -0.0278929453343153, 0.06836386024951935, -0.015032435767352581, 0.08959125727415085, 0.03452432528138161, 0.012293871492147446, 0.039386194199323654, 0.06851551681756973, -0.039823465049266815, -0.04151573032140732, 0.0342513807117939, 0.07604065537452698, -0.028152579441666603, -0.012310037389397621, -0.005877952557057142, 0.0149860680103302, -0.016126282513141632, 0.007804714143276215, -0.023318370804190636, 0.06644027680158615, 0.007765794172883034, 0.04444340243935585, 0.04033568874001503, 0.030198736116290092, 0.016942378133535385, 0.00481713330373168, 0.017576707527041435, 0.026628902181982994, -0.059360798448324203, 0.009247533977031708, 0.003709186566993594, 0.035605255514383316, -0.012481292709708214, 0.023455599322915077, 0.0002958663972094655, 0.03558281809091568, -0.04370443522930145, -0.01247700210660696, -0.07151129096746445, -0.0027817583177238703, -0.06942056119441986, 0.030085066333413124, -0.0768406018614769, -0.00227624480612576, -0.024261023849248886, 0.028482535853981972, -0.09583213925361633, 0.00396298011764884, -0.021844973787665367, 0.030397506430745125, 0.011855261400341988, 0.023550547659397125, -0.006966788321733475, -0.044710539281368256, 0.020640568807721138, -0.0072590745985507965, -0.01908402144908905, 0.06584537029266357, 0.01104086171835661, -0.001627763151191175, 0.02227386087179184, 0.005872838664799929, 0.016855234280228615, -0.008850670419633389, 0.019812313839793205, -0.028176957741379738, 0.0006572993006557226, -0.005315990187227726, -0.008594210259616375, -0.02964101731777191, -0.008218993432819843, -0.024475540965795517, -0.03095167875289917, 0.008342333137989044, 0.0009470235672779381, -0.019345493987202644, -0.05294543504714966, 0.0020883174147456884, -0.010790104046463966, 0.05944778025150299, -0.028232276439666748, 0.01271001622080803, -0.005195396486669779, 0.05664624273777008, 0.04465906322002411, -0.015324256382882595, 0.023728882893919945, 0.04456193000078201, -0.015892131254076958, -0.0485553964972496, -0.016085054725408554, 0.003859694115817547, 0.04274367541074753, 0.005408493336290121, -0.06285908073186874, 0.06939511746168137, 0.024173356592655182, -0.010557646863162518, -0.01849553920328617, -0.03735015168786049, -0.02193359285593033, -0.01965106464922428, 0.038902804255485535, 0.017016209661960602, -0.04409682750701904, 0.0075466143898665905, -0.002211824059486389, -0.015157897025346756, -0.02795054018497467, -0.0011286167427897453, -0.027950188145041466, -0.06068956106901169, 0.013438698835670948, -0.001784427324309945, -0.041334863752126694, 0.045598048716783524, 0.007914498448371887, -0.0012194926384836435, -0.018912019208073616, -0.046807821840047836, 0.014854551292955875, 0.05107007920742035, 0.034551214426755905, -0.005877497605979443, -0.01467440277338028, -0.023131221532821655, 0.045901812613010406, -0.01021558791399002, -0.026319866999983788, -0.036568064242601395, -0.030667878687381744, -0.0215244572609663, 0.02953655645251274, 0.0029723632615059614, 0.041902001947164536, 0.010878841392695904, -0.02538960985839367, 0.0053748502396047115, -0.015454704873263836, 0.029974710196256638, 0.018752913922071457, 0.05884856358170509, 0.010582033544778824, -0.004375926684588194, -0.037136487662792206, -0.004971429239958525, -0.0134853171184659, -0.0752255842089653, 0.0028021906036883593, -0.0041750152595341206, -0.00383643782697618, -0.028295449912548065, -0.022500446066260338, -0.010445640422403812, -0.03670634329319, 0.05098221078515053, 0.0010869551915675402, -0.027110204100608826, -0.08751535415649414, -0.045178577303886414, 0.016731129959225655, -0.000660040182992816, 0.016011258587241173, -0.00995319988578558, 0.001475388533435762, -0.05539139360189438, 0.0029209183994680643, -0.0013332789530977607, 0.023409826681017876, 0.006779557093977928, -0.006082725711166859, -0.016750715672969818, 0.03794460371136665, 0.023341737687587738, -0.015016372315585613, -0.013717791996896267, -0.03760988637804985, -0.018790263682603836, 0.03841278329491615, -0.02758285030722618, -0.06329406797885895, -0.031731922179460526, 0.0006941843894310296, -0.012090962380170822, -0.004733221139758825, 0.03821833059191704, -0.01045067049562931, 0.01467162650078535, 0.03340597078204155, 0.023707089945673943, 0.010028106160461903, 0.014371708035469055, 0.02217276394367218, -0.00905721727758646, 0.0014294828288257122, -0.05523146316409111, 0.008034517988562584, 0.04460969939827919, 0.005531368311494589, 0.026821032166481018, 0.016594748944044113, 0.010012926533818245, 0.04533679038286209, 0.0179672259837389, -0.008290913887321949, -0.041502345353364944, 0.01841815933585167, -0.002102714264765382, 0.01985195092856884, -0.039213627576828, -0.007435191422700882, 0.05579417943954468, -0.004750889725983143, 0.03599483147263527, -0.029765639454126358, -0.01946580410003662, -0.008594955317676067, -0.016327645629644394, 0.012211665511131287, 0.05213534086942673, -0.020361946895718575, 0.058688972145318985, 0.059531666338443756, 0.05127068608999252, -0.02220200002193451, 0.030408602207899094, -0.0728900358080864, -0.06393734365701675, -0.001720980741083622, -0.01789376139640808, 0.017728472128510475, 0.05585775896906853, -0.05333755910396576, -0.015645740553736687, -0.002706427127122879, 0.01571096107363701, -0.01454051211476326, -0.036980729550123215, -0.008714784868061543, -0.008489183150231838, -0.0446249358355999, 0.04292779415845871, -0.057707253843545914, -0.019435200840234756, 0.020256327465176582, 0.040452755987644196, 0.013492584228515625, 0.03803262487053871, 0.036914363503456116, 0.010081310756504536, -0.010183156467974186, 0.06118404492735863, -0.010055922903120518, -0.006019992288202047, 0.008327067829668522, -0.027148237451910973, -0.0522543266415596, 0.020541468635201454, -0.0011240431340411305, 0.007355035748332739, -0.004902783781290054, 0.013443922623991966, -0.005461363587528467, -0.0699530839920044, -0.018276406452059746, 0.00922386720776558, 0.020869432017207146, -0.013875721022486687, -0.0647946149110794, 0.010821890085935593, 0.05124562606215477, -0.07949091494083405, 0.07553042471408844, 0.01148750726133585, -0.06136525422334671, -0.03739286959171295, -0.01588207669556141, -0.06527929753065109, 0.010267512872815132, -0.06649385392665863, 0.0341072604060173, -0.08620830625295639, 0.056333668529987335, -0.0006156611489132047, 0.07006200402975082, 0.005305208265781403, -0.01826358400285244, 0.0440654493868351, -0.021131714805960655, 0.007519384380429983, -0.060631491243839264, 0.029227394610643387, 0.08310212939977646, -0.005372979678213596, -0.017982861027121544, 0.015416872687637806, -0.06673280894756317, -0.01978966034948826, 0.018151037395000458, -0.002807074459269643, 0.06465491652488708, 0.008110915310680866, 0.06252098828554153, 0.040772709995508194, -0.01361625362187624, -0.004737103823572397, 0.07730717957019806, -0.0331849567592144, -0.03245377540588379, 0.04800368845462799, -0.07197727262973785, 0.009073479101061821, 0.0028721678536385298, -0.04190785065293312, -0.029025478288531303, 0.04563223570585251, 0.025202475488185883, 0.004568638280034065, 0.0016727411421015859, 0.03500622883439064, -0.04209337383508682, 0.014602533541619778, 0.0716654360294342, 0.05787841975688934, -0.02345990017056465, -0.023679763078689575, -0.05128033459186554, -0.004739623516798019, -0.010282238945364952, 0.006672066170722246, -0.05112794414162636, 0.02118898183107376, 0.029023174196481705, 0.0005109935300424695, 0.02988084778189659, 0.05342010408639908, 0.049569036811590195, 0.034636177122592926, 0.01253623515367508, -0.039446376264095306, -0.030499843880534172, 0.057748034596443176, -0.03982102498412132, -0.02748578041791916, 0.02757718227803707, -0.04169025644659996, -0.01813814416527748, 0.00289446790702641, -0.01733235828578472, 0.03455821052193642, -0.031783849000930786, -0.025154337286949158, -0.008868214674293995, -0.06040872633457184, -0.00038047131965868175, 0.008154338225722313, -0.03278208523988724, 0.025187509134411812, 0.03422870859503746, -0.03096151165664196, -0.004140925128012896, -0.0007395623251795769, -0.006690591108053923, -0.06185213103890419, 0.10938391089439392, -0.00693949731066823, 0.0067909792996943, -0.06300579011440277, 0.07188352942466736, 0.02862662635743618, 0.0053174677304923534, 0.0016567049315199256, 0.01376347616314888, 0.06671268492937088, 0.03915543481707573, 0.008736529387533665, -0.009245909750461578, -0.034867431968450546, -0.06669717282056808, 0.006140860263258219, -0.024490756914019585, 0.026153599843382835, 0.042740918695926666, 0.025649864226579666, 0.03504322096705437, 0.01797417365014553, -0.02095145918428898, 0.12645390629768372, 0.0007104533724486828, 0.06562246382236481, 0.008868801407516003, 0.06076082959771156, 0.03760938718914986, 0.0073030791245400906, 0.01677904836833477, -0.024415867403149605, 0.018020128831267357, 0.03219732642173767, 0.0011488047894090414, -0.010912454687058926, 0.014932217076420784, -0.01676817238330841, -0.028530508279800415, 0.06943453103303909, -0.01935500279068947, 0.029395733028650284, 0.034881897270679474, -0.03674011677503586, 0.025728795677423477, 0.025300752371549606, 0.006142752710729837, 0.029842529445886612, 0.07819727063179016, 0.07025889307260513, -0.02531410939991474, 0.02377701736986637, 0.07407716661691666, 0.016356339678168297, 0.04393965005874634, 0.042301543056964874, 0.013596974313259125, 0.005817928817123175, 0.039783887565135956, 0.034325581043958664, -0.03147399052977562, -0.0003508872468955815, -0.006220541428774595, 0.004393677692860365, 0.044716328382492065, 0.015592485666275024, 0.0030735251493752003, 0.004705572035163641, -0.03823523223400116, -0.04702489823102951, -0.08990917354822159, -0.044312454760074615, -0.0020006122067570686, 0.016238486394286156, 0.011762289330363274, -0.03340829163789749, 0.0033311520237475634, -0.07737448811531067, 0.01733233965933323, -0.014701779000461102, -0.01909385435283184, -0.04115765541791916, -0.00803971104323864, 0.04134782776236534, 0.00624547153711319, 0.02307944744825363, 0.002982261823490262, 0.040514469146728516, 0.06931472569704056, 0.05475715920329094, -0.07315743714570999, 0.005782994907349348, 0.007072144653648138, -0.027883920818567276, 0.03126808628439903, 0.04085025191307068, -0.012178795412182808, -0.0061381543055176735, -0.011368137784302235, -0.035516612231731415, -0.04373396933078766, 0.024270979687571526, 0.02842635288834572, -0.06327410787343979, -0.006008668337017298, -0.03781386837363243, -0.02443043328821659, 0.0066797202453017235, -0.07138904929161072, -0.000322155246976763, -0.026993278414011, 0.02631431259214878, -0.006868227384984493, -6.1338360517662395e-33, 0.023537587374448776, -0.031009715050458908, -0.0038836400490254164, 9.703862451715395e-05, -0.03527495265007019, -0.01916525326669216, -0.012860161252319813, -0.006095588207244873, 0.021493256092071533, -0.006877191364765167, -0.060032520443201065, -0.007607186678797007, 0.02296159230172634, 0.008188081905245781, -0.006644563749432564, -0.04611889272928238, 0.008490217849612236, -0.0022731369826942682, -0.020461903885006905, -0.02518284320831299, 0.013526855036616325, 0.03348619118332863, 0.058070629835128784, -0.03478642553091049, -0.031078912317752838, 0.027896085754036903, -0.04337332025170326, 0.006656705401837826, -0.008763318881392479, 0.019006449729204178, 0.009584604762494564, -0.0212219450622797, -0.04433721303939819, -0.06506656110286713, -0.01209618803113699, -0.029039675369858742, 0.02713094837963581, -0.04274313524365425, -0.03971925005316734, 0.03506387025117874, -0.024632496759295464, 0.012047898955643177, 0.025542957708239555, 0.008689592592418194, -0.06858031451702118, 0.03852827101945877, -0.040812693536281586, -0.023829208686947823, -0.03270343318581581, -0.02375921979546547, -0.010589469224214554, 0.012115963734686375, 0.051330018788576126, -0.0037983490619808435, 0.015520949847996235, 0.12020591646432877, 0.00480318907648325, -0.03414582833647728, 0.02546239085495472, -0.005234117154031992, 0.037586648017168045, -0.0056440201587975025, 0.01333175040781498, 0.09188953042030334, 0.04371223971247673, 0.017580315470695496, 0.04201039671897888, -0.005415802821516991, -0.03445558249950409, -0.056814514100551605, -0.043358802795410156, -0.004507926758378744, -0.04052863270044327, 0.0746476873755455, 0.08947959542274475, -0.023376205936074257, -0.002528653247281909, 0.021205661818385124, 0.03067415952682495, 0.03134696185588837, 0.008598992601037025, 0.019133105874061584, -0.054698504507541656, -0.04075205326080322, 0.005496399477124214, -0.03689444437623024, 0.0455239862203598, 0.0016155934426933527, -0.02486233040690422, 0.014652066864073277, 0.008873417973518372, 0.06760521233081818, -0.01639939658343792, -0.00037984398659318686, -0.011287655681371689, -0.026638830080628395, -0.008580077439546585, 0.00258710328489542, -0.024536127224564552, -0.02778714708983898, -0.06067810207605362, -0.06921865046024323, -0.05276026204228401, -0.023341067135334015, 0.005668352358043194, 0.029135698452591896, 0.01526609156280756, 0.009076668880879879, -0.03787890449166298, 0.001714623998850584, -0.022022413089871407, 0.036890558898448944, 0.08609464019536972, -0.0968402624130249, 0.09448793530464172, -0.08356529474258423, -0.006124625913798809, 0.013078784570097923, 0.027943238615989685, 0.02243351936340332, -0.013507690280675888, -0.027494508773088455, -0.06959223747253418, 0.039487093687057495, -0.012198273092508316, -0.014935947023332119, -0.04438287392258644, 0.012697635218501091, -0.07720852643251419, -0.08144833147525787, -0.0021294725593179464, 0.00613419571891427, 3.208131147403037e-07, -0.028876034542918205, -0.005945162381976843, 0.025826696306467056, 0.0009287919383496046, 0.04719347879290581, 0.06264407187700272, -0.014297141693532467, -0.011731468141078949, -0.016864124685525894, 0.0037948708049952984, 0.024702997878193855, 0.013179915957152843, 0.02389976941049099, 0.02884366549551487, 0.027173247188329697, -0.06063712760806084, -0.013409901410341263, 0.03899864852428436, 0.018637273460626602, 0.0023610806092619896, -0.02506592869758606, -0.0037089905235916376, -0.001385324401780963, -0.02745823748409748, 0.03288004547357559, -0.002645164728164673, 0.08547915518283844, -0.03357022628188133, 0.05060853809118271, -0.02862516976892948, 0.002020834479480982, 0.02371407300233841, -0.008913276717066765, -0.015605843625962734, 0.006010028533637524, 0.04247743636369705, -0.00847062561661005, -0.018687715753912926, 0.0039053575601428747, -0.026553239673376083, -0.047467924654483795, 0.0048480890691280365, 0.03813689202070236, -0.05297001823782921, 0.08836127817630768, -0.06432041525840759, 0.0401272252202034, -0.009690826758742332, 0.025558797642588615, -0.005964858457446098, 0.01638304442167282, 0.009969955310225487, -0.010785331018269062, -0.006566629279404879, -0.01505469623953104, 0.001808668253943324, -0.01215128693729639, 0.010130262933671474, 0.017855525016784668, 0.051026351749897, -0.012684758752584457, -0.05678874999284744, -0.012576770968735218, 0.06599968671798706, -0.017325695604085922, -0.05944015085697174, -0.07865235209465027, 2.954837165008359e-34, -0.005692309234291315, -0.05673280730843544, 0.06242978945374489, -0.09416067600250244, -0.027008820325136185, -0.005330794490873814, -0.027806522324681282, 0.025819772854447365, 0.06252776831388474, 0.050792887806892395, -0.026026904582977295], [-0.05276346579194069, 4.5482673158403486e-05, -0.027323473244905472, 0.011823449283838272, 0.014224875718355179, -0.02177630178630352, -0.020776165649294853, 0.025731174275279045, -0.035872552543878555, -0.007062368560582399, -0.04871335253119469, 0.000771133229136467, -0.026169510558247566, 0.02325722575187683, 0.008677396923303604, -0.030285662040114403, -0.001563534839078784, -0.010833500884473324, -0.012382294051349163, -0.018074117600917816, -0.025981472805142403, -0.04925176501274109, -0.004819911904633045, 0.036148518323898315, 0.03486265614628792, -0.00474602309986949, 6.840785772510571e-06, 0.018710626289248466, 0.018300190567970276, -0.09556464105844498, -0.023906873539090157, -0.01094526331871748, 0.030713459476828575, 0.07842128723859787, 2.155716856577783e-06, -0.025763362646102905, 0.0699576884508133, -0.006671791430562735, 0.04050721600651741, 0.09448845684528351, 0.008008910343050957, 0.003623276948928833, -0.0012385104782879353, 0.0008360871579498053, -0.03519299253821373, 0.014465950429439545, 0.003338039852678776, 0.031157342717051506, 0.03634779900312424, 0.02584817446768284, 0.025282347574830055, -0.02072008140385151, -0.015644913539290428, -0.014538025483489037, -0.0075071025639772415, -0.10465187579393387, 0.01732567884027958, 0.025706179440021515, -0.005771022755652666, -0.05750538036227226, -0.0165995005518198, -0.025275206193327904, 0.00980372168123722, 0.0020684378687292337, -0.03335864841938019, 0.04099893569946289, 0.031618084758520126, 0.026992179453372955, -0.016302302479743958, 0.027778973802924156, -0.0991756021976471, 0.02374217100441456, -0.0007490227581001818, -0.023720314726233482, 0.020769152790308, -0.034721218049526215, 0.00018792810442391783, -0.04520227015018463, 0.005942183081060648, -0.025447551161050797, -0.04463573917746544, 0.03275756910443306, 0.025596322491765022, -0.03150317445397377, -0.03438780829310417, 0.04444758966565132, -0.005296546500176191, -0.03389337286353111, -0.005082832649350166, 0.05090777575969696, 0.02014717273414135, -0.10082133859395981, -0.032960131764411926, -0.025563888251781464, -0.05263102427124977, -0.019187001511454582, 0.004732503090053797, 0.010532068088650703, -0.03484158217906952, 0.0038076103664934635, 0.08492662012577057, 0.01959400437772274, 0.05613922327756882, -0.027097558602690697, -0.06777531653642654, 0.06223533675074577, -0.05282997339963913, 0.03498199209570885, 0.0008141043363139033, 0.050399865955114365, -0.040672093629837036, 0.0016781690064817667, -0.011135615408420563, 0.024524057283997536, -0.0040650684386491776, -0.014710793271660805, -0.08059629797935486, 0.0018485409673303366, -0.02177525870501995, -0.01425054669380188, 0.019449183717370033, 0.04162409156560898, 0.01265170332044363, 0.045984815806150436, -0.060760825872421265, -0.043548163026571274, -0.09049763530492783, 0.016018608585000038, 0.061641935259103775, -0.006234522443264723, -0.013028831221163273, -0.007192691322416067, -0.024219170212745667, -0.010222119279205799, -0.00144576421007514, 0.08919573575258255, 0.030735816806554794, 0.024024609476327896, 0.027553921565413475, 0.05348420515656471, -0.018089534714818, -0.05689384415745735, -0.03497789427638054, -0.04231853410601616, -0.019863134250044823, -0.05143580585718155, -0.030405860394239426, 0.008816265501081944, 0.016079695895314217, -0.0015930745285004377, -0.02053968422114849, 0.021924378350377083, 0.01955597661435604, -0.00969152431935072, -0.0020204302854835987, -0.023117246106266975, -0.06339384615421295, 0.08927012979984283, 0.015454311855137348, 0.01960808038711548, 0.0025512641295790672, 0.02650512009859085, 0.02045607753098011, -0.0021434458903968334, 0.0042145573534071445, 0.014756790362298489, -0.0012407810427248478, -0.03379349410533905, 0.013172803446650505, 0.11587639898061752, 0.011244744062423706, -0.003097216133028269, -0.06599529087543488, -0.010187912732362747, 0.04821351170539856, 0.02099689468741417, 0.03833399713039398, -0.0024490621872246265, 0.03899705037474632, -0.018698984757065773, 0.0027299481444060802, 0.00309965037740767, -0.014171023853123188, 0.06520383059978485, 0.05692274868488312, 0.04102081432938576, -0.04082946851849556, 0.035388652235269547, -0.03213074058294296, -0.07991893589496613, 0.0004237212997395545, -0.00040943355998024344, -0.011585372500121593, -0.01304761040955782, 0.03336270526051521, 0.013416086323559284, -0.053573548793792725, -0.02096506953239441, -0.013820439577102661, 0.04220594838261604, -0.02537424862384796, -0.05805414542555809, -0.008763027377426624, -0.04465721175074577, -0.027389677241444588, -0.043445296585559845, -0.01063578948378563, -0.008075112476944923, -0.07640594989061356, -0.016650235280394554, -0.01925710402429104, 0.04241757467389107, 0.02338034100830555, -0.018104812130331993, -0.003260982222855091, -0.04510154947638512, 0.0404852069914341, 0.087489053606987, 0.004331459291279316, 0.05783519148826599, 0.0061454433016479015, 0.04372059926390648, -0.0004748636274598539, 0.026143541559576988, -0.011978275142610073, 0.008612959645688534, 0.06456092745065689, -0.04726878181099892, 0.006626877933740616, 0.028602905571460724, -0.016223371028900146, -0.025885438546538353, -0.006080924533307552, 0.03253279626369476, -0.02128531225025654, 0.0012066398048773408, -0.018606361001729965, 0.01033070683479309, 0.013790316879749298, 0.0068697757087647915, 0.07294480502605438, 0.055370647460222244, 0.008116751909255981, 0.018322542309761047, 0.02013568952679634, 0.04909246414899826, 0.0043782140128314495, 0.0056132120080292225, -0.027500852942466736, -0.012483600527048111, -0.010889003053307533, 0.0062263887375593185, 0.020251087844371796, 0.0061293700709939, -0.042561862617731094, 0.049734871834516525, -0.03958945721387863, 0.0005062666605226696, -0.08069245517253876, 0.024649571627378464, 0.05237909406423569, 0.01144391018897295, -0.026811491698026657, 0.0014726057415828109, 0.025096870958805084, -0.006929914932698011, -0.0036096496041864157, 0.03345073014497757, 0.04304773360490799, -0.07074939459562302, 0.0212125051766634, 0.015259304083883762, -0.054787393659353256, -0.026581479236483574, 0.025856435298919678, 0.002519490197300911, 0.02493196539580822, 0.0452469140291214, -0.028129007667303085, 0.0528942234814167, 0.02879808284342289, -0.01711825653910637, 8.763352525420487e-05, -0.015434161759912968, 0.012865360826253891, 0.02928910404443741, -0.01628059707581997, 0.009825215674936771, -0.02283053658902645, 0.016209904104471207, -0.02454499341547489, -9.233055607182905e-05, -0.008955084718763828, -0.0608399361371994, 0.042989179491996765, 0.04676191881299019, -0.04395406320691109, 0.08704736083745956, -0.01957731321454048, -0.022110523656010628, 0.02967698499560356, 0.00028631227905862033, -0.0014678987208753824, 0.0479874424636364, -0.007475447375327349, -0.07375489920377731, -0.04322345182299614, -0.007944374345242977, 0.007082748226821423, 0.1175825372338295, -0.037225231528282166, -0.024574492126703262, -0.03841120004653931, 0.041297655552625656, -0.042214248329401016, -0.06471214443445206, -0.016692591831088066, -0.011769052594900131, -0.0430220328271389, -0.005788027308881283, -0.014815491624176502, 0.03506788611412048, 0.04124666377902031, 0.004633451346307993, 0.01506798341870308, 0.010741415433585644, 0.007142251823097467, 0.013144908472895622, -0.019192928448319435, 0.011811479926109314, 0.008880039677023888, -0.05574636906385422, -0.019415931776165962, -0.020165150985121727, -0.03331797569990158, 0.014209815301001072, 0.0064003667794167995, -0.00039766848203726113, 0.02925071306526661, -0.010305004194378853, -0.005304600112140179, -0.04838430508971214, -0.001524516032077372, 0.06091533228754997, -0.01641474850475788, -0.0011621505254879594, -0.03375563025474548, -0.023408865556120872, -0.03198999911546707, -0.03357389196753502, -0.0015094404807314277, 0.025113174691796303, -0.03856080397963524, -0.02074917033314705, 0.0011991560459136963, -0.005396496970206499, 0.0025298488326370716, 0.023776015266776085, 0.03768051415681839, -0.03347501531243324, 0.12866917252540588, 0.02412840910255909, 0.05349761247634888, -0.01375568937510252, 0.019634032621979713, -0.0048997849225997925, -0.03847990557551384, -0.034286607056856155, -0.035777460783720016, 0.08909539133310318, 0.09409726411104202, -0.04267223924398422, -0.0016104793176054955, 0.025463376194238663, -0.030671987682580948, 0.018174750730395317, 0.005134650971740484, -0.008195784874260426, 0.02042759209871292, -0.007286758627742529, 0.003537027398124337, -0.07880550622940063, -0.00894889235496521, 0.005797477439045906, 0.07380932569503784, -0.0073407599702477455, -0.021523118019104004, 0.04834610968828201, -0.05664661526679993, 0.015751974657177925, -0.004202449228614569, -0.06606719642877579, -0.016845885664224625, 0.012438051402568817, -0.013627900741994381, -0.07476803660392761, 0.032775167375802994, 0.018718888983130455, 0.041291244328022, -0.021084528416395187, 0.003114374354481697, 0.004717182833701372, 0.021701417863368988, -0.0020480139646679163, -0.08565225452184677, 0.00741642015054822, 0.014925841242074966, 0.040509358048439026, -0.018633553758263588, -0.044466905295848846, -0.03675948828458786, -0.05070876702666283, 0.04959665983915329, 0.04774728789925575, 0.02211667411029339, -0.026759333908557892, 0.01747940480709076, -0.005797803867608309, 0.046541567891836166, 0.027365870773792267, 0.017941661179065704, 0.027769772335886955, 0.00404221098870039, 0.03186837211251259, -0.06300108879804611, -0.007615414448082447, -0.009934823028743267, 0.008391816169023514, 0.012929573655128479, -0.006815511733293533, 0.004895017948001623, -0.04522199556231499, -0.03095582127571106, 0.004053505603224039, -0.08258718997240067, -0.04545574262738228, -0.050941649824380875, 0.022350825369358063, -0.03951472043991089, -0.05537082627415657, -0.037780631333589554, -0.07367328554391861, 0.07049714773893356, -0.02089007943868637, 0.005877246148884296, -0.060473233461380005, 0.03981902450323105, 0.04291732981801033, -0.03923657909035683, 0.046984247863292694, 0.02333042025566101, -0.017592888325452805, 0.013057089410722256, 0.0186234712600708, -0.028588516637682915, -0.027856117114424706, -0.03793807327747345, 0.021047664806246758, 0.009285292588174343, 0.03926251828670502, 0.03940054029226303, 0.017211252823472023, 0.03367144986987114, 0.018170760944485664, 0.043539803475141525, 0.027045052498579025, 0.04990414157509804, 0.029707035049796104, 0.006174546200782061, 0.05645948275923729, -0.0038047577254474163, 0.00839762669056654, 0.04584164917469025, -0.021136518567800522, 0.01948653906583786, 0.011880024336278439, 0.015785422176122665, -0.05548284575343132, -0.007558391895145178, -0.027581840753555298, -0.024852409958839417, 0.12558415532112122, -0.0017274398123845458, 0.0024455622769892216, 0.010099676437675953, 0.00993775948882103, -0.06963995844125748, 0.030859297141432762, -0.06111331656575203, -0.028306951746344566, 0.04908783733844757, -0.018700720742344856, -0.0013303549494594336, 0.007840505801141262, 0.05688244849443436, -0.05914176255464554, -0.036778051406145096, -0.002397196600213647, 0.03611627221107483, 0.07308906316757202, -0.040334105491638184, -0.004935557022690773, 0.03321237862110138, 0.0005687780794687569, 0.019772516563534737, 0.049250733107328415, -0.005913564935326576, -0.007974463514983654, 0.014318505302071571, -0.018335793167352676, 0.023199502378702164, -0.004582826513797045, -0.03954940289258957, 0.002139310585334897, 0.00768804457038641, -0.004872697871178389, 0.12987761199474335, 0.012179900892078876, -0.02339470200240612, -0.015022729523479939, -0.005751600954681635, 0.008273283950984478, -0.026788897812366486, -0.008820443414151669, 0.016435014083981514, -0.008571171201765537, 0.0044680931605398655, -0.03412964195013046, -0.002583565656095743, 0.02074948139488697, 0.03413264825940132, 0.08136732876300812, -0.05480264499783516, 0.022521980106830597, 0.001510897185653448, -0.031047681346535683, 0.02399148792028427, 0.07747480273246765, -0.02198920026421547, -0.04740352928638458, -0.05372840538620949, 0.015669066458940506, -0.03207479044795036, 0.05756204202771187, 0.017105253413319588, -0.03247285261750221, 0.017147323116660118, -0.021846596151590347, -0.00833509024232626, -0.02985852211713791, -0.026000037789344788, -0.034019798040390015, -0.00878165103495121, -0.014549239538609982, 0.00845947302877903, -5.446279855078683e-33, 0.01307452842593193, 0.039869897067546844, 0.024824216961860657, -0.018351133912801743, -0.04729481413960457, 0.004381906241178513, -0.002379173180088401, -0.023243684321641922, -0.017279626801609993, 0.007377547677606344, -0.024765463545918465, -0.012991228140890598, 0.016418786719441414, -0.027646590024232864, 0.010103244334459305, -0.06630843132734299, 0.0375058650970459, -0.05106925219297409, -0.034663617610931396, -0.01175148505717516, -0.004037525039166212, 0.03812985494732857, -0.00983169674873352, 0.03433701768517494, -0.013312824070453644, -0.0015655744355171919, -0.017224466428160667, -0.01320456899702549, -0.01730159856379032, 0.00864808727055788, 0.0064530097879469395, -0.02980399876832962, -0.017803894355893135, -0.06645827740430832, -0.007359762210398912, -0.05754494667053223, -0.01383056491613388, -0.014494966715574265, -0.08100759983062744, 0.0014469427987933159, -0.035727228969335556, -0.051361795514822006, 0.05414494872093201, 0.03978170454502106, -0.10552191734313965, -0.018381888046860695, 0.006261820439249277, 0.01124813687056303, -0.014628976583480835, -0.046325743198394775, 0.02429039031267166, 0.032199062407016754, 0.04326330125331879, 0.014493506401777267, 0.013293949887156487, -0.0073450892232358456, 0.0012511631939560175, 0.005610282067209482, -0.059368863701820374, 0.009713047184050083, -0.014365410432219505, 0.013600131496787071, 0.016324611380696297, 0.05714410915970802, 0.07492997497320175, 0.009554279036819935, 0.09481330215930939, -0.01592615433037281, -0.029854128137230873, -0.03162185475230217, -0.0037034719716757536, 0.008248765952885151, 0.07670861482620239, 0.027133267372846603, 0.07702647149562836, -0.06863650679588318, -0.025148626416921616, 0.06842625141143799, 0.005840649828314781, 0.04120193421840668, -0.028270298615098, 0.03181913495063782, -0.05238231271505356, 0.020818231627345085, 0.0025197335053235292, 0.0040029194205999374, 0.0043288348242640495, -0.07363669574260712, 0.009121582843363285, 0.012171125039458275, 0.02286314219236374, -0.01473233662545681, 0.0007236087694764137, -0.010290667414665222, 0.05415785685181618, -0.007715041749179363, -0.00047014994197525084, 0.047209903597831726, -0.024255802854895592, -0.03138174116611481, -0.03443945571780205, -0.03708915412425995, 0.003681028261780739, 0.0388147234916687, 0.0026279620360583067, -0.021387854591012, -0.05233318358659744, 0.0052101342007517815, -0.002150530694052577, 0.02863958291709423, 0.00931556522846222, 0.026870321482419968, 0.06477876752614975, -0.048260126262903214, 0.024657052010297775, -0.04752776399254799, 0.021399635821580887, 0.00881822220981121, -0.026574959978461266, -0.05527976155281067, -0.025587746873497963, 0.0514129176735878, 0.020846838131546974, 0.03106878511607647, 0.0024668886326253414, 0.016169270500540733, -0.03828664496541023, 0.022203076630830765, -0.08883495628833771, -0.041543539613485336, -0.05792675167322159, -0.01011294312775135, 2.8832806719947257e-07, -4.150231688981876e-05, 0.036850787699222565, -0.006470588501542807, 0.014175612479448318, -0.009670538827776909, 0.06085497513413429, -0.05966823175549507, 0.019921908155083656, 0.009618868120014668, 0.026784459128975868, 0.03647615760564804, 0.03013933263719082, -0.01536960992962122, 0.0029196159448474646, -0.0374918133020401, -0.026390284299850464, -0.04455395042896271, 0.06920632719993591, -0.005501577164977789, -0.009095381014049053, -0.02183542773127556, 0.032799333333969116, 0.014776481315493584, 0.024959169328212738, 0.05154874920845032, -0.04596185311675072, 0.013622133061289787, 0.022365223616361618, 0.04356857389211655, 0.0063671329990029335, 0.04280584305524826, 0.05665648728609085, -0.027299193665385246, -0.04222433269023895, -0.017380068078637123, 0.05013523995876312, 0.02227305807173252, 0.017664160579442978, -0.01827937364578247, -0.025790320709347725, -0.023168599233031273, 0.031212270259857178, 0.04499973729252815, -0.0520053468644619, 0.058306898921728134, -0.026392914354801178, -0.014670049771666527, -0.034931380301713943, -0.05614493787288666, -0.016267310827970505, 0.03603527694940567, 0.0013928324915468693, -0.0012351878685876727, 0.018063431605696678, -0.03978535905480385, 0.039170365780591965, 0.02972155436873436, -0.036815397441387177, 0.00654797675088048, 0.03456518054008484, -0.02657664567232132, -0.0334206186234951, -0.04292532056570053, 0.04197968915104866, 0.03712983801960945, -0.03322720527648926, -0.03205832839012146, 2.1230137405058333e-34, -0.028095725923776627, -0.026626665145158768, 0.0026149095501750708, -0.048095740377902985, -0.0015117332804948092, 0.004727017600089312, 0.03662234917283058, -0.0005139785935170949, 0.029661940410733223, 0.030139120295643806, 0.0028596629854291677], [-0.021530289202928543, 0.04898805171251297, -0.014925939962267876, -0.020630642771720886, 0.0023565341252833605, -0.00031681687687523663, -0.0031596024055033922, 0.0005632670945487916, -0.022081950679421425, 0.019389422610402107, -0.042398851364851, -0.021926097571849823, -0.04528858885169029, 0.020714331418275833, 0.015581862069666386, -0.03871575742959976, 0.017252204939723015, -0.03988796845078468, 0.010676538571715355, -0.036389924585819244, -0.023052910342812538, -0.06905783712863922, -0.01041776966303587, 0.024129630997776985, 0.01617703214287758, -0.003730997908860445, -0.030661912634968758, -0.009325049817562103, 0.00870033074170351, -0.07310269773006439, 0.007214262615889311, -0.01946830004453659, 0.04435397684574127, 0.07367052137851715, 2.2580145468964474e-06, -0.02835865505039692, -0.005064775701612234, -0.0327235572040081, 0.036605678498744965, 0.023883171379566193, -0.0042211804538965225, 0.04916765168309212, -0.007487967144697905, 0.018132830038666725, -0.011187119409441948, -0.048097044229507446, 0.05184142291545868, 0.05569147318601608, 0.046077910810709, 0.0005280099576339126, 0.02665271982550621, -0.0172096136957407, 0.004329266492277384, -0.03149591013789177, -0.00505972420796752, -0.10819116234779358, 0.040914423763751984, 0.023738251999020576, 0.012023687362670898, -0.036112044006586075, -0.009539319202303886, -0.006086356472223997, 0.023890003561973572, 0.0157467108219862, -0.021066831424832344, 0.01536854449659586, -0.01558203436434269, 0.009283164516091347, -0.026039335876703262, 0.015428347513079643, -0.08230294287204742, 0.01636412926018238, 0.02484005317091942, -0.025980375707149506, -0.007090480532497168, -0.030778223648667336, -0.012807256542146206, 0.02097715623676777, -0.035707153379917145, -0.006654231809079647, -0.015854790806770325, 0.03592275083065033, 0.029028138145804405, -0.0056445179507136345, -0.03622237592935562, 0.022563865408301353, -0.03571394830942154, -0.03575799614191055, 0.041956715285778046, 0.052023570984601974, -0.04324803501367569, -0.015757638961076736, -0.016297219321131706, -0.013619055971503258, -0.06101585924625397, 0.0006074724951758981, 0.032152533531188965, -0.010987435467541218, -0.026499155908823013, -0.023844126611948013, 0.047249771654605865, -0.01547951065003872, 0.027068519964814186, 0.026044685393571854, -0.015980646014213562, 0.06538522243499756, -0.06146426126360893, 0.005017621908336878, 0.004379733465611935, 0.03786635026335716, -0.040099598467350006, 0.006442399229854345, -0.042270973324775696, 0.053398605436086655, 0.030242616310715675, -0.011380111798644066, -0.04657881334424019, -0.0019084600498899817, -0.03679375350475311, 0.003586825681850314, -0.007416531443595886, 0.040621720254421234, -0.027156276628375053, 0.04728797823190689, -0.050018344074487686, -0.028100283816456795, -0.09087897092103958, 0.034342702478170395, 0.02700207754969597, 0.020312510430812836, -0.00161628273781389, -0.005111457780003548, -0.038540069013834, -0.03531033918261528, -0.007913119159638882, 0.12034386396408081, 0.005360106471925974, 0.036753471940755844, -0.030374128371477127, 0.02353830635547638, 0.012787248007953167, 0.004286064766347408, 0.025629093870520592, -0.05146056413650513, 0.003777814097702503, -0.06548946350812912, -0.0016592844622209668, 0.011991489678621292, 0.004425134044140577, -0.013590812683105469, -0.013366264291107655, 0.03805099055171013, 0.022360578179359436, -0.02515980415046215, 0.06172984465956688, 0.017920151352882385, -0.047444041818380356, 0.07212067395448685, 0.06159864738583565, 0.04087761789560318, 0.013813738711178303, 0.024345284327864647, 0.02977881021797657, 0.008133111521601677, 0.009465902112424374, -0.019667701795697212, 0.04484563320875168, -0.04143925756216049, 0.020719198510050774, 0.12973569333553314, 0.0228497963398695, 0.013954268768429756, -0.08237355947494507, -0.009192964993417263, 0.033944182097911835, -0.004025168251246214, 0.0582331083714962, 0.01372456829994917, 0.018161648884415627, -0.003623851342126727, -0.014413287863135338, -0.0023985079023987055, -0.015491217374801636, 0.04518086090683937, 0.0481630377471447, 0.017745988443493843, -0.004808624740689993, 0.029020680114626884, -0.032689668238162994, -0.05808836966753006, 0.04040684178471565, -0.007020596880465746, -0.016811011359095573, -0.015349515713751316, 0.018608471378684044, 0.003120630979537964, -0.06277201324701309, -0.11638695001602173, 0.0033270535059273243, 0.07083986699581146, -0.0357767678797245, -0.06779829412698746, 0.012478945776820183, -0.04366513714194298, 0.016736900433897972, -0.04571276158094406, -0.007446196395903826, -0.042540043592453, -0.04614424332976341, -0.038035862147808075, 0.028247814625501633, -0.01494357455521822, 0.029324909672141075, -0.03709568455815315, -0.04810291901230812, -0.026867270469665527, -0.01559709943830967, 0.10464585572481155, 0.028747010976076126, 0.02153123915195465, -0.04366563260555267, 0.032993465662002563, 0.02518739551305771, -0.004945602733641863, -0.04892594367265701, -0.07666225731372833, 0.07375053316354752, -0.008760624565184116, 0.025359248742461205, -0.03119117021560669, -0.011171367950737476, -0.00389654072932899, -0.0146650280803442, 0.010724849067628384, -0.01409870944917202, -0.07082656770944595, -0.04059501737356186, -0.0032092526089400053, 0.0030697558540850878, -0.06074732169508934, 0.04577392339706421, 0.02678290195763111, 0.0024651787243783474, -0.015308516100049019, 0.015402151271700859, -0.003515371587127447, -0.026499848812818527, -0.0030132278334349394, -0.054755646735429764, -0.004521607421338558, 0.015805337578058243, -0.03297247365117073, 0.05499131605029106, -0.0027430211193859577, -0.039613377302885056, -0.00786106288433075, -0.04966520145535469, -0.016818707808852196, -0.11841440945863724, 0.02406957373023033, 0.0028662660624831915, 0.014279956929385662, -0.0004161662654951215, 0.035814300179481506, 0.003481905208900571, 0.012716219760477543, -0.01536859106272459, -0.0032215407118201256, 0.02271050028502941, -0.047779422253370285, 0.002379470970481634, -0.011208119802176952, -0.02969181537628174, 0.007709507364779711, -0.017121577635407448, -0.017820917069911957, -0.009687203913927078, -0.005280082114040852, -0.007355933077633381, 0.0510956235229969, 0.044492561370134354, 0.019957367330789566, -0.012470210902392864, -0.042652398347854614, 0.0008441336103715003, 0.0029468892607837915, -0.010339362546801567, -0.008928384631872177, -0.01181569043546915, -0.028013359755277634, -0.007442620117217302, 0.001189999165944755, -0.0236895103007555, 0.0024127059150487185, 0.008473312482237816, -0.02281828783452511, -0.03841089829802513, 0.07747960835695267, 0.009691822342574596, 0.020998340100049973, 0.04267406836152077, 0.024981124326586723, -0.03379359096288681, 0.054973054677248, 0.0014637877466157079, -0.05699430778622627, 0.005295493174344301, -0.01211098674684763, 0.01580096036195755, 0.08615674078464508, -0.035571951419115067, 0.013093221932649612, -0.026526734232902527, 0.03946816548705101, 0.016688840463757515, -0.09677941352128983, -0.05196802318096161, -0.005078263580799103, -0.014304391108453274, -0.005411501508206129, -0.016829947009682655, 0.002888148184865713, 0.11170276254415512, 0.04342233017086983, 0.0029326078947633505, -0.001007134560495615, -0.02658715844154358, 0.007052921690046787, -0.005945561453700066, -0.00842701829969883, 0.026676081120967865, -0.03282211348414421, -0.033358506858348846, -0.056726373732089996, -0.049597613513469696, 0.007545909844338894, -0.019294701516628265, -0.009845812804996967, -0.003976300358772278, -0.018699511885643005, -0.00579274445772171, -0.07442717999219894, 0.0007874901057220995, 0.019702119752764702, -0.010169001296162605, -0.007674813270568848, -0.009734543971717358, -0.016310369595885277, -0.029543248936533928, -0.016106704249978065, 0.05070903152227402, 0.024183865636587143, -0.02459513582289219, -0.03209003433585167, 0.00885291863232851, 0.023987287655472755, 0.03525668382644653, 0.007125319913029671, 0.05952197313308716, -0.06458869576454163, 0.07101087272167206, -0.026266857981681824, 0.06244096904993057, -0.013480287045240402, 0.005707733333110809, 0.04354051500558853, -0.011616080068051815, -0.05084998533129692, -0.04623416066169739, 0.04493540897965431, 0.01400770153850317, -0.03040660358965397, -0.04464871063828468, -0.008649179711937904, -0.02064860239624977, -0.01391752902418375, 0.002826050855219364, -0.015729520469903946, 0.0366591140627861, -0.016103196889162064, 0.0528072826564312, -0.0847284272313118, -0.03460344672203064, -0.011369050480425358, 0.08611743152141571, -0.012386738322675228, -0.05933334305882454, 0.024307018145918846, -0.0508917011320591, 0.0076332841999828815, -0.006213046610355377, -0.06287198513746262, -0.03538166359066963, -0.004499850329011679, -0.05171746388077736, -0.07304378598928452, 0.011789041571319103, 0.041115280240774155, 0.011497863568365574, -0.017064470797777176, 0.03415331244468689, 0.03083551488816738, 0.06553075462579727, 0.0015391367487609386, -0.06286584585905075, -0.0011625700863078237, 0.0628160834312439, -0.0032043750397861004, -0.012625305913388729, -0.0034216877538710833, -0.05624422803521156, -0.02321779727935791, 0.01959204114973545, 0.05800217390060425, 0.008556046523153782, -0.033842314034700394, 0.018765350803732872, 0.016189299523830414, 0.031906601041555405, 0.04821854829788208, 0.02991594932973385, 0.04067876562476158, 0.032191671431064606, -0.021156826987862587, -0.05381391569972038, -0.01158320251852274, -0.03171180188655853, 0.030196532607078552, 0.009574750438332558, -0.02639225497841835, -0.005488131195306778, -0.06977993994951248, -0.03220045566558838, -0.01019743550568819, -0.05947894603013992, 0.017410285770893097, -0.03961670771241188, -0.009784193709492683, 0.021337484940886497, -0.0634775385260582, -0.013388953171670437, -0.03678251430392265, 0.07358283549547195, -0.011677307076752186, -0.03754623234272003, -0.012431073933839798, 0.06668000668287277, 0.0201189573854208, -0.05290594324469566, 0.012410481460392475, -0.0013773319078609347, -0.04068741947412491, 0.06187592074275017, 0.009111924096941948, -0.04954172670841217, -0.01992470771074295, -0.005584607366472483, -0.0038847639225423336, -0.017878783866763115, 0.04777064174413681, 0.07895782589912415, 0.01415348518639803, 0.009532415308058262, 0.004659757949411869, -0.03507102280855179, 0.040413998067379, 0.044888950884342194, 0.07632988691329956, 0.0005395792541094124, 0.0543791726231575, 0.004153619520366192, -0.006997930817306042, 0.018383679911494255, -0.06107568368315697, 0.005977559369057417, -0.026078110560774803, 0.03609639033675194, -0.05923962965607643, 0.020547278225421906, -0.046820953488349915, -0.05088885501027107, 0.0695047378540039, -0.0020774672739207745, -0.02019183710217476, -0.01452169381082058, 0.032393332570791245, 0.03286781534552574, 0.0007081676740199327, -0.04323272779583931, 0.04120840132236481, 0.04074922949075699, 0.008187893778085709, 0.00020543498976621777, 0.023993011564016342, 0.11128289997577667, -0.023158209398388863, -0.017787400633096695, 0.007776911370456219, 0.05177322030067444, 0.05784260109066963, -0.023243578150868416, 0.00750199519097805, 0.01581907644867897, 0.020802179351449013, 0.03211544081568718, 0.06049933657050133, 0.009179085493087769, 0.01640191115438938, 0.032327134162187576, 0.007117096334695816, -6.772563938284293e-05, -0.0347265750169754, -0.0569634884595871, -0.06843135505914688, 0.010091559961438179, 0.019618267193436623, 0.10169093310832977, 0.005534139461815357, -0.04863521456718445, -0.03220844641327858, 0.008949031122028828, 0.0001391629921272397, -0.056475602090358734, -0.017289327457547188, 0.007027786690741777, 0.014094940386712551, 0.04483433812856674, -0.004820091649889946, 0.009060785174369812, 0.0415315181016922, 0.03497260436415672, 0.06899669766426086, -0.05553934723138809, 0.012066701427102089, -0.0020355747547000647, -0.04038412868976593, 0.08642161637544632, 0.08551514148712158, -0.014985865913331509, -0.05821061506867409, 0.002551572397351265, 0.020229242742061615, -0.015391162596642971, 0.06955108046531677, 0.042671095579862595, -0.03427340090274811, 0.038161322474479675, -0.03330995887517929, -0.0035810668487101793, 0.01766786351799965, -0.03610410913825035, -0.028582077473402023, -0.02166479267179966, -0.0036942146252840757, -0.01188761368393898, -5.603884627507166e-33, 0.01076706126332283, 0.005206660367548466, 0.012910807505249977, -0.05315867066383362, -0.0251457616686821, -0.015960104763507843, 0.0032861833460628986, -0.030454590916633606, 0.0012280514929443598, 0.017054246738553047, -0.02004343457520008, -0.013015231117606163, 0.01341747771948576, -0.005124053452163935, 0.013955743052065372, -0.029192280024290085, 0.01819087751209736, -0.017795398831367493, -0.03536386787891388, 0.01199876144528389, 0.024781107902526855, 0.048901427537202835, 0.019812924787402153, 0.006646015215665102, -0.038288265466690063, 0.031560108065605164, 0.003692308673635125, -0.0044672186486423016, 0.004632012918591499, 0.03825468569993973, 0.008860036730766296, -0.026915349066257477, -0.03140055015683174, -0.06376253813505173, -1.673941733315587e-05, -0.0346604622900486, -0.02118484489619732, -0.02406053990125656, -0.05198565125465393, 0.004222881980240345, 0.006598940584808588, -0.03277230262756348, 0.04412367567420006, 0.035670775920152664, -0.06379994004964828, 0.01251280028373003, 0.017973249778151512, 0.03198925405740738, -0.0283559188246727, -0.08006833493709564, 0.04933740943670273, 0.013433719053864479, 0.0017979340627789497, 0.0008775648893788457, 0.038102954626083374, 0.01536639966070652, -0.010637949220836163, 0.01949465088546276, -0.020678460597991943, 0.018981998786330223, -0.003689585719257593, -0.002242006827145815, 0.013059621676802635, 0.07526908069849014, 0.06364636868238449, 0.03854861110448837, 0.05088045448064804, -0.0054349517449736595, 0.010698474943637848, -0.012815956026315689, -0.01673966832458973, -0.007656935136765242, 0.041602376848459244, 0.03429829329252243, 0.07549909502267838, 0.016223253682255745, -0.04311937838792801, 0.05615584924817085, 0.009766250848770142, 0.049734532833099365, 0.0019168138969689608, 0.030332084745168686, -0.03300149738788605, 0.007225155830383301, -0.005567499436438084, -0.01968894526362419, 0.004528570920228958, -0.012697351165115833, 0.007507104892283678, 0.008475232869386673, 0.018834080547094345, 0.013037169352173805, 0.01012382097542286, -0.030069490894675255, 0.027079006657004356, 0.008122391067445278, 0.01821686513721943, 0.043604884296655655, -0.005801565945148468, 0.005030367057770491, -0.04268113151192665, -0.04547978937625885, 0.003326158504933119, -0.00720525486394763, 0.010051305405795574, 0.006864862982183695, -0.038082100450992584, 0.0001180367253255099, -0.00923303421586752, 0.027136335149407387, 0.015123977325856686, 0.013979039154946804, 0.04294976592063904, -0.037515684962272644, 0.032955996692180634, -0.07226308435201645, 0.02224055863916874, 0.02132822759449482, -0.004023364279419184, -0.020500892773270607, 0.01316371001303196, -0.013197710737586021, -0.029101964086294174, 0.033784352242946625, -0.002949559362605214, 0.01767054572701454, -0.08005750179290771, 0.0087314797565341, -0.06960839778184891, -0.08125335723161697, -0.02288554050028324, 0.018999574705958366, 2.911249055159715e-07, -0.061544910073280334, 0.04867413640022278, 0.051934994757175446, -0.005143498070538044, 0.00041870641871355474, 0.056579601019620895, -0.04394305497407913, 0.009866162203252316, -0.02302086353302002, 0.031128590926527977, 0.06556832045316696, 0.021722830832004547, -0.010757043026387691, 0.0029051348101347685, -0.058172788470983505, -0.004936838056892157, -0.05617297440767288, 0.09096142649650574, -0.015175219625234604, -0.03504681959748268, -0.022729303687810898, 0.04965706542134285, -0.008428631350398064, -0.004785993602126837, 0.082277812063694, -0.024112273007631302, 0.028438471257686615, 0.015748364850878716, 0.04883899912238121, 0.0003146482340525836, 0.04533080756664276, 0.03761376813054085, -0.03736526891589165, -0.03353480249643326, -0.0053030820563435555, 0.048079416155815125, 0.011480103246867657, 0.03469434008002281, 0.037868648767471313, -0.051860325038433075, -0.011703791096806526, 0.014780370518565178, 0.06666768342256546, -0.026971008628606796, 0.03627985715866089, -0.025688139721751213, -0.001992274774238467, -0.012065589427947998, -0.014754327945411205, -0.025659121572971344, 0.030361207202076912, -0.01364858541637659, 0.002350620925426483, -0.03896865248680115, -0.026622237637639046, 0.013429824262857437, 0.029357511550188065, -0.044812947511672974, 0.033309414982795715, 0.0768471211194992, 0.02138642780482769, -0.045720916241407394, -0.047838762402534485, 0.04248054325580597, 0.04269257187843323, -0.0615558847784996, -0.043198112398386, 2.3758143617322244e-34, 0.022422712296247482, -0.0586949922144413, 0.0031054348219186068, -0.01955108530819416, -0.011375792324543, 0.003436069702729583, 0.01663924753665924, 0.010805368423461914, 0.06752856075763702, 0.01783214509487152, -0.023399321362376213], [-0.05590800195932388, -0.018253611400723457, -0.0284926425665617, -0.007795252371579409, -0.004382930230349302, -0.009067066945135593, -0.04167887941002846, 0.019164996221661568, -0.03752436488866806, 0.04882456362247467, 0.025210363790392876, 0.06635590642690659, 0.01510770246386528, -0.00997775699943304, -0.021169524639844894, 0.03308534622192383, 0.03206852823495865, -0.0488690510392189, -0.03215903788805008, -0.036212626844644547, -0.04654013738036156, -0.08726409077644348, -0.017239857465028763, 0.028385447338223457, 0.05340786278247833, -0.021942198276519775, -0.03881334885954857, -0.00216828566044569, 0.019979998469352722, -0.04397917166352272, 0.009820179082453251, -0.026682524010539055, 0.020179500803351402, 0.021991657093167305, 1.6821722965687513e-06, -0.0343128964304924, -0.012335502542555332, -0.041896648705005646, 0.023274939507246017, 0.025042125955224037, -0.029050732031464577, 0.03940160945057869, -0.051826611161231995, -0.031072650104761124, 0.01695866510272026, 0.05169982090592384, -0.013791288249194622, 0.0038790793623775244, -0.03228962421417236, 0.011707116849720478, 0.015089105814695358, 0.008624979294836521, -0.04916192591190338, -0.07168831676244736, 0.005447824019938707, -0.09082087874412537, 0.04317304491996765, 0.012665173970162868, 0.015376301482319832, -0.031868547201156616, -0.03350147604942322, 0.0030793172772973776, 0.04513613134622574, 0.004957563243806362, -0.0058767045848071575, 0.02895914949476719, 0.050732407718896866, 0.05717790499329567, 0.0034633639734238386, -0.006435752846300602, -0.0015356841031461954, -0.05879107490181923, -0.012796454131603241, -0.009239796549081802, -0.03817228600382805, 0.013659973628818989, 0.02724384143948555, -0.00420087855309248, -0.01150751207023859, 0.02578338421881199, 0.017548423260450363, 0.07977394759654999, 0.002944061066955328, 0.010425366461277008, 0.03712141513824463, 0.06368670612573624, 0.015754876658320427, -0.06019015237689018, 0.044942013919353485, 0.04398144409060478, -0.03315495699644089, -0.01346183754503727, -0.03578037768602371, 0.028715351596474648, -0.02474362589418888, 0.017467666417360306, -0.005794387776404619, 0.0008317012689076364, -0.05717069283127785, 0.011119337752461433, 0.014827154576778412, -0.0354178249835968, 0.06908389180898666, 0.016856662929058075, 0.05203450098633766, 0.016200557351112366, -0.05418084189295769, 0.012214216403663158, 0.002177376765757799, 0.08549872040748596, 0.02086060307919979, -0.005829972680658102, 0.02033299393951893, -0.0005184906767681241, 0.024534573778510094, 0.021052759140729904, -0.05918465927243233, 0.020305199548602104, -0.02158576250076294, 0.041842956095933914, 0.002943861298263073, 0.02592756412923336, -0.07658478617668152, -0.014314587228000164, -0.07935705780982971, -0.021660324186086655, -0.014611074700951576, 0.010688870213925838, -0.0007655523950234056, 0.04302193969488144, 0.0347677543759346, 0.02602900192141533, -0.010700557380914688, -0.02121688425540924, -0.043332356959581375, 0.026994269341230392, -0.036816395819187164, 0.04164864495396614, 0.020705904811620712, 0.003916166722774506, -0.012689933180809021, -0.0054253097623586655, -0.0477813221514225, -0.06709682196378708, 0.009304594248533249, -0.01074261125177145, 0.021404879167675972, 0.007360404822975397, -0.003857246832922101, -0.03760874643921852, 0.027353966608643532, 0.01593949645757675, 0.08742307871580124, -0.019168132916092873, 0.04081336036324501, 0.018625393509864807, 0.00921960361301899, 0.05473173037171364, -0.030536601319909096, 0.04865967482328415, 0.030013076961040497, 0.05969446524977684, 0.040174685418605804, 0.007464258465915918, 0.0038380653131753206, 0.0072433240711688995, -0.015270432457327843, -0.03702855855226517, 0.011333734728395939, 0.002576718805357814, -0.04613811895251274, -0.0225792545825243, -0.041964635252952576, -0.014017265290021896, -0.001530419453047216, 0.006294365972280502, -0.019013069570064545, -0.09234505891799927, -0.06090772524476051, 0.011944166384637356, 0.025692956522107124, 0.018826503306627274, -0.04869008809328079, 0.0033277927432209253, 0.05006140097975731, 0.02017100341618061, 0.023769130930304527, -0.026989413425326347, -0.004928478039801121, -0.009664117358624935, -0.030190065503120422, -0.0763324722647667, -0.10498037934303284, -0.058920834213495255, 0.02557479217648506, 0.005772358272224665, -0.028901202604174614, 0.01228216104209423, 0.00541725754737854, 0.0814228281378746, 0.02302979677915573, -0.011113043874502182, 0.019351541996002197, -0.08639218658208847, 0.06963251531124115, -0.041730329394340515, 0.017382973805069923, 0.0021931370720267296, -0.0396910235285759, -0.03071299009025097, 0.0006248871213756502, -0.0009031763183884323, 0.06080210953950882, 0.029100319370627403, 0.0016805450432002544, 0.040562305599451065, -0.005705487448722124, 0.006661546882241964, 0.0003112927661277354, 0.005840524565428495, 0.01571064256131649, 0.036345917731523514, -0.028523115441203117, -0.035625506192445755, 0.02070462517440319, -0.0899665430188179, -0.025123782455921173, 0.016077876091003418, 0.04066277667880058, -0.0210818350315094, -0.06701634079217911, 0.00277720601297915, -0.03641119971871376, 0.04160798341035843, -0.00472704041749239, 0.026395972818136215, 0.03812302276492119, -0.012103176675736904, 0.029955683276057243, -0.03411264717578888, 0.03926519677042961, 0.017095647752285004, 0.0014297496527433395, 0.0006685068365186453, 0.009473891928792, 0.04698791354894638, -0.05188867077231407, 0.018434444442391396, 0.001314416527748108, 0.024293769150972366, 0.027478216215968132, 0.00822178740054369, 0.01925511658191681, 0.03094724752008915, 0.022855298593640327, -0.04194921255111694, 0.0006252498715184629, 0.01423335075378418, -0.10320402681827545, 0.014527685940265656, -0.043321333825588226, -0.04236423596739769, -0.04334690794348717, 0.002274330472573638, 0.07391411811113358, 0.08023221790790558, -0.012651847675442696, -0.013718600384891033, -0.015820210799574852, 0.019691767171025276, -0.002339832717552781, 0.010115467943251133, -0.03471791744232178, -0.013453139923512936, 0.04229748621582985, -0.0005470592877827585, -0.033985696732997894, -0.01811952516436577, -0.050013672560453415, 0.014566328376531601, 0.0035522726830095053, -0.013492796570062637, -0.005125388503074646, -0.010831056162714958, 0.003979249857366085, -0.022189917042851448, -0.016745949164032936, -0.03326140716671944, -0.005423408001661301, 0.02001894824206829, 0.002585434587672353, 0.056181445717811584, -0.009851302020251751, -0.015309920534491539, 0.03113689459860325, -0.038872309029102325, -0.044871389865875244, 0.06878957897424698, 0.01328841969370842, 0.02549482136964798, -0.039461083710193634, 0.07891622930765152, 0.006231564097106457, 0.012275462970137596, -0.023609846830368042, -0.008326376788318157, -0.03464822471141815, -0.021088730543851852, 0.08879974484443665, 0.06247905641794205, 0.0033599662128835917, -0.014183124527335167, 0.03178524598479271, -0.012753079645335674, -0.020110126584768295, -0.06897367537021637, -0.015803098678588867, 0.03543631732463837, -0.0668196976184845, -0.007231892552226782, -0.040729131549596786, -0.026234570890665054, 0.020250339061021805, 0.014776155352592468, 0.022678490728139877, -0.029896646738052368, 0.013721856288611889, 0.0110561428591609, -0.05455783009529114, 0.0707087442278862, 0.016112998127937317, -0.07164153456687927, 0.05219172313809395, 0.006995426956564188, 0.007218963000923395, 0.004310042597353458, -0.030493421480059624, -0.010292028076946735, 0.027616262435913086, 0.031804319471120834, -0.036794453859329224, -0.041245099157094955, 0.04024345427751541, -0.0026034617330878973, -0.006368616130203009, -0.08502000570297241, -0.012990685179829597, 0.02211202122271061, 0.07687938213348389, -0.04999510198831558, 0.05250127986073494, 0.010209831409156322, -0.04828450083732605, -0.028102710843086243, -0.058027882128953934, -0.04678535461425781, -0.013466112315654755, -0.033530380576848984, -0.04520728811621666, -0.0615411140024662, 0.023143025115132332, -0.011544798500835896, 0.05044625326991081, 0.07852353900671005, 0.016891270875930786, -0.007395305670797825, 0.03021494299173355, -0.006993499584496021, -0.0005409670411609113, 0.012718708254396915, 0.06896945834159851, 0.04876935854554176, 0.015876149758696556, 0.002943496685475111, -0.010302768088877201, -0.004493769258260727, 0.009964562021195889, -0.006069357972592115, 0.005017719231545925, -0.008958263322710991, 0.041692040860652924, -0.005329109262675047, 0.010258295573294163, -0.044996730983257294, 0.055816467851400375, -0.02169298566877842, 0.014733420684933662, 0.0029413083102554083, -0.007446060422807932, -0.043171659111976624, 0.003022218821570277, 0.011986945755779743, -0.03425091505050659, 0.07077200710773468, -0.022419637069106102, -0.042832355946302414, -0.027514401823282242, 0.025232652202248573, 0.03148941695690155, -0.0010987124405801296, 0.03644673898816109, 0.037837762385606766, 0.009083790704607964, -0.007142598275095224, -0.039502911269664764, 0.023398401215672493, 0.038775451481342316, 0.03644628822803497, -0.06755336374044418, -0.008503297343850136, 0.015860090032219887, -0.003068925580009818, 0.05745009332895279, 0.036604952067136765, -0.009017460979521275, -0.0026717069558799267, -0.035730089992284775, -0.016973067075014114, -0.03150869905948639, 0.04393816366791725, -0.03634629398584366, 0.01891891099512577, 0.0193942878395319, -0.009385393932461739, 0.021877799183130264, 0.01621948555111885, -0.02856631763279438, 0.010362607426941395, -0.04234318435192108, -0.056701261550188065, 0.00539441267028451, -0.022385580465197563, -0.04454793408513069, 0.024314817041158676, -0.03282104432582855, 0.007797392550855875, -0.01317087933421135, 0.006470903288573027, -0.03893205150961876, -0.022474130615592003, 0.010349232703447342, -0.06291362643241882, 0.14990901947021484, 0.003309538820758462, -0.020021386444568634, -0.008260340429842472, 0.043900053948163986, 0.023912711068987846, -0.0667102187871933, -0.03239976242184639, 0.0015966887585818768, 0.02321099303662777, 0.03865792602300644, -0.03508894518017769, -0.015782909467816353, -0.0670236349105835, -0.06378553807735443, -0.034905388951301575, -0.06134645640850067, 0.030255230143666267, 0.03477274253964424, 0.024083716794848442, 0.005941697861999273, -0.037323057651519775, -0.0210482906550169, 0.03390498086810112, -0.02567162737250328, 0.08798462152481079, 0.04841875657439232, 0.07339228689670563, 0.01228166464716196, 0.016665620729327202, 0.01384266559034586, 0.017986154183745384, -0.030202116817235947, -0.0438108816742897, 0.004806568846106529, 0.002511640079319477, -0.07373304665088654, -0.05668500438332558, -0.020194798707962036, 0.016354285180568695, 0.023410776630043983, 0.005960987880825996, -0.024413559585809708, 0.029664404690265656, 0.03474559634923935, 0.042525514960289, 0.02681802585721016, 0.04958989471197128, 0.06562985479831696, 0.022184593603014946, 0.012582274153828621, 0.0033142580650746822, 0.040024612098932266, -0.06460358202457428, 0.07369106262922287, 0.005340805742889643, 0.04537803679704666, 0.015696991235017776, -0.03822111338376999, -0.0009721869719214737, 0.013015818782150745, 0.04163632169365883, 0.012482436373829842, -0.01315609086304903, 0.03909057751297951, 0.006077336613088846, -0.010719694197177887, -0.021600650623440742, 0.024978768080472946, -0.06431756168603897, -0.022952748462557793, -0.060053419321775436, -0.030645467340946198, -0.017089275643229485, 0.025935523211956024, 0.03212154284119606, -0.009654405526816845, -0.04854367673397064, -0.010998602956533432, -0.013384475372731686, -0.008917344734072685, -0.007878008298575878, 0.05621115863323212, 0.004478994756937027, 0.029741503298282623, 0.008349275216460228, 0.02851267158985138, 0.014217814430594444, -0.012205862440168858, 0.012324590235948563, -0.07676617056131363, 0.036819469183683395, -0.004479896277189255, 0.033248286694288254, -0.03527968004345894, 0.005312968976795673, 0.02418767474591732, -0.021226493641734123, 0.004508784506469965, -0.02525993064045906, 0.0014504790306091309, 0.06073528155684471, 0.024196049198508263, -0.03451817110180855, -0.044491469860076904, -0.00958454329520464, -0.045506104826927185, 0.06475375592708588, -0.03239427134394646, -0.031094780191779137, 0.007355976849794388, -0.04537573456764221, 0.008400483056902885, -5.663563006330475e-33, -0.0038680480793118477, 0.007130263838917017, -0.038869909942150116, -0.02855837531387806, 0.028790844604372978, -0.038142818957567215, 0.005959724076092243, -0.03570789471268654, 0.03189259395003319, -0.021576402708888054, 0.010411777533590794, -0.007592968642711639, 0.02477349154651165, -0.006437456700950861, -0.019380496814846992, -0.05423709750175476, 0.0007477200124412775, -0.015120713971555233, -0.026255249977111816, -0.04236267879605293, 0.022719748318195343, -0.007214596960693598, 0.024573814123868942, 0.017092064023017883, 0.012367548421025276, 0.031776152551174164, -0.045911289751529694, 0.05301380902528763, -0.013935757800936699, 0.056633077561855316, -0.01549396850168705, -0.023790430277585983, -0.039921484887599945, -0.09013071656227112, 0.04176563769578934, -0.06836143136024475, 0.04809678718447685, -0.07535140961408615, 0.0032300082966685295, 0.02684503234922886, 0.01760978065431118, 0.00419400492683053, 0.006551182363182306, -0.011951620690524578, -0.0675504058599472, 0.017534589394927025, 0.0008730810950510204, 0.038196075707674026, 0.03592758998274803, -0.053488489240407944, 0.005259846802800894, -0.012739392928779125, 0.030581243336200714, 0.03909854590892792, -0.009476330131292343, 0.012425500899553299, 0.02495812065899372, -0.01862190291285515, 0.022354694083333015, -0.026312798261642456, 0.05854853615164757, 0.08978407829999924, 0.025366095826029778, -0.0027580242604017258, 0.059111934155225754, 0.03701077029109001, 0.05043121427297592, -0.0067232572473585606, -0.020607320591807365, -0.0514591783285141, -0.04720454663038254, -0.01824486255645752, -0.051680248230695724, 0.044707223773002625, -0.028401728719472885, -0.04050464928150177, -0.02656528912484646, 0.010828893631696701, 0.026140693575143814, -0.03037244826555252, 0.002169728511944413, 0.007008921355009079, -0.09553272277116776, 0.02194315381348133, -0.019317694008350372, -0.016611473634839058, 0.027299458160996437, 0.004190800711512566, -0.00962749682366848, 0.04163999482989311, 0.031621988862752914, 0.021221976727247238, -0.0233946293592453, -0.01197506207972765, 0.008525492623448372, -0.06290808320045471, 0.04702817648649216, -0.005428854841738939, -0.012779046781361103, -0.03534652665257454, -0.01819252036511898, -0.011708302423357964, -0.045289069414138794, -0.023474039509892464, 0.01709054969251156, 0.029501749202609062, -0.008854367770254612, -0.010359484702348709, 0.017280984669923782, 0.021966606378555298, 0.009531449526548386, -0.03715325519442558, 0.06410203874111176, -0.02581840753555298, 0.10339657962322235, -0.0518915057182312, -0.07565227895975113, 0.05229784548282623, 0.024283472448587418, 0.002363806124776602, 0.02692202478647232, 0.035499900579452515, -0.030422374606132507, 0.04772572964429855, -0.018970945850014687, 0.014902341179549694, 0.007037575356662273, 0.02453499846160412, -0.06879989802837372, -0.026486316695809364, -0.019975541159510612, 0.0012395113008096814, 2.5829194783000275e-07, 0.00515304459258914, -0.0251984354108572, 0.014971807599067688, 0.0732254907488823, 0.06350672245025635, 0.062118254601955414, -0.05604768171906471, 0.028263237327337265, 0.02580067329108715, -0.02956276759505272, -0.02502923086285591, 0.014389080926775932, -0.006519329268485308, -0.037651196122169495, 0.022941764444112778, -0.044249698519706726, 0.0390329509973526, 0.08924071490764618, 0.02819005958735943, -0.03328540921211243, 0.00665304996073246, 0.040709059685468674, 0.06771599501371384, -0.007047007791697979, -0.0028871323447674513, 0.0033222055062651634, 0.015559496358036995, 0.015296069905161858, -0.0065000057220458984, 0.026304000988602638, 0.0007724189781583846, -0.04899792745709419, -0.03237943351268768, -0.03899158164858818, 0.015213634818792343, 0.02405109442770481, 0.013893664814531803, 0.016901247203350067, 0.04597456008195877, -0.014664449729025364, -0.027364563196897507, 0.026260992512106895, 0.008161919191479683, -0.020528562366962433, 0.05329884588718414, -0.03715680539608002, 0.0022758778650313616, 0.03351673483848572, 0.02073233388364315, 0.049958959221839905, -0.006520917639136314, 0.07015715539455414, -0.022551381960511208, -0.01607973501086235, -0.056835971772670746, 0.05889296904206276, 0.012142352759838104, 0.03156362101435661, -0.002016838174313307, 0.04033801704645157, 0.01626279577612877, -0.08083269000053406, 0.01911184750497341, 0.028503697365522385, 0.039864856749773026, -0.07897292822599411, -0.039401207119226456, 2.1138304204787746e-34, -0.007573731709271669, 0.000737641763407737, 0.05085986480116844, -0.04135601595044136, 0.008312015794217587, -0.00035529935848899186, -0.004075502511113882, -0.018191823735833168, 0.043786536902189255, 0.06729286164045334, -0.01331151369959116], [-0.01751711219549179, 0.0008949381299316883, -0.030039502307772636, 0.023612847551703453, -0.020613914355635643, -0.015755336731672287, -0.003903692588210106, 0.015306017361581326, -0.0646728128194809, 0.009708328172564507, -0.03982706740498543, -0.0010766913183033466, -0.03176954388618469, 0.03665197268128395, 0.02266070246696472, -0.018700942397117615, 0.027861539274454117, 0.0152817377820611, -0.013449913822114468, -0.018720922991633415, -0.024301541969180107, -0.08039811998605728, -0.004917871672660112, 0.026103103533387184, -0.014388551004230976, 0.02692052163183689, -0.07684856653213501, 0.005713518708944321, 0.02418859302997589, -0.08394287526607513, 0.03917346149682999, -0.015001257881522179, 0.05772091820836067, 0.08561732620000839, 2.1116709376656218e-06, -0.03627653047442436, 0.01601872220635414, -0.02860417403280735, 0.04056277126073837, 0.03032016009092331, 0.00988346803933382, 0.054776839911937714, -0.06285315752029419, 0.010807452723383904, -0.03008643351495266, -0.03374218940734863, 0.03028259612619877, 0.012131113559007645, 0.04208148643374443, 0.04899780824780464, -0.0016237753443419933, -0.06700923293828964, 0.011163297109305859, -0.015048511326313019, 0.0006564006907865405, -0.051026903092861176, 0.030095694586634636, 0.0039941370487213135, 0.005082929972559214, -0.02584374137222767, -0.03156175836920738, 0.036108002066612244, 0.056992337107658386, -0.02765779383480549, -0.005670495331287384, 0.019854098558425903, -0.017505237832665443, -0.0351589061319828, -0.02063862793147564, -0.01753287948668003, -0.07617105543613434, 0.00599757581949234, 0.0028598320204764605, 0.008891839534044266, -0.023102767765522003, -0.08163778483867645, 0.002579760504886508, 0.015165366232395172, -0.03013906627893448, -0.023593543097376823, -0.0717930719256401, 0.034469589591026306, 0.01144719123840332, -0.012349464930593967, 0.002232967410236597, -0.008476714603602886, 0.010121031664311886, -0.005923915188759565, 0.030390292406082153, 0.004267449956387281, 0.03534764423966408, -0.010887161828577518, -0.005316589958965778, 0.004643579013645649, -0.028130127117037773, 0.011171212419867516, 0.003933771979063749, 0.01158408634364605, -0.016313957050442696, -0.03762055188417435, 0.06619134545326233, 0.002224656753242016, 0.034032803028821945, 0.01930965855717659, -0.05243026837706566, 0.03957439213991165, -0.10232618451118469, 0.009085363708436489, 0.0021365657448768616, 0.07049795240163803, -0.0836082175374031, -0.005467140581458807, -0.033628642559051514, 0.08141780644655228, 0.03290875628590584, -0.012202628888189793, -0.04485078901052475, -0.010618181899189949, -0.015132876113057137, 0.014628313481807709, -0.0764329731464386, 0.017744990065693855, -0.013072408735752106, 0.020653339102864265, -0.06585169583559036, 0.0041493303142488, -0.02753365971148014, -0.018055979162454605, 0.04837486147880554, 0.005673740990459919, -0.007140202447772026, -0.007475478574633598, -0.05333264172077179, 0.01227868627756834, 0.01803644932806492, 0.09119141101837158, 0.0051694526337087154, -0.033522944897413254, -0.07591954618692398, 0.043406032025814056, 0.03189541772007942, 0.010983224958181381, 0.05771136283874512, -0.015574664808809757, -0.019464785233139992, -0.04996412619948387, -0.003295812290161848, 0.039905447512865067, 0.004165126476436853, -0.025014135986566544, 0.02260807529091835, 0.06622485816478729, 0.020675059407949448, -0.048826687037944794, 0.049207232892513275, 0.013515393249690533, -0.03667444735765457, 0.06240428611636162, 0.009724806994199753, 0.018222400918602943, -0.036681920289993286, 0.0035749743692576885, -2.472065170877613e-05, -0.006070000119507313, 0.0008060258696787059, 0.01891500875353813, -0.036966171115636826, -0.053172044456005096, -0.020035788416862488, 0.1150297224521637, 0.035654373466968536, 0.0026488848961889744, -0.058130376040935516, -0.014332422986626625, 0.011859931983053684, 0.016271168366074562, 0.028298942372202873, 0.021968865767121315, 0.007722842041403055, -0.01385902613401413, 0.03988316282629967, 3.9116839616326615e-05, -0.0025351247750222683, 0.08477656543254852, 0.031498536467552185, -0.017735259607434273, -0.03781815245747566, 0.045436326414346695, -0.06351012736558914, -0.057871777564287186, 0.0364539809525013, -0.008794290944933891, -0.010159839875996113, -0.021504109725356102, 0.030117271468043327, 0.015309068374335766, -0.07460632920265198, -0.05739870294928551, 0.012629365548491478, 0.019796736538410187, 0.005126140080392361, -0.05368358641862869, 0.051438573747873306, 0.006828954443335533, 0.04447849839925766, -0.02488991804420948, -0.011661938391625881, -0.032514847815036774, -0.029965579509735107, 0.03687511011958122, 0.01939939148724079, 0.0016495048766955733, 0.016188692301511765, -0.03357016667723656, -0.02696525678038597, -0.05312284454703331, -0.024083303287625313, 0.10515855997800827, 0.008069893345236778, -0.011983047239482403, -0.01432171929627657, 0.07932047545909882, -5.9457801398821175e-05, -0.00019807895296253264, -0.015955636277794838, -0.02105611003935337, 0.00025665920111350715, -0.0058773052878677845, 0.03605547919869423, 0.009634310379624367, -0.0164776723831892, -0.016896896064281464, 0.032159071415662766, 0.00399109348654747, -0.0018899290589615703, -0.028479641303420067, -0.04140240699052811, -0.005324459634721279, 0.022574804723262787, -0.04691263288259506, 0.06041550636291504, 0.04613359645009041, -0.001667878939770162, -0.038829255849123, 0.029244352132081985, -0.008858142420649529, 0.021693481132388115, -0.007845992222428322, 0.018864456564188004, 0.003162527456879616, -0.004105889704078436, -0.002584723522886634, 0.014113005250692368, -0.00044598380918614566, -0.017674459144473076, -0.016167238354682922, 0.008146670646965504, 0.027480080723762512, -0.10897155851125717, 0.013416705653071404, 0.028197145089507103, -0.018135201185941696, -0.02525286003947258, 0.01165241003036499, -0.02389998733997345, -0.00692964531481266, 0.022432440891861916, 0.038168564438819885, -0.01914818584918976, -0.0162526685744524, 0.00565746147185564, 0.020519977435469627, -0.04701971262693405, 0.01984441466629505, -0.016215801239013672, 0.021773723885416985, 0.06796032190322876, -0.027175765484571457, -0.024373266845941544, 0.053822990506887436, 0.006426108069717884, -0.04793812707066536, -0.022680552676320076, -0.05454593524336815, 0.02324157953262329, 0.012374947778880596, 0.026176370680332184, -0.018831199035048485, -0.05094917491078377, -0.0509520098567009, 0.03679024055600166, 0.024034488946199417, -0.005607139319181442, -0.07101580500602722, 0.017785876989364624, 0.026454873383045197, -0.07476566731929779, 0.11100190132856369, 0.02227628231048584, 0.04760637506842613, 0.02109602652490139, 0.04055529460310936, 0.018639227375388145, 0.03526398167014122, -0.036875054240226746, -0.031173260882496834, -0.031085478141903877, 0.023703256621956825, -0.0011822665110230446, 0.10594889521598816, -0.03968051075935364, 0.03631910681724548, -0.08383599668741226, -0.02336980029940605, 0.023047221824526787, -0.07683701068162918, -0.025848524644970894, -0.009802151471376419, -0.05854182317852974, -0.038786210119724274, -0.023907262831926346, 0.006154133938252926, 0.032326843589544296, 0.026844171807169914, -0.0041599576361477375, -0.007257401943206787, 0.025981267914175987, 0.04846193641424179, 0.02990940771996975, 0.029361773282289505, 0.0195161122828722, -0.01679718680679798, -0.0012053773971274495, -0.008481482975184917, 0.0042646704241633415, -0.004106059204787016, 0.005063704214990139, -0.042657189071178436, -0.022055648267269135, -0.028253519907593727, -0.02613145112991333, -0.017070626839995384, -0.035275403410196304, 0.01982554793357849, 0.0021601980552077293, 0.027627011761069298, 0.010408611036837101, -0.030160313472151756, -0.0323440283536911, -0.04158557951450348, 0.047116804867982864, 0.0464789979159832, -0.02211504802107811, -0.019097479060292244, -0.060052528977394104, 0.017390906810760498, 0.02369462139904499, -0.0023689917288720608, 0.009779898449778557, -0.002280487446114421, 0.09936147928237915, 0.009572785347700119, 0.04109109938144684, 0.026549674570560455, -0.08886291831731796, 0.030807791277766228, -0.008533257059752941, -0.033162038773298264, -0.009317521937191486, 0.04562268778681755, 0.07058203220367432, -0.039037272334098816, -0.018585113808512688, -0.02971394918859005, -0.01780976913869381, -0.07657256722450256, -0.015251646749675274, 0.038586702197790146, -0.024149462580680847, 0.00780102564021945, 0.025281686335802078, 0.011778956279158592, -0.012006732635200024, -0.015430825762450695, -0.012821047566831112, 0.005622221622616053, 0.014279562048614025, 0.0470377579331398, -0.06002185493707657, 0.04245524853467941, 0.010697613470256329, -0.0513700433075428, -0.008469336666166782, -0.02708645910024643, -0.06894215196371078, -0.022774100303649902, 0.015436516143381596, 0.06344173848628998, 0.01954035647213459, -0.03199189901351929, 0.05615780130028725, 0.09920667856931686, 0.020942939445376396, 0.018526924774050713, -0.04553375020623207, 0.017682982608675957, 0.0749039500951767, 0.012370828539133072, -0.04729555547237396, -0.03381822258234024, 0.0066149672493338585, -0.012034127488732338, 0.05421369895339012, 0.02465994283556938, 0.06365938484668732, 0.0018041259609162807, 0.00776220066472888, 0.019091999158263206, 0.02864278107881546, 0.0003132256679236889, 0.02580263651907444, 0.005652561318129301, 0.009796014055609703, -0.02819833904504776, 0.010962745174765587, 0.009528456255793571, -0.018082115799188614, -0.00912396889179945, 0.03260532766580582, -0.007546140346676111, -0.00035060729715041816, -0.027213381603360176, -0.0025151625741273165, 0.016650531440973282, -0.07839619368314743, -0.00036451820051297545, -0.023904521018266678, 0.012327495962381363, -0.0019774427637457848, -0.03424171358346939, 0.016187794506549835, -0.028801480308175087, 0.050085216760635376, 0.024152614176273346, -0.07192843407392502, 5.762890577898361e-05, 0.03767707198858261, -0.025118131190538406, -0.027113959193229675, 0.0341043584048748, 0.03138069808483124, -0.06476237624883652, 0.015771297737956047, 0.0470844991505146, -0.04999923333525658, 0.004105850122869015, 0.009255259297788143, -0.05041728541254997, -0.010157025419175625, 0.06114315986633301, 0.07021080702543259, 0.012844323180615902, 0.02908387780189514, 0.013920415192842484, 0.006303579080849886, 0.025317978113889694, -0.022441139444708824, 0.03408277779817581, 0.017529791221022606, 0.04358888417482376, -0.005863285157829523, -0.028656573966145515, 0.008898249827325344, -0.0337032824754715, 0.007334826979786158, -0.006006528623402119, -0.01622907817363739, -0.014975369907915592, 0.01740187592804432, 0.012798752635717392, -0.06878560781478882, 0.09026558697223663, -0.020538505166769028, -0.0026968768797814846, 0.01621745154261589, 0.05484765022993088, 0.00861476082354784, 0.06153364107012749, -0.06021500751376152, -0.0224087443202734, 0.038256119936704636, -0.013431557454168797, 0.04053766280412674, -0.006399449892342091, 0.0760418102145195, -0.02795976772904396, 0.04012872278690338, 0.026525840163230896, 0.02702338993549347, 0.026523537933826447, -0.04704398661851883, 0.0078496178612113, 0.04134733974933624, 0.025786636397242546, -0.022265490144491196, 0.060580067336559296, 0.014529971405863762, 0.03114158846437931, 0.029564758762717247, -0.0117331026121974, -0.02108226902782917, 0.03427819162607193, -0.044012464582920074, -0.0023358892649412155, -0.017554625868797302, 0.01765485107898712, 0.09024489670991898, -0.03393560275435448, -0.01882350631058216, -0.038861602544784546, -0.0031574685126543045, -0.0012686534319072962, -0.04160666838288307, -0.023707572370767593, 0.04097782447934151, 0.039119526743888855, 0.032511502504348755, 0.017027366906404495, -0.008181542158126831, 0.02035454846918583, 0.018383627757430077, 0.05983254685997963, -0.0072099496610462666, 0.055321477353572845, -0.006369448266923428, 0.0011996692046523094, 0.02731073647737503, 0.07057654857635498, -0.026284826919436455, -0.06146840378642082, 0.027109431102871895, -0.009429216384887695, -0.007893284782767296, -0.0015330553287640214, 0.05360295623540878, -0.022540152072906494, 0.0558948740363121, -0.013229436241090298, 0.016685474663972855, -0.04785991832613945, 0.004739012103527784, -0.05629829689860344, 0.010831945575773716, 0.01786247082054615, 0.0023743989877402782, -5.865060003134693e-33, 0.010882952250540257, 0.025304315611720085, 0.036823805421590805, -0.027444934472441673, -0.009871267713606358, 0.002269600983709097, -0.016942692920565605, -0.047260452061891556, -0.01056432444602251, -0.017114076763391495, 0.00935334898531437, 0.005554105620831251, 0.018696649000048637, 0.005224625580012798, 0.005956449545919895, -0.0312805250287056, 0.03509821370244026, -0.08075621724128723, 0.006983826868236065, 0.002786774653941393, 0.012817704118788242, 0.0369817353785038, 0.010776807554066181, 0.0021955715492367744, -0.053627077490091324, 0.04673133045434952, -0.01629154570400715, 0.005406930577009916, -0.03555894270539284, 0.015841247513890266, -0.018737729638814926, -0.06673423200845718, -0.05109792575240135, -0.08118551224470139, 0.002689316403120756, -0.07230424135923386, -0.05731797590851784, -0.006256321910768747, -0.06621680408716202, 0.004134682007133961, -0.036125924438238144, -0.025610165670514107, 0.024801582098007202, 0.004806437063962221, -0.0765233039855957, -0.004193935543298721, 0.006741059012711048, -0.009253556840121746, -0.023946838453412056, -0.010166522115468979, -0.016389628872275352, 0.00762803852558136, 0.016253018751740456, -0.036205705255270004, 0.06445478647947311, -0.03920908644795418, -0.03134435787796974, 0.003940380644053221, -0.02053113654255867, 0.01830916479229927, -0.020517293363809586, 0.023913556709885597, -0.005061169620603323, 0.05848086252808571, 0.07964757084846497, 0.046028975397348404, 0.08651473373174667, 0.0105440029874444, -0.02678825333714485, -0.009080975316464901, -0.012257068417966366, -0.029816895723342896, 0.04569357633590698, -0.03452707827091217, 0.06914062798023224, -0.06465469300746918, 0.005152925383299589, -0.005226386245340109, 0.05311793088912964, 0.06260991841554642, 0.0011760428315028548, 0.038884807378053665, -0.01699705608189106, 0.010881680063903332, 0.0015452636871486902, -0.03864559903740883, 0.02013782039284706, -0.04122377932071686, 0.01762169413268566, 0.009630712680518627, 0.055834922939538956, -0.012827908620238304, -0.0022198574151843786, -0.024707667529582977, 0.10763809084892273, -0.018216164782643318, 0.00609950814396143, 0.0566706657409668, 0.03666697070002556, -0.021425927057862282, -0.018857359886169434, -0.014257186092436314, 0.010102595202624798, -0.010242201387882233, -0.02049395814538002, 0.016535161063075066, -0.027284210547804832, 0.09259821474552155, 0.009738985449075699, 0.012892636470496655, -0.03325243294239044, 0.006654422730207443, 0.07203589379787445, -0.0701516792178154, 0.01985466107726097, -0.026330377906560898, -0.004880486521869898, 0.014214538969099522, -0.009861046448349953, 0.018420403823256493, 0.004594285972416401, 0.0026650256477296352, -0.026257866993546486, 0.06397799402475357, -0.023515164852142334, -0.004479159135371447, -0.07959984242916107, 0.042033229023218155, -0.05544562637805939, -0.07462675869464874, -0.03027927875518799, 0.028224246576428413, 2.8759879455719783e-07, -0.0953870564699173, 0.0018883391749113798, 0.049735791981220245, -0.0014363793889060616, 0.018657812848687172, -0.004131076391786337, -0.04339029639959335, -0.01473852526396513, 0.0043892282992601395, -0.025623686611652374, 0.0287588182836771, 0.03789356350898743, 0.01803596317768097, -0.02298400178551674, -0.03684328868985176, -0.03956323489546776, 0.003782762913033366, 0.013838502578437328, 0.011891299858689308, -0.017917541787028313, -0.008415269665420055, 0.05958577245473862, 0.0474528893828392, -0.0013760020956397057, 0.0679466649889946, -0.02409602887928486, 0.014073386788368225, 0.03637554123997688, 0.03927953168749809, -0.031468894332647324, -0.009554577991366386, 0.016153814271092415, -0.03655559569597244, 0.0001101548841688782, -0.024093056097626686, -0.0033192806877195835, -0.0324658527970314, 0.02923036739230156, -0.029639575630426407, -0.04143446311354637, 0.014064846560359001, 0.0686643198132515, 0.031539831310510635, 0.005672719329595566, 0.01767086423933506, -0.010192624293267727, -0.029069282114505768, 0.011912360787391663, -0.019129032269120216, 0.002694298978894949, -0.01392381638288498, -0.0180291086435318, 0.023736143484711647, -0.027026813477277756, -0.05875283107161522, 0.023210076615214348, -0.009812263771891594, -0.04055442661046982, 0.04585861414670944, 0.018193187192082405, -0.010272880084812641, -0.03596477583050728, -0.08807055652141571, 0.0022973923478275537, 0.04438474029302597, -0.03265136107802391, -0.04845475032925606, 2.288366076814441e-34, 0.009816333651542664, -0.058278441429138184, 0.01801229827105999, -0.02017729915678501, -0.024692673236131668, 0.013355079106986523, 0.029652299359440804, 0.01334469486027956, 0.021896112710237503, -0.014139614999294281, -0.034570664167404175]], node_map={'3f4a67f5-5bc7-4af1-8a9e-4e5e024876de': Node(page_content='Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 0, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3f4a67f5-5bc7-4af1-8a9e-4e5e024876de', wins=19), 'bec2012e-c935-4494-84e6-4c83a1304cb3': Node(page_content='Estimates for the optimal action values can be learned\\nusing Q-learning (Watkins, 1989), a form of temporal dif-\\nference learning (Sutton, 1988). Most interesting problems\\nare too large to learn all action values in all states sepa-\\nrately. Instead, we can learn a parameterized value function\\nQ(s,a;θt). The standard Q-learning update for the param-\\neters after taking action Atin stateStand observing the\\nimmediate reward Rt+1and resulting state St+1is then\\nθt+1=θt+α(YQ\\nt−Q(St,At;θt))∇θtQ(St,At;θt).(1)\\nwhereαis a scalar step size and the target YQ\\ntis deﬁned as\\nYQ\\nt≡Rt+1+γmax\\naQ(St+1,a;θt). (2)\\nThis update resembles stochastic gradient descent, updating\\nthe current value Q(St,At;θt)towards a target value YQ\\nt.\\nDeep Q Networks\\nA deep Q network (DQN) is a multi-layered neural network\\nthat for a given state soutputs a vector of action values\\nQ(s,·;θ), where θare the parameters of the network. For\\nann-dimensional state space and an action space contain-\\ningmactions, the neural network is a function from Rnto\\nRm. Two important ingredients of the DQN algorithm as\\nproposed by Mnih et al. (2015) are the use of a target net-\\nwork, and the use of experience replay. The target network,\\nwith parameters θ−, is the same as the online network ex-\\ncept that its parameters are copied every τsteps from the\\nonline network, so that then θ−\\nt=θt, and kept ﬁxed on all\\nother steps. The target used by DQN is then\\nYDQN\\nt≡Rt+1+γmax\\naQ(St+1,a;θ−\\nt). (3)\\nFor the experience replay (Lin, 1992), observed transitions\\nare stored for some time and sampled uniformly from this\\nmemory bank to update the network. Both the target network\\nand the experience replay dramatically improve the perfor-\\nmance of the algorithm (Mnih et al., 2015).\\nDouble Q-learning\\nThe max operator in standard Q-learning and DQN, in (2)\\nand (3), uses the same values both to select and to evalu-\\nate an action. This makes it more likely to select overesti-\\nmated values, resulting in overoptimistic value estimates. To\\nprevent this, we can decouple the selection from the evalua-\\ntion. This is the idea behind Double Q-learning (van Hasselt,\\n2010).\\nIn the original Double Q-learning algorithm, two value\\nfunctions are learned by assigning each experience ran-\\ndomly to update one of the two value functions, such that\\nthere are two sets of weights, θandθ′. For each update, one\\nset of weights is used to determine the greedy policy and the\\nother to determine its value. For a clear comparison, we can\\nﬁrst untangle the selection and evaluation in Q-learning and\\nrewrite its target (2) as\\nYQ\\nt=Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θt).The Double Q-learning error can then be written as\\nYDoubleQ\\nt≡Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θ′\\nt).\\n(4)\\nNotice that the selection of the action, in the argmax , is\\nstill due to the online weights θt. This means that, as in Q-\\nlearning, we are still estimating the value of the greedy pol-\\nicy according to the current values, as deﬁned by θt. How-\\never, we use the second set of weights θ′\\ntto fairly evaluate\\nthe value of this policy. This second set of weights can be\\nupdated symmetrically by switching the roles of θandθ′.\\nOveroptimism due to estimation errors\\nQ-learning’s overestimations were ﬁrst investigated by\\nThrun and Schwartz (1993), who showed that if the action\\nvalues contain random errors uniformly distributed in an in', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 1, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='bec2012e-c935-4494-84e6-4c83a1304cb3', wins=17), '58b9d738-ba0c-403d-8f3a-2a5f4971ba3f': Node(page_content=' this\\ncauses higher estimation errors for unseen states, resulting\\nin higher overestimations. This is important because ﬂexi-\\nble parametric function approximators are often employed\\nin reinforcement learning (see, e.g., Tesauro 1995; Sallans\\nand Hinton 2004; Riedmiller 2005; Mnih et al. 2015).\\nIn contrast to van Hasselt (2010) we did not use a sta-\\ntistical argument to ﬁnd overestimations, the process to ob-\\ntain Figure 2 is fully deterministic. In contrast to Thrun and\\nSchwartz (1993), we did not rely on inﬂexible function ap-\\nproximation with irreducible asymptotic errors; the bottom\\nrow shows that a function that is ﬂexible enough to cover all\\nsamples leads to high overestimations. This indicates that\\nthe overestimations can occur quite generally.\\nIn the examples above, overestimations occur even when\\nassuming we have samples of the true action value at cer-\\ntain states. The value estimates can further deteriorate if we\\nbootstrap off of action values that are already overoptimistic,\\nsince this causes overestimations to propagate throughout\\nour estimates. Although uniformly overestimating values\\nmight not hurt the resulting policy, in practice overestima-\\ntion errors will differ for different states and actions. Over-\\nestimation combined with bootstrapping then has the perni-\\ncious effect of propagating the wrong relative information\\nabout which states are more valuable than others, directly\\naffecting the quality of the learned policies.\\nThe overestimations should not be confused with opti-\\nmism in the face of uncertainty (Sutton, 1990; Agrawal,\\n1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and\\nTennenholtz, 2003; Szita and L ˝orincz, 2008; Strehl et al.,\\n2009), where an exploration bonus is given to states or\\nactions with uncertain values. Conversely, the overestima-\\ntions discussed here occur only after updating, resulting in\\noveroptimism in the face of apparent certainty. This was al-\\nready observed by Thrun and Schwartz (1993), who noted\\nthat, in contrast to optimism in the face of uncertainty, these\\noverestimations actually can impede learning an optimal\\npolicy. We will see this negative effect on policy quality con-\\nﬁrmed later in the experiments as well: when we reduce the\\noverestimations using Double Q-learning, the policies im-\\nprove.\\n2We arbitrarily used the samples of action ai+5(fori≤5)\\norai−5(fori > 5) as the second set of samples for the double\\nestimator of action ai.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 2, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='58b9d738-ba0c-403d-8f3a-2a5f4971ba3f', wins=14), '3a3d16f1-c204-4708-a36f-2299250ba53a': Node(page_content='DQN Double DQN Double DQN (tuned)\\nMedian 47.5% 88.4% 116.7%\\nMean 122.0% 273.1% 475.2%\\nTable 2: Summary of normalized performance up to 30 minutes\\nof play on 49 games with human starts. Results for DQN are from\\nNair et al. (2015).\\nto allow for a controlled experiment focused just on re-\\nducing overestimations. The learned policies are evaluated\\nfor 5 mins of emulator time (18,000 frames) with an ϵ-\\ngreedy policy where ϵ= 0.05. The scores are averaged over\\n100 episodes. The only difference between Double DQN\\nand DQN is the target, using YDoubleDQN\\nt rather thanYDQN.\\nThis evaluation is somewhat adversarial, as the used hyper-\\nparameters were tuned for DQN but not for Double DQN.\\nTo obtain summary statistics across games, we normalize\\nthe score for each game as follows:\\nscore normalized =score agent−score random\\nscore human−score random. (5)\\nThe ‘random’ and ‘human’ scores are the same as used by\\nMnih et al. (2015), and are given in the appendix.\\nTable 1, under no ops , shows that on the whole Double\\nDQN clearly improves over DQN. A detailed comparison\\n(in appendix) shows that there are several games in which\\nDouble DQN greatly improves upon DQN. Noteworthy ex-\\namples include Road Runner (from 233% to 617%), Asterix\\n(from 70% to 180%), Zaxxon (from 54% to 111%), and\\nDouble Dunk (from 17% to 397%).\\nThe Gorila algorithm (Nair et al., 2015), which is a mas-\\nsively distributed version of DQN, is not included in the ta-\\nble because the architecture and infrastructure is sufﬁciently\\ndifferent to make a direct comparison unclear. For complete-\\nness, we note that Gorila obtained median and mean normal-\\nized scores of 96% and 495%, respectively.\\nRobustness to Human starts\\nOne concern with the previous evaluation is that in deter-\\nministic games with a unique starting point the learner could\\npotentially learn to remember sequences of actions with-\\nout much need to generalize. While successful, the solution\\nwould not be particularly robust. By testing the agents from\\nvarious starting points, we can test whether the found so-\\nlutions generalize well, and as such provide a challenging\\ntestbed for the learned polices (Nair et al., 2015).\\nWe obtained 100 starting points sampled for each game\\nfrom a human expert’s trajectory, as proposed by Nair et al.\\n(2015). We start an evaluation episode from each of these\\nstarting points and run the emulator for up to 108,000 frames\\n(30 mins at 60Hz including the trajectory before the starting\\npoint). Each agent is only evaluated on the rewards accumu-\\nlated after the starting point.\\nFor this evaluation we include a tuned version of Double\\nDQN. Some tuning is appropriate because the hyperparame-\\nters were tuned for DQN, which is a different algorithm. For\\nthe tuned version of Double DQN, we increased the num-\\nber of frames between each two copies of the target network\\nfrom 10,000 to 30,000, to reduce overestimations further be-\\ncause immediately after each switch DQN and Double DQN\\n0% 100% 200% 300% 400% 500%1000%1500%2000%2500%5000%7500%\\nNormalized score\\nHuman\\n∗∗Solaris∗∗Private EyeGravitarVentureMontezuma’s RevengeAsteroids∗∗Pitfall∗∗Ms. PacmanAmidar∗∗Yars Revenge∗∗AlienCentipedeBowling∗∗Skiing∗∗FrostbiteChopper CommandSeaquest∗∗Berzerk∗∗H.E.R.O.TutankhamIce HockeyBattle ZoneRiver Raid∗∗Surround∗∗Q*BertTennisFishing DerbyZaxxonPongFreewayBeam RiderBank HeistTime PilotName This GameWizard of WorKung-Fu MasterEnduroJames BondSpace InvadersUp and Down∗∗Phoenix∗∗∗∗Defender∗∗AsterixKangarooCrazy ClimberKrullRoad RunnerStar GunnerBoxingGopherRobotankDouble DunkAssaultBreakoutDemon AttackAtlantisVideo', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 5, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3a3d16f1-c204-4708-a36f-2299250ba53a', wins=17), 'abb026b9-5154-4765-b3ba-0680e0f9af45': Node(page_content='human, or even surpassing these.\\nDouble DQN appears more robust to this more challeng-\\ning evaluation, suggesting that appropriate generalizations\\noccur and that the found solutions do not exploit the deter-\\nminism of the environments. This is appealing, as it indi-\\ncates progress towards ﬁnding general solutions rather than\\na deterministic sequence of steps that would be less robust.\\nDiscussion\\nThis paper has ﬁve contributions. First, we have shown why\\nQ-learning can be overoptimistic in large-scale problems,\\neven if these are deterministic, due to the inherent estima-\\ntion errors of learning. Second, by analyzing the value es-\\ntimates on Atari games we have shown that these overesti-\\nmations are more common and severe in practice than pre-\\nviously acknowledged. Third, we have shown that Double\\nQ-learning can be used at scale to successfully reduce this\\noveroptimism, resulting in more stable and reliable learning.\\nFourth, we have proposed a speciﬁc implementation called\\nDouble DQN, that uses the existing architecture and deep\\nneural network of the DQN algorithm without requiring ad-\\nditional networks or parameters. Finally, we have shown that\\nDouble DQN ﬁnds better policies, obtaining new state-of-\\nthe-art results on the Atari 2600 domain.\\nAcknowledgments\\nWe would like to thank Tom Schaul, V olodymyr Mnih, Marc\\nBellemare, Thomas Degris, Georg Ostrovski, and Richard\\nSutton for helpful comments, and everyone at Google Deep-\\nMind for a constructive research environment.\\nReferences\\nR. Agrawal. Sample mean based index policies with O(log n) re-\\ngret for the multi-armed bandit problem. Advances in Applied\\nProbability , pages 1054–1078, 1995.\\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the\\nmultiarmed bandit problem. Machine learning , 47(2-3):235–\\n256, 2002.\\nL. Baird. Residual algorithms: Reinforcement learning with func-\\ntion approximation. In Machine Learning: Proceedings of the\\nTwelfth International Conference , pages 30–37, 1995.\\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The ar-\\ncade learning environment: An evaluation platform for general\\nagents. J. Artif. Intell. Res. (JAIR) , 47:253–279, 2013.\\nR. I. Brafman and M. Tennenholtz. R-max-a general polynomial\\ntime algorithm for near-optimal reinforcement learning. The\\nJournal of Machine Learning Research , 3:213–231, 2003.\\nK. Fukushima. Neocognitron: A hierarchical neural network ca-\\npable of visual pattern recognition. Neural networks , 1(2):119–\\n130, 1988.\\nL. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement\\nlearning: A survey. Journal of Artiﬁcial Intelligence Research ,\\n4:237–285, 1996.\\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based\\nlearning applied to document recognition. Proceedings of the\\nIEEE , 86(11):2278–2324, 1998.\\nL. Lin. Self-improving reactive agents based on reinforcement\\nlearning, planning and teaching. Machine learning , 8(3):293–\\n321, 1992.H. R. Maei. Gradient temporal-difference learning algorithms .\\nPhD thesis, University of Alberta, 2011.\\nV . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-\\nvski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,\\nD. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-\\nlevel control through deep reinforcement learning. Nature , 518\\n(7540):529–533, 2015.\\nA. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.\\nMaria, V . Panneershelvam, M. Suleyman, C. Beattie, S. Pe-', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 6, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='abb026b9-5154-4765-b3ba-0680e0f9af45', wins=20), '205d470b-7014-478d-8130-1ec0f0be88a6': Node(page_content='. Consider a state sin which all the true optimal action\\nvalues are equal at Q∗(s,a) =V∗(s). Suppose that the estimation\\nerrorsQt(s,a)−Q∗(s,a)are independently distributed uniformly\\nrandomly in [−1,1]. Then,\\nE[\\nmax\\naQt(s,a)−V∗(s)]\\n=m−1\\nm+ 1\\nProof. Deﬁneϵa=Qt(s,a)−Q∗(s,a); this is a uniform ran-\\ndom variable in [−1,1]. The probability that max aQt(s,a)≤x\\nfor somexis equal to the probability that ϵa≤xfor allasimul-\\ntaneously. Because the estimation errors are independent, we can\\nderive\\nP(max\\naϵa≤x) =P(X1≤x∧X2≤x∧...∧Xm≤x)\\n=m∏\\na=1P(ϵa≤x).\\nThe function P(ϵa≤x)is the cumulative distribution function\\n(CDF) ofϵa, which here is simply deﬁned as\\nP(ϵa≤x) =\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1\\n1+x\\n2ifx∈(−1,1)\\n1 ifx≥1\\nThis implies that\\nP(max\\naϵa≤x) =m∏\\na=1P(ϵa≤x)\\n=\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1(1+x\\n2)mifx∈(−1,1)\\n1 ifx≥1\\nThis gives us the CDF of the random variable max aϵa. Its expec-\\ntation can be written as an integral\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx,\\nwherefmaxis the probability density function of this variable, de-\\nﬁned as the derivative of the CDF: fmax(x) =d\\ndxP(max aϵa≤\\nx), so that for x∈[−1,1]we havefmax(x) =m\\n2(1+x\\n2)m−1.\\nEvaluating the integral yields\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx\\n=[(x+ 1\\n2)mmx−1\\nm+ 1]1\\n−1\\n=m−1\\nm+ 1.\\nExperimental Details for the Atari 2600\\nDomain\\nWe selected the 49 games to match the list used by Mnih et al.\\n(2015), see Tables below for the full list. Each agent step is com-\\nposed of four frames (the last selected action is repeated during\\nthese frames) and reward values (obtained from the Arcade Learn-\\ning Environment (Bellemare et al., 2013)) are clipped between -1\\nand 1.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 7, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='205d470b-7014-478d-8130-1ec0f0be88a6', wins=19), '9d2b58d5-997a-4b85-81c9-50ffb321603e': Node(page_content='Network Architecture\\nThe convolution network used in the experiment is exactly the one\\nproposed by proposed by Mnih et al. (2015), we only provide de-\\ntails here for completeness. The input to the network is a 84x84x4\\ntensor containing a rescaled, and gray-scale, version of the last four\\nframes. The ﬁrst convolution layer convolves the input with 32 ﬁl-\\nters of size 8 (stride 4), the second layer has 64 layers of size 4\\n(stride 2), the ﬁnal convolution layer has 64 ﬁlters of size 3 (stride\\n1). This is followed by a fully-connected hidden layer of 512 units.\\nAll these layers are separated by Rectiﬁer Linear Units (ReLu). Fi-\\nnally, a fully-connected linear layer projects to the output of the\\nnetwork, i.e., the Q-values. The optimization employed to train the\\nnetwork is RMSProp (with momentum parameter 0.95).\\nHyper-parameters\\nIn all experiments, the discount was set to γ= 0.99, and the learn-\\ning rate toα= 0.00025 . The number of steps between target net-\\nwork updates was τ= 10,000. Training is done over 50M steps\\n(i.e., 200M frames). The agent is evaluated every 1M steps, and\\nthe best policy across these evaluations is kept as the output of the\\nlearning process. The size of the experience replay memory is 1M\\ntuples. The memory gets sampled to update the network every 4\\nsteps with minibatches of size 32. The simple exploration policy\\nused is anϵ-greedy policy with the ϵdecreasing linearly from 1 to\\n0.1over 1M steps.\\nSupplementary Results in the Atari 2600\\nDomain\\nThe Tables below provide further detailed results for our experi-\\nments in the Atari domain.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 8, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='9d2b58d5-997a-4b85-81c9-50ffb321603e', wins=11)}, run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), node_filter=NodeFilter(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), threshold=7.5, context_scoring_prompt=Prompt(name='score_context', instruction='Given a context, complete the two following tasks and output answer valid json format\\n1.Evaluate the provided context and assign a numerical score between 0 and 10 based on the following criteria:\\n    - Award a high score to context that thoroughly delves into and explains concepts.\\n    - Assign a lower score to context that contains excessive references, acknowledgments, personal information, or other non-essential elements.', examples=[{'context': 'Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.', 'output': {'score': 6.0}}], input_keys=['context'], output_key='output', output_type='json', language='english')), question_filter=QuestionFilter(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), filter_question_prompt=Prompt(name='filter_question', instruction='\\nAsses the given question for clarity and answerability given enough domain knowledge, consider the following criteria:\\n1.Independence: Can the question be understood and answered without needing additional context or access to external references not provided within the question itself? Questions should be self-contained, meaning they do not rely on specific documents, tables, or prior knowledge not shared within the question.\\n2.Clear Intent: Is it clear what type of answer or information the question seeks? The question should convey its purpose without ambiguity, allowing for a direct and relevant response.\\nBased on these criteria, assign a verdict of \"1\" if a question is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. Assign \"0\" if it fails to meet one or more of these criteria due to vagueness, reliance on external references, or ambiguity in intent.\\nProvide feedback and a verdict in JSON format, including suggestions for improvement if the question is deemed unclear. Highlight aspects of the question that contribute to its clarity or lack thereof, and offer advice on how it could be reframed or detailed for better understanding and answerability.\\n', examples=[{'question': 'What is the discovery about space?', 'output': {'feedback': \"The question is too vague and broad, asking for a 'discovery about space' without specifying any particular aspect, time frame, or context of interest. This could refer to a wide range of topics, from the discovery of new celestial bodies to advancements in space travel technology. To improve clarity and answerability, the question could specify the type of discovery (e.g., astronomical, technological), the time frame (e.g., recent, historical), or the context (e.g., within a specific research study or space mission).\", 'verdict': '0'}}, {'question': \"How does ALMA-13B-R perform compared to other translation models in the WMT'23 study, based on the results in context1 and context2?\", 'output': {'feedback': \"This question asks for a comparison of the ALMA-13B-R model's performance against other translation models within the WMT'23 study, specifically referring to results in 'context1' and 'context2'. While it clearly specifies the model of interest (ALMA-13B-R) and the study (WMT'23), it assumes access to and understanding of 'context1' and 'context2' without explaining what these contexts entail. This makes the question unclear for those not familiar with the WMT'23 study or these specific contexts. To improve clarity and answerability for a broader audience, the question could benefit from defining or describing 'context1' and 'context2' or explaining the criteria used for comparison in these contexts.\", 'verdict': '0'}}, {'question': 'How do KIWI-XXL and XCOMET compare to the gold standard references in Table 1 in terms of evaluation scores, translation model performance, and success rate in surpassing the references?', 'output': {'feedback': \"The question requests a comparison between KIWI-XXL and XCOMET models and gold standard references in 'Table 1', focusing on evaluation scores, translation model performance, and success rates in surpassing the references. It specifies the models and criteria for comparison, making the intent clear. However, the question assumes access to 'Table 1' without providing its content or context, making it unclear for those without direct access to the source material. To be clearer and more answerable for a general audience, the question could include a brief description of the content or key findings of 'Table 1', or alternatively, frame the question in a way that does not rely on specific, unpublished documents.\", 'verdict': '0'}}, {'question': 'What is the configuration of UL2 training objective in OpenMoE and why is it a better choice for pre-training?', 'output': {'feedback': 'The question asks for the configuration of the UL2 training objective within the OpenMoE framework and the rationale behind its suitability for pre-training. It is clear in specifying the topic of interest (UL2 training objective, OpenMoE) and seeks detailed information on both the configuration and the reasons for its effectiveness in pre-training. However, the question might be challenging for those unfamiliar with the specific terminology or the context of OpenMoE and UL2. For broader clarity and answerability, it would be helpful if the question included a brief explanation or context about OpenMoE and the UL2 training objective, or clarified the aspects of pre-training effectiveness it refers to (e.g., efficiency, accuracy, generalization).', 'verdict': '1'}}, {'question': 'What is the detailed configuration of the UL2 training objective in OpenMoE, based on the provided context?', 'output': {'feedback': \"The question seeks detailed information on the UL2 training objective's configuration within the OpenMoE framework, mentioning 'the provided context' without actually including or describing this context within the query. This makes the question unclear for those who do not have access to the unspecified context. For the question to be clear and answerable, it needs to either include the relevant context directly within the question or be framed in a way that does not require external information. Detailing the specific aspects of the configuration of interest (e.g., loss functions, data augmentation techniques) could also help clarify the query.\", 'verdict': '0'}}], input_keys=['question'], output_key='output', output_type='json', language='english')), question_answer_prompt=Prompt(name='answer_formulate', instruction=\"Answer the question using the information from the given context. Output verdict as '1' if answer is present '-1' if answer is not present in the context.\", examples=[{'context': 'Climate change is significantly influenced by human activities, notably the emission of greenhouse gases from burning fossil fuels. The increased greenhouse gas concentration in the atmosphere traps more heat, leading to global warming and changes in weather patterns.', 'question': 'How do human activities contribute to climate change?', 'answer': {'answer': 'Human activities contribute to climate change primarily through the emission of greenhouse gases from burning fossil fuels. These emissions increase the concentration of greenhouse gases in the atmosphere, which traps more heat and leads to global warming and altered weather patterns.', 'verdict': '1'}}, {'context': 'The concept of artificial intelligence (AI) has evolved over time, but it fundamentally refers to machines designed to mimic human cognitive functions. AI can learn, reason, perceive, and, in some instances, react like humans, making it pivotal in fields ranging from healthcare to autonomous vehicles.', 'question': 'What are the key capabilities of artificial intelligence?', 'answer': {'answer': 'Artificial intelligence is designed to mimic human cognitive functions, with key capabilities including learning, reasoning, perception, and reacting to the environment in a manner similar to humans. These capabilities make AI pivotal in various fields, including healthcare and autonomous driving.', 'verdict': '1'}}, {'context': 'The novel \"Pride and Prejudice\" by Jane Austen revolves around the character Elizabeth Bennet and her family. The story is set in the 19th century in rural England and deals with issues of marriage, morality, and misconceptions.', 'question': \"What year was 'Pride and Prejudice' published?\", 'answer': {'answer': 'The answer to given question is not present in context', 'verdict': '-1'}}], input_keys=['context', 'question'], output_key='answer', output_type='json', language='english'), find_relevant_context_prompt=Prompt(name='find_relevant_context', instruction='Given a question and set of contexts, find the most relevant contexts to answer the question.', examples=[{'question': 'What is the capital of France?', 'contexts': ['1. France is a country in Western Europe. It has several cities, including Paris, Lyon, and Marseille. Paris is not only known for its cultural landmarks like the Eiffel Tower and the Louvre Museum but also as the administrative center.', '2. The capital of France is Paris. It is also the most populous city in France, with a population of over 2 million people. Paris is known for its cultural landmarks like the Eiffel Tower and the Louvre Museum.', '3. Paris is the capital of France. It is also the most populous city in France, with a population of over 2 million people. Paris is known for its cultural landmarks like the Eiffel Tower and the Louvre Museum.'], 'output': {'relevant_contexts': [1, 2]}}, {'question': 'How does caffeine affect the body and what are its common sources?', 'contexts': ['1. Caffeine is a central nervous system stimulant. It can temporarily ward off drowsiness and restore alertness. It primarily affects the brain, where it alters the function of neurotransmitters.', '2. Regular physical activity is essential for maintaining good health. It can help control weight, combat health conditions, boost energy, and promote better sleep.', '3. Common sources of caffeine include coffee, tea, cola, and energy drinks. These beverages are consumed worldwide and are known for providing a quick boost of energy.'], 'output': {'relevant_contexts': [1, 2]}}], input_keys=['question', 'contexts'], output_key='output', output_type='json', language='english'), rewrite_invalid_question_prompt=Prompt(name='rewrite_question', instruction='Given a context, question and feedback, rewrite the question to improve its clarity and answerability based on the feedback provided.', examples=[{'context': \"The Eiffel Tower was constructed using iron and was originally intended as a temporary exhibit for the 1889 World's Fair held in Paris. Despite its initial temporary purpose, the Eiffel Tower quickly became a symbol of Parisian ingenuity and an iconic landmark of the city, attracting millions of visitors each year. The tower's design, created by Gustave Eiffel, was initially met with criticism from some French artists and intellectuals, but it has since been celebrated as a masterpiece of structural engineering and architectural design.\", 'question': 'Who created the design for the Tower?', 'feedback': \"The question asks about the creator of the design for 'the Tower', but it does not specify which tower it refers to. There are many towers worldwide, and without specifying the exact tower, the question is unclear and unanswerable. To improve the question, it should include the name or a clear description of the specific tower in question.\", 'output': 'Who created the design for the Eiffel Tower?'}, {'context': \"'Exploring Zero-Shot Learning in Neural Networks' was published by Smith and Lee in 2021, focusing on the application of zero-shot learning techniques in artificial intelligence.\", 'question': 'What datasets were used for the zero-shot evaluations in this study?', 'feedback': \"The question asks about the datasets used for zero-shot evaluations in 'this study', without specifying or providing any details about the study in question. This makes the question unclear for those who do not have access to or knowledge of the specific study. To improve clarity and answerability, the question should specify the study it refers to, or provide enough context about the study for the question to be understood and answered independently.\", 'output': 'What datasets were used for the zero-shot evaluations Exploring Zero-Shot Learning in Neural Networks paper?'}], input_keys=['context', 'question', 'feedback'], output_key='output', output_type='string', language='english'), max_tries=5, is_async=True, compress_question_prompt=Prompt(name='compress_question', instruction='Rewrite the following question to make it more indirect and shorter while retaining the essence of the original question.\\n    The goal is to create a question that conveys the same meaning but in a less direct manner. The rewritten question should shorter so use abbreviation wherever possible.', examples=[{'question': 'What is the distance between the Earth and the Moon?', 'output': 'How far is the Moon from Earth?'}, {'question': 'What ingredients are required to bake a chocolate cake?', 'output': \"What's needed for a chocolate cake?\"}], input_keys=['question'], output_key='output', output_type='string', language='english'), multi_context_question_prompt=Prompt(name='multi_context_question', instruction=\"\\n    The task is to rewrite and complicate the given question in a way that answering it requires information derived from both context1 and context2. \\n    Follow the rules given below while rewriting the question.\\n        1. The rewritten question should not be very long. Use abbreviation wherever possible.\\n        2. The rewritten question must be reasonable and must be understood and responded by humans.\\n        3. The rewritten question must be fully answerable from information present in context1 and context2. \\n        4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\\n        5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\", examples=[{'question': 'What process turns plants green?', 'context1': 'Chlorophyll is the pigment that gives plants their green color and helps them photosynthesize.', 'context2': 'Photosynthesis in plants typically occurs in the leaves where chloroplasts are concentrated.', 'output': 'In which plant structures does the pigment responsible for their verdancy facilitate energy production?'}, {'question': 'How do you calculate the area of a rectangle?', 'context1': \"The area of a shape is calculated based on the shape's dimensions. For rectangles, this involves multiplying the length and width.\", 'context2': 'Rectangles have four sides with opposite sides being equal in length. They are a type of quadrilateral.', 'output': \"What multiplication involving equal opposites yields a quadrilateral's area?\"}], input_keys=['question', 'context1', 'context2'], output_key='output', output_type='string', language='english'))\n",
      "max retries exceeded for MultiContextEvolution(generator_llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), docstore=InMemoryDocumentStore(splitter=<langchain_text_splitters.base.TokenTextSplitter object at 0x000001C4341095A0>, nodes=[Node(page_content='Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 0, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3f4a67f5-5bc7-4af1-8a9e-4e5e024876de', wins=21), Node(page_content='Estimates for the optimal action values can be learned\\nusing Q-learning (Watkins, 1989), a form of temporal dif-\\nference learning (Sutton, 1988). Most interesting problems\\nare too large to learn all action values in all states sepa-\\nrately. Instead, we can learn a parameterized value function\\nQ(s,a;θt). The standard Q-learning update for the param-\\neters after taking action Atin stateStand observing the\\nimmediate reward Rt+1and resulting state St+1is then\\nθt+1=θt+α(YQ\\nt−Q(St,At;θt))∇θtQ(St,At;θt).(1)\\nwhereαis a scalar step size and the target YQ\\ntis deﬁned as\\nYQ\\nt≡Rt+1+γmax\\naQ(St+1,a;θt). (2)\\nThis update resembles stochastic gradient descent, updating\\nthe current value Q(St,At;θt)towards a target value YQ\\nt.\\nDeep Q Networks\\nA deep Q network (DQN) is a multi-layered neural network\\nthat for a given state soutputs a vector of action values\\nQ(s,·;θ), where θare the parameters of the network. For\\nann-dimensional state space and an action space contain-\\ningmactions, the neural network is a function from Rnto\\nRm. Two important ingredients of the DQN algorithm as\\nproposed by Mnih et al. (2015) are the use of a target net-\\nwork, and the use of experience replay. The target network,\\nwith parameters θ−, is the same as the online network ex-\\ncept that its parameters are copied every τsteps from the\\nonline network, so that then θ−\\nt=θt, and kept ﬁxed on all\\nother steps. The target used by DQN is then\\nYDQN\\nt≡Rt+1+γmax\\naQ(St+1,a;θ−\\nt). (3)\\nFor the experience replay (Lin, 1992), observed transitions\\nare stored for some time and sampled uniformly from this\\nmemory bank to update the network. Both the target network\\nand the experience replay dramatically improve the perfor-\\nmance of the algorithm (Mnih et al., 2015).\\nDouble Q-learning\\nThe max operator in standard Q-learning and DQN, in (2)\\nand (3), uses the same values both to select and to evalu-\\nate an action. This makes it more likely to select overesti-\\nmated values, resulting in overoptimistic value estimates. To\\nprevent this, we can decouple the selection from the evalua-\\ntion. This is the idea behind Double Q-learning (van Hasselt,\\n2010).\\nIn the original Double Q-learning algorithm, two value\\nfunctions are learned by assigning each experience ran-\\ndomly to update one of the two value functions, such that\\nthere are two sets of weights, θandθ′. For each update, one\\nset of weights is used to determine the greedy policy and the\\nother to determine its value. For a clear comparison, we can\\nﬁrst untangle the selection and evaluation in Q-learning and\\nrewrite its target (2) as\\nYQ\\nt=Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θt).The Double Q-learning error can then be written as\\nYDoubleQ\\nt≡Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θ′\\nt).\\n(4)\\nNotice that the selection of the action, in the argmax , is\\nstill due to the online weights θt. This means that, as in Q-\\nlearning, we are still estimating the value of the greedy pol-\\nicy according to the current values, as deﬁned by θt. How-\\never, we use the second set of weights θ′\\ntto fairly evaluate\\nthe value of this policy. This second set of weights can be\\nupdated symmetrically by switching the roles of θandθ′.\\nOveroptimism due to estimation errors\\nQ-learning’s overestimations were ﬁrst investigated by\\nThrun and Schwartz (1993), who showed that if the action\\nvalues contain random errors uniformly distributed in an in', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 1, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='bec2012e-c935-4494-84e6-4c83a1304cb3', wins=19), Node(page_content=' this\\ncauses higher estimation errors for unseen states, resulting\\nin higher overestimations. This is important because ﬂexi-\\nble parametric function approximators are often employed\\nin reinforcement learning (see, e.g., Tesauro 1995; Sallans\\nand Hinton 2004; Riedmiller 2005; Mnih et al. 2015).\\nIn contrast to van Hasselt (2010) we did not use a sta-\\ntistical argument to ﬁnd overestimations, the process to ob-\\ntain Figure 2 is fully deterministic. In contrast to Thrun and\\nSchwartz (1993), we did not rely on inﬂexible function ap-\\nproximation with irreducible asymptotic errors; the bottom\\nrow shows that a function that is ﬂexible enough to cover all\\nsamples leads to high overestimations. This indicates that\\nthe overestimations can occur quite generally.\\nIn the examples above, overestimations occur even when\\nassuming we have samples of the true action value at cer-\\ntain states. The value estimates can further deteriorate if we\\nbootstrap off of action values that are already overoptimistic,\\nsince this causes overestimations to propagate throughout\\nour estimates. Although uniformly overestimating values\\nmight not hurt the resulting policy, in practice overestima-\\ntion errors will differ for different states and actions. Over-\\nestimation combined with bootstrapping then has the perni-\\ncious effect of propagating the wrong relative information\\nabout which states are more valuable than others, directly\\naffecting the quality of the learned policies.\\nThe overestimations should not be confused with opti-\\nmism in the face of uncertainty (Sutton, 1990; Agrawal,\\n1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and\\nTennenholtz, 2003; Szita and L ˝orincz, 2008; Strehl et al.,\\n2009), where an exploration bonus is given to states or\\nactions with uncertain values. Conversely, the overestima-\\ntions discussed here occur only after updating, resulting in\\noveroptimism in the face of apparent certainty. This was al-\\nready observed by Thrun and Schwartz (1993), who noted\\nthat, in contrast to optimism in the face of uncertainty, these\\noverestimations actually can impede learning an optimal\\npolicy. We will see this negative effect on policy quality con-\\nﬁrmed later in the experiments as well: when we reduce the\\noverestimations using Double Q-learning, the policies im-\\nprove.\\n2We arbitrarily used the samples of action ai+5(fori≤5)\\norai−5(fori > 5) as the second set of samples for the double\\nestimator of action ai.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 2, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='58b9d738-ba0c-403d-8f3a-2a5f4971ba3f', wins=16), Node(page_content='DQN Double DQN Double DQN (tuned)\\nMedian 47.5% 88.4% 116.7%\\nMean 122.0% 273.1% 475.2%\\nTable 2: Summary of normalized performance up to 30 minutes\\nof play on 49 games with human starts. Results for DQN are from\\nNair et al. (2015).\\nto allow for a controlled experiment focused just on re-\\nducing overestimations. The learned policies are evaluated\\nfor 5 mins of emulator time (18,000 frames) with an ϵ-\\ngreedy policy where ϵ= 0.05. The scores are averaged over\\n100 episodes. The only difference between Double DQN\\nand DQN is the target, using YDoubleDQN\\nt rather thanYDQN.\\nThis evaluation is somewhat adversarial, as the used hyper-\\nparameters were tuned for DQN but not for Double DQN.\\nTo obtain summary statistics across games, we normalize\\nthe score for each game as follows:\\nscore normalized =score agent−score random\\nscore human−score random. (5)\\nThe ‘random’ and ‘human’ scores are the same as used by\\nMnih et al. (2015), and are given in the appendix.\\nTable 1, under no ops , shows that on the whole Double\\nDQN clearly improves over DQN. A detailed comparison\\n(in appendix) shows that there are several games in which\\nDouble DQN greatly improves upon DQN. Noteworthy ex-\\namples include Road Runner (from 233% to 617%), Asterix\\n(from 70% to 180%), Zaxxon (from 54% to 111%), and\\nDouble Dunk (from 17% to 397%).\\nThe Gorila algorithm (Nair et al., 2015), which is a mas-\\nsively distributed version of DQN, is not included in the ta-\\nble because the architecture and infrastructure is sufﬁciently\\ndifferent to make a direct comparison unclear. For complete-\\nness, we note that Gorila obtained median and mean normal-\\nized scores of 96% and 495%, respectively.\\nRobustness to Human starts\\nOne concern with the previous evaluation is that in deter-\\nministic games with a unique starting point the learner could\\npotentially learn to remember sequences of actions with-\\nout much need to generalize. While successful, the solution\\nwould not be particularly robust. By testing the agents from\\nvarious starting points, we can test whether the found so-\\nlutions generalize well, and as such provide a challenging\\ntestbed for the learned polices (Nair et al., 2015).\\nWe obtained 100 starting points sampled for each game\\nfrom a human expert’s trajectory, as proposed by Nair et al.\\n(2015). We start an evaluation episode from each of these\\nstarting points and run the emulator for up to 108,000 frames\\n(30 mins at 60Hz including the trajectory before the starting\\npoint). Each agent is only evaluated on the rewards accumu-\\nlated after the starting point.\\nFor this evaluation we include a tuned version of Double\\nDQN. Some tuning is appropriate because the hyperparame-\\nters were tuned for DQN, which is a different algorithm. For\\nthe tuned version of Double DQN, we increased the num-\\nber of frames between each two copies of the target network\\nfrom 10,000 to 30,000, to reduce overestimations further be-\\ncause immediately after each switch DQN and Double DQN\\n0% 100% 200% 300% 400% 500%1000%1500%2000%2500%5000%7500%\\nNormalized score\\nHuman\\n∗∗Solaris∗∗Private EyeGravitarVentureMontezuma’s RevengeAsteroids∗∗Pitfall∗∗Ms. PacmanAmidar∗∗Yars Revenge∗∗AlienCentipedeBowling∗∗Skiing∗∗FrostbiteChopper CommandSeaquest∗∗Berzerk∗∗H.E.R.O.TutankhamIce HockeyBattle ZoneRiver Raid∗∗Surround∗∗Q*BertTennisFishing DerbyZaxxonPongFreewayBeam RiderBank HeistTime PilotName This GameWizard of WorKung-Fu MasterEnduroJames BondSpace InvadersUp and Down∗∗Phoenix∗∗∗∗Defender∗∗AsterixKangarooCrazy ClimberKrullRoad RunnerStar GunnerBoxingGopherRobotankDouble DunkAssaultBreakoutDemon AttackAtlantisVideo', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 5, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3a3d16f1-c204-4708-a36f-2299250ba53a', wins=17), Node(page_content='human, or even surpassing these.\\nDouble DQN appears more robust to this more challeng-\\ning evaluation, suggesting that appropriate generalizations\\noccur and that the found solutions do not exploit the deter-\\nminism of the environments. This is appealing, as it indi-\\ncates progress towards ﬁnding general solutions rather than\\na deterministic sequence of steps that would be less robust.\\nDiscussion\\nThis paper has ﬁve contributions. First, we have shown why\\nQ-learning can be overoptimistic in large-scale problems,\\neven if these are deterministic, due to the inherent estima-\\ntion errors of learning. Second, by analyzing the value es-\\ntimates on Atari games we have shown that these overesti-\\nmations are more common and severe in practice than pre-\\nviously acknowledged. Third, we have shown that Double\\nQ-learning can be used at scale to successfully reduce this\\noveroptimism, resulting in more stable and reliable learning.\\nFourth, we have proposed a speciﬁc implementation called\\nDouble DQN, that uses the existing architecture and deep\\nneural network of the DQN algorithm without requiring ad-\\nditional networks or parameters. Finally, we have shown that\\nDouble DQN ﬁnds better policies, obtaining new state-of-\\nthe-art results on the Atari 2600 domain.\\nAcknowledgments\\nWe would like to thank Tom Schaul, V olodymyr Mnih, Marc\\nBellemare, Thomas Degris, Georg Ostrovski, and Richard\\nSutton for helpful comments, and everyone at Google Deep-\\nMind for a constructive research environment.\\nReferences\\nR. Agrawal. Sample mean based index policies with O(log n) re-\\ngret for the multi-armed bandit problem. Advances in Applied\\nProbability , pages 1054–1078, 1995.\\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the\\nmultiarmed bandit problem. Machine learning , 47(2-3):235–\\n256, 2002.\\nL. Baird. Residual algorithms: Reinforcement learning with func-\\ntion approximation. In Machine Learning: Proceedings of the\\nTwelfth International Conference , pages 30–37, 1995.\\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The ar-\\ncade learning environment: An evaluation platform for general\\nagents. J. Artif. Intell. Res. (JAIR) , 47:253–279, 2013.\\nR. I. Brafman and M. Tennenholtz. R-max-a general polynomial\\ntime algorithm for near-optimal reinforcement learning. The\\nJournal of Machine Learning Research , 3:213–231, 2003.\\nK. Fukushima. Neocognitron: A hierarchical neural network ca-\\npable of visual pattern recognition. Neural networks , 1(2):119–\\n130, 1988.\\nL. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement\\nlearning: A survey. Journal of Artiﬁcial Intelligence Research ,\\n4:237–285, 1996.\\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based\\nlearning applied to document recognition. Proceedings of the\\nIEEE , 86(11):2278–2324, 1998.\\nL. Lin. Self-improving reactive agents based on reinforcement\\nlearning, planning and teaching. Machine learning , 8(3):293–\\n321, 1992.H. R. Maei. Gradient temporal-difference learning algorithms .\\nPhD thesis, University of Alberta, 2011.\\nV . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-\\nvski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,\\nD. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-\\nlevel control through deep reinforcement learning. Nature , 518\\n(7540):529–533, 2015.\\nA. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.\\nMaria, V . Panneershelvam, M. Suleyman, C. Beattie, S. Pe-', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 6, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='abb026b9-5154-4765-b3ba-0680e0f9af45', wins=22), Node(page_content='. Consider a state sin which all the true optimal action\\nvalues are equal at Q∗(s,a) =V∗(s). Suppose that the estimation\\nerrorsQt(s,a)−Q∗(s,a)are independently distributed uniformly\\nrandomly in [−1,1]. Then,\\nE[\\nmax\\naQt(s,a)−V∗(s)]\\n=m−1\\nm+ 1\\nProof. Deﬁneϵa=Qt(s,a)−Q∗(s,a); this is a uniform ran-\\ndom variable in [−1,1]. The probability that max aQt(s,a)≤x\\nfor somexis equal to the probability that ϵa≤xfor allasimul-\\ntaneously. Because the estimation errors are independent, we can\\nderive\\nP(max\\naϵa≤x) =P(X1≤x∧X2≤x∧...∧Xm≤x)\\n=m∏\\na=1P(ϵa≤x).\\nThe function P(ϵa≤x)is the cumulative distribution function\\n(CDF) ofϵa, which here is simply deﬁned as\\nP(ϵa≤x) =\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1\\n1+x\\n2ifx∈(−1,1)\\n1 ifx≥1\\nThis implies that\\nP(max\\naϵa≤x) =m∏\\na=1P(ϵa≤x)\\n=\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1(1+x\\n2)mifx∈(−1,1)\\n1 ifx≥1\\nThis gives us the CDF of the random variable max aϵa. Its expec-\\ntation can be written as an integral\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx,\\nwherefmaxis the probability density function of this variable, de-\\nﬁned as the derivative of the CDF: fmax(x) =d\\ndxP(max aϵa≤\\nx), so that for x∈[−1,1]we havefmax(x) =m\\n2(1+x\\n2)m−1.\\nEvaluating the integral yields\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx\\n=[(x+ 1\\n2)mmx−1\\nm+ 1]1\\n−1\\n=m−1\\nm+ 1.\\nExperimental Details for the Atari 2600\\nDomain\\nWe selected the 49 games to match the list used by Mnih et al.\\n(2015), see Tables below for the full list. Each agent step is com-\\nposed of four frames (the last selected action is repeated during\\nthese frames) and reward values (obtained from the Arcade Learn-\\ning Environment (Bellemare et al., 2013)) are clipped between -1\\nand 1.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 7, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='205d470b-7014-478d-8130-1ec0f0be88a6', wins=19), Node(page_content='Network Architecture\\nThe convolution network used in the experiment is exactly the one\\nproposed by proposed by Mnih et al. (2015), we only provide de-\\ntails here for completeness. The input to the network is a 84x84x4\\ntensor containing a rescaled, and gray-scale, version of the last four\\nframes. The ﬁrst convolution layer convolves the input with 32 ﬁl-\\nters of size 8 (stride 4), the second layer has 64 layers of size 4\\n(stride 2), the ﬁnal convolution layer has 64 ﬁlters of size 3 (stride\\n1). This is followed by a fully-connected hidden layer of 512 units.\\nAll these layers are separated by Rectiﬁer Linear Units (ReLu). Fi-\\nnally, a fully-connected linear layer projects to the output of the\\nnetwork, i.e., the Q-values. The optimization employed to train the\\nnetwork is RMSProp (with momentum parameter 0.95).\\nHyper-parameters\\nIn all experiments, the discount was set to γ= 0.99, and the learn-\\ning rate toα= 0.00025 . The number of steps between target net-\\nwork updates was τ= 10,000. Training is done over 50M steps\\n(i.e., 200M frames). The agent is evaluated every 1M steps, and\\nthe best policy across these evaluations is kept as the output of the\\nlearning process. The size of the experience replay memory is 1M\\ntuples. The memory gets sampled to update the network every 4\\nsteps with minibatches of size 32. The simple exploration policy\\nused is anϵ-greedy policy with the ϵdecreasing linearly from 1 to\\n0.1over 1M steps.\\nSupplementary Results in the Atari 2600\\nDomain\\nThe Tables below provide further detailed results for our experi-\\nments in the Atari domain.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 8, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='9d2b58d5-997a-4b85-81c9-50ffb321603e', wins=13)], node_embeddings_list=[[-0.04956690967082977, 0.05030025169253349, -0.03161194548010826, -0.03288557380437851, 0.01843535155057907, 0.0030159135349094868, -0.04557950794696808, 0.003113736631348729, -0.03918442875146866, 0.044702835381031036, -0.046046625822782516, -0.023291850462555885, -0.04208846762776375, 0.04081600159406662, 0.0008428394212387502, -0.01272555347532034, 0.03858095780014992, -0.031005043536424637, -0.0021355892531573772, -0.0263868048787117, -0.019500315189361572, -0.06270435452461243, -0.004712102934718132, 0.03614285588264465, -0.008492016233503819, -0.008154899813234806, -0.022909270599484444, -0.029226038604974747, 0.023909516632556915, -0.04845501855015755, 0.016177255660295486, -0.01821986399590969, 0.03537508100271225, 0.07697879523038864, 2.149998636014061e-06, -0.024586323648691177, 0.0026486176066100597, -0.030079323798418045, 0.027112800627946854, 0.04406825825572014, -0.007023315876722336, 0.024078145623207092, -0.03997643664479256, -0.0002518778492230922, -0.01077247317880392, -0.041674334555864334, 0.04684838652610779, 0.027832284569740295, 0.040335237979888916, 0.01932237483561039, 0.025564471259713173, -0.01299844030290842, 0.007462881039828062, -0.00503364996984601, 0.014621518552303314, -0.08512124419212341, 0.033202532678842545, 0.038979317992925644, 0.023208150640130043, -0.07128992676734924, -0.019696498289704323, -0.008343297056853771, 0.039767321199178696, -0.004311467986553907, -0.010576926171779633, 0.034841056913137436, 0.03286511078476906, 0.03844849765300751, -0.009584137238562107, 0.02926979959011078, -0.11251521110534668, -0.003184487111866474, 0.014170180074870586, -0.018793893977999687, -0.01477824430912733, -0.05464312806725502, -0.00925841648131609, 0.004600195679813623, -0.010893245227634907, 0.0027479573618620634, -0.012223675847053528, 0.0073741739615798, 0.03547237440943718, -0.019527563825249672, -0.02712169475853443, 0.036739230155944824, -0.039569608867168427, -0.03296245262026787, 0.026563992723822594, 0.047920167446136475, -0.02188410796225071, -0.023699888959527016, -0.0228212121874094, 0.008442508056759834, -0.08602551370859146, -0.0032441981602460146, 0.017753038555383682, 0.01990649476647377, -0.053375206887722015, -0.03753526508808136, 0.04794248938560486, 0.00438083428889513, 0.0416092611849308, 0.032957226037979126, -0.023404056206345558, 0.0643588975071907, -0.07362734526395798, 0.016156217083334923, -0.014290009625256062, 0.057421308010816574, -0.019295191392302513, 0.022706398740410805, -0.043158549815416336, 0.06357996165752411, -0.010380125604569912, -0.027038097381591797, -0.05441286787390709, -0.01633065938949585, -0.06266791373491287, 0.028577269986271858, -0.025486286729574203, 0.05074427276849747, -0.01839800365269184, 0.03272087499499321, -0.06857135891914368, 0.005552635062485933, -0.06000499427318573, 0.035123344510793686, 0.027371956035494804, 0.013303566724061966, 0.0016372428508475423, -0.02401612512767315, -0.03464997187256813, -0.011425425298511982, 0.01611497439444065, 0.1212221309542656, -0.016620660200715065, 0.0573403425514698, -0.008569801226258278, 0.024873845279216766, 0.02887134440243244, -0.005614537745714188, 0.03259080648422241, -0.04940129816532135, 0.0037878442090004683, -0.0533355176448822, 0.016369109973311424, 0.006827794015407562, -0.00017759716138243675, -0.001276011927984655, 0.024550406262278557, 0.07139012217521667, 0.02180689573287964, -0.018303725868463516, 0.03832290694117546, 0.005785456858575344, -0.08122086524963379, 0.06939934939146042, 0.0708138644695282, 0.017690518870949745, -0.011429138481616974, 0.016395462676882744, 0.0366867296397686, 0.02167961373925209, -0.0002495442458894104, -0.015720680356025696, 0.012048636563122272, -0.056377165019512177, 0.018716612830758095, 0.09623261541128159, 0.025793397799134254, 0.0020899081137031317, -0.07367810606956482, 0.020922085270285606, 0.0321451835334301, -0.009429696947336197, 0.035609904676675797, 0.04683776572346687, 0.01547489408403635, -0.0019847729708999395, 0.003428798634558916, -0.032974973320961, -0.03405817225575447, 0.05185338854789734, 0.057354845106601715, -0.005111761391162872, -0.03181115910410881, 0.018379351124167442, -0.025368699803948402, -0.08989953994750977, 0.07761101424694061, -0.03392950817942619, -0.0010409398237243295, -0.005600721575319767, 0.01664034090936184, 0.011814948171377182, -0.08045651763677597, -0.07757017761468887, 0.007214331068098545, 0.08210252225399017, 0.00714770145714283, -0.0576181598007679, 0.032748397439718246, -0.013818069361150265, 0.0137260053306818, -0.08512803912162781, -0.009280887432396412, -0.04084613546729088, -0.06721779704093933, -0.024022048339247704, 0.025394370779395103, -0.03132794797420502, 0.02795223332941532, -0.0012632743455469608, -0.03879129886627197, -0.03045129030942917, -0.010286913253366947, 0.0739719569683075, 0.008769582025706768, 0.018274089321494102, -0.03925643488764763, 0.06059976667165756, -0.012500639073550701, -0.010989345610141754, -0.02826921083033085, -0.08887050300836563, 0.06163368374109268, -0.0034426336642354727, 0.017172494903206825, -0.03198494762182236, -0.000173825173988007, -0.005194282624870539, -0.006883100140839815, 0.021072372794151306, -0.017542852088809013, -0.04622218757867813, -0.0786595270037651, -0.002477356232702732, -0.0058600823394954205, -0.03545485436916351, 0.040372252464294434, 0.0399077869951725, -0.004569454118609428, -0.0010490784188732505, 0.025372540578246117, -0.017541198059916496, -0.026828192174434662, 0.026768334209918976, -0.023631805554032326, -0.007192598190158606, -0.003604464465752244, -6.169293919811025e-05, 0.043534401804208755, 0.0029465872794389725, -0.028786279261112213, 0.012549839913845062, -0.03656994551420212, -0.0390135757625103, -0.07188323140144348, 0.011403464712202549, 0.0004116089839953929, -0.0030171708203852177, 0.004657383542507887, 0.04503307119011879, -0.008829002268612385, -0.0074689555913209915, 0.03309345990419388, 0.02473643608391285, 0.02424710802733898, -0.037099551409482956, -0.005151750985532999, -0.0011958505492657423, -0.023298479616642, 0.007246533874422312, -0.019917519763112068, -0.003190400777384639, -0.009359658695757389, 0.01866842620074749, -0.007855579257011414, 0.04516812786459923, 0.04992367699742317, 0.03316927328705788, -0.008716070093214512, -0.03759509697556496, -0.010453482158482075, 0.006495311390608549, -0.029378529638051987, -0.012247872538864613, -0.007959171198308468, -0.00512451259419322, -0.012904105708003044, 0.03132719546556473, -0.01787458173930645, -0.012055667117238045, 0.015984466299414635, -0.030895309522747993, -0.026987392455339432, 0.09747907519340515, -0.017459619790315628, 0.023969298228621483, 0.03587264195084572, 0.0352080762386322, -0.037214674055576324, 0.07119585573673248, -0.02100016176700592, -0.046305280178785324, -0.019978828728199005, 0.019801659509539604, 0.03852659463882446, 0.08900782465934753, -0.06891528517007828, 0.03329094126820564, -0.031006095930933952, 0.05408318340778351, 0.010144073516130447, -0.07400228083133698, -0.009552961215376854, 0.008767182938754559, -0.03414269909262657, -0.0047276816330850124, -0.04157065600156784, 0.0026749989483505487, 0.06187150999903679, 0.07103555649518967, 0.009358062408864498, 0.017061466351151466, 0.005598212592303753, 0.05317479372024536, 0.014188873581588268, 0.03242873400449753, 0.018814990296959877, -0.0616469606757164, -0.019207686185836792, -0.050389986485242844, -0.034628454595804214, -0.010077671147882938, -0.03193289041519165, 0.011392203159630299, -0.0039245192892849445, -0.02092493698000908, -0.005906561855226755, -0.05125012621283531, -0.012407233007252216, 0.03443283215165138, -0.017866019159555435, 0.01250829454511404, -0.027765337377786636, -0.009294365532696247, -0.024218088015913963, -0.052021682262420654, 0.04773879051208496, 0.022135594859719276, -0.02410902827978134, -0.017023401334881783, -0.026226451620459557, 0.011308490298688412, 0.020139852538704872, -0.006132879760116339, 0.03974349796772003, -0.06352242082357407, 0.07311461120843887, -0.01773030310869217, 0.03676407039165497, 0.006706724874675274, -0.00113665999379009, 0.014348911121487617, -0.026536082848906517, -0.0308107640594244, -0.034079134464263916, 0.052283644676208496, 0.04096300154924393, -0.03515404462814331, -0.04475938901305199, -0.01596837118268013, -0.020565446466207504, -0.01847333461046219, 0.019493091851472855, 0.008841834962368011, 0.02457306534051895, -0.01257703360170126, 0.0632074698805809, -0.05528101325035095, -0.020462218672037125, -0.00772258872166276, 0.014005635865032673, -5.446333307190798e-05, -0.024307800456881523, 0.013770432211458683, -0.07240741699934006, 0.0011185075854882598, 0.0038715829141438007, -0.04053213447332382, -0.05016007646918297, 0.03667386621236801, -0.04187147319316864, -0.04028069227933884, 0.014091952703893185, 0.030159082263708115, 0.018237927928566933, -0.025925692170858383, 0.04533470794558525, 0.01699031889438629, 0.05208983272314072, -0.012026390992105007, -0.06956001371145248, 0.011980989016592503, 0.05501585826277733, 0.017857961356639862, -0.03165769577026367, -0.005820938386023045, -0.042281366884708405, 0.02048397809267044, 0.006348191760480404, 0.07693615555763245, 0.01606963574886322, -0.011864441446959972, 0.008255308493971825, 0.026007264852523804, 0.010685653425753117, 0.04990028217434883, 0.02351139485836029, 0.031110204756259918, 0.04332878813147545, -0.012153967283666134, -0.03794131428003311, 0.025629857555031776, -0.018776461482048035, 0.012083655223250389, -0.0005816958728246391, -0.025191569700837135, 0.0008335255552083254, -0.07716438919305801, -0.02297556772828102, -0.02814149670302868, -0.088962122797966, 1.7185151591547765e-05, -0.020434973761439323, 0.016032952815294266, 0.005323451012372971, -0.03690676391124725, 0.0011500546243041754, -0.036443114280700684, 0.018618030473589897, -0.0176237840205431, -0.013270661234855652, -0.006224134471267462, 0.06981797516345978, -0.0005877199582755566, -0.048915762454271317, -0.002769813872873783, 0.029081476852297783, -0.008111154660582542, 0.05358076095581055, -0.006325608119368553, -0.049744926393032074, -0.0161903016269207, -0.02019103616476059, -0.004875012673437595, 0.009113934822380543, 0.023439906537532806, 0.07227034121751785, 0.008071894757449627, 0.036758240312337875, -0.005599034950137138, -0.013699065893888474, 0.041046466678380966, 0.05880441516637802, 0.022640852257609367, -0.032338712364435196, 0.07543806731700897, 0.0014054124476388097, 0.00021080787701066583, 0.028200365602970123, -0.06495729833841324, -0.0009477616986259818, -0.007826036773622036, 0.026593919843435287, -0.04710095375776291, 0.030852418392896652, -0.018303919583559036, -0.05774788185954094, 0.04364979639649391, 0.013156036846339703, -0.018749352544546127, 0.0034918866585940123, 0.010379191488027573, 0.02955382876098156, 0.010919143445789814, -0.04005775600671768, 0.0030696613248437643, 0.04186369106173515, 0.02955406717956066, 0.0031217450741678476, -0.004175861831754446, 0.10254082828760147, -0.03302936255931854, -0.013329079374670982, 0.026858331635594368, 0.05360277369618416, 0.0335363894701004, -0.006293140351772308, -0.0051373643800616264, 0.028665807098150253, -0.02126343548297882, 0.03197021037340164, 0.04389636591076851, 0.03102790378034115, -0.015325527638196945, 0.0203153807669878, -0.01759224198758602, -0.014048167504370213, -0.01878613419830799, -0.054337628185749054, -0.08147112280130386, 0.006435368675738573, 0.030237050727009773, 0.09055858850479126, 0.0062395185232162476, -0.04611249268054962, -0.017497709020972252, 0.002797875553369522, -0.0017475589411333203, -0.054203033447265625, 0.008608764968812466, -0.0035609116312116385, 0.030582701787352562, 0.028722237795591354, -0.00027848099125549197, -0.01907588727772236, 0.01941657066345215, 0.020659247413277626, 0.0591428242623806, -0.038499992340803146, 0.018652664497494698, -0.006281610112637281, -0.04479268938302994, 0.06129003316164017, 0.04852548614144325, -0.027460653334856033, -0.05722035467624664, 0.020719166845083237, -0.004362539388239384, 0.0004621725529432297, 0.07286541163921356, 0.02440125308930874, -0.05857822299003601, 0.02733508124947548, -0.040851153433322906, 0.001233183196745813, -0.010925082489848137, -0.044086117297410965, -0.019097426906228065, -0.028419246897101402, 0.0009672532323747873, 0.018251528963446617, -5.2393660333690824e-33, 0.005733703728765249, 0.0327339842915535, 0.013665182515978813, -0.013834282755851746, -0.03953896835446358, 0.012471248395740986, -0.008237186819314957, -0.06624552607536316, 0.012249778024852276, 0.01691672019660473, -0.022888822481036186, 0.006525000091642141, 0.005824986379593611, -0.01810799352824688, 0.009818199090659618, -0.0665813460946083, 0.03861081972718239, -0.03042105957865715, -0.037292104214429855, 0.04174313321709633, -0.009261527098715305, 0.03208363801240921, 0.04066682606935501, 0.014413449913263321, -0.030452514067292213, 0.021000316366553307, -0.030024368315935135, 0.01764131709933281, -0.015565648674964905, 0.035078324377536774, -0.013481101021170616, -0.018075278028845787, -0.045937031507492065, -0.08943227678537369, 0.008996015414595604, -0.00177675555460155, -0.053804680705070496, -0.008313515223562717, -0.05668061971664429, 0.018642116338014603, -0.008951383642852306, -0.0412890799343586, 0.03725530207157135, 0.053809188306331635, -0.08073937147855759, 0.005439059343189001, 0.004016018472611904, 0.008666821755468845, -0.0027602296322584152, -0.08467312902212143, -0.007249248679727316, -0.001326903817243874, 0.016480321064591408, -0.02427643910050392, 0.0422847680747509, 0.06903411448001862, -0.017198549583554268, 0.007142256945371628, -0.032724928110837936, -0.01114514097571373, 0.007348882034420967, 0.007590506225824356, -0.0009148311801254749, 0.08791342377662659, 0.08708048611879349, 0.03475499898195267, 0.04031061381101608, -0.0031139147467911243, -0.04015132784843445, 0.009228046983480453, -0.007614428643137217, 0.012225717306137085, 0.030473385006189346, -0.01602003164589405, 0.08681568503379822, 0.004674796015024185, -0.036929965019226074, 0.035452283918857574, 0.013656810857355595, 0.07004045695066452, 0.014242538250982761, -0.0032166766468435526, -0.018858937546610832, 0.0038062664680182934, -0.009649226441979408, -0.015989605337381363, 0.015520875342190266, -0.01825098507106304, -0.009923762641847134, 0.006572339218109846, 0.03542119637131691, 0.019002918154001236, -0.000930476700887084, -0.03828636556863785, 0.05465613678097725, -0.01938931830227375, 0.0027082867454737425, 0.03808492049574852, -0.014031925238668919, 0.005083836615085602, -0.034158360213041306, -0.06560160964727402, -0.015721118077635765, -0.02117462269961834, -0.005181961227208376, 0.02705833502113819, -0.02057328261435032, 0.011634482070803642, -0.0025265796575695276, 0.01984385959804058, -0.008347034454345703, 0.020092995837330818, 0.07017629593610764, -0.02701658196747303, 0.044486694037914276, -0.04864874854683876, 0.016991257667541504, 0.007083837408572435, -0.00965020339936018, 0.024995123967528343, 0.00997256487607956, -0.018661057576537132, -0.039940718561410904, 0.034596625715494156, -0.013127707876265049, -0.027812914922833443, -0.1137981191277504, 0.03329765051603317, -0.0954248309135437, -0.08871246874332428, -0.028346283361315727, 0.002901082392781973, 2.830523726515821e-07, -0.06526628881692886, 0.02176315151154995, 0.030600890517234802, 0.010186759755015373, 0.006753704976290464, 0.03764456883072853, -0.01513525377959013, 0.015110800042748451, -0.014872671104967594, 0.02616477757692337, 0.0769086629152298, 0.042842019349336624, 0.004197615198791027, -0.005126357544213533, -0.05055604875087738, -0.032553959637880325, -0.05483115836977959, 0.05333319306373596, 0.0027884861920028925, -0.023650875315070152, -0.029190247878432274, 0.04392814636230469, 0.0030022342689335346, -0.017225341871380806, 0.07329633831977844, -0.00618797168135643, 0.02890566550195217, -0.01015797071158886, 0.04330960661172867, 0.005862101446837187, 0.016835445538163185, 0.018966244533658028, -0.03767523169517517, -0.05417787656188011, -0.020708922296762466, 0.042562227696180344, -0.006089566275477409, 0.04329114034771919, 0.006681707687675953, -0.032713744789361954, -0.04041823372244835, 0.03448858857154846, 0.07394971698522568, -0.012641963548958302, 0.04563938453793526, -0.056690141558647156, 0.02270055189728737, -0.006796339526772499, -0.04671710729598999, -0.006431576795876026, 0.02362627349793911, 0.0031040781177580357, -0.01609502173960209, -0.05438423529267311, -0.023288927972316742, 0.028326116502285004, 0.03590812906622887, -0.03162478283047676, 0.015953650698065758, 0.08908253163099289, 0.028270892798900604, -0.029228312894701958, -0.044985976070165634, 0.022145522758364677, 0.02303995192050934, -0.05817921459674835, -0.024420034140348434, 1.8514450433127441e-34, 0.01697324961423874, -0.06399133801460266, 0.009780898690223694, -0.038277916610240936, -0.012614056468009949, 0.004733836278319359, 0.03986188396811485, 0.010966276749968529, 0.0971110463142395, 0.037209633737802505, -0.03184156119823456], [-0.04602940008044243, 0.019681455567479134, -0.01960614137351513, -0.041058249771595, -0.024524137377738953, -0.009800859726965427, -0.053674984723329544, -0.001083041075617075, -0.038948751986026764, 0.036183033138513565, -0.02878202497959137, -0.003416056977584958, -0.02670563943684101, 0.05225274711847305, 0.016610287129878998, -0.04243627190589905, 0.04263217747211456, 0.001855393871665001, -0.043818097561597824, -0.0031126905232667923, -0.014428502880036831, -0.0765562579035759, -0.0329836830496788, 0.03188700973987579, 0.009657010436058044, -0.029219139367341995, -0.07348824292421341, -0.040029339492321014, 0.05639645829796791, -0.0542156845331192, 0.021953236311674118, -0.03277580440044403, 0.025720426812767982, 0.062254466116428375, 1.995331558646285e-06, -0.0006518788286484778, 0.00286106183193624, -0.06626146286725998, -0.0029985804576426744, -0.023997310549020767, -0.035406019538640976, 0.04237125441431999, -0.03611861914396286, 0.024609211832284927, -0.027215858921408653, -0.007935874164104462, 0.05315399542450905, 0.021626750007271767, 0.04986194893717766, 0.06225023418664932, 0.01645629294216633, 0.010575380176305771, 0.004307347349822521, -0.027056006714701653, 0.014077995903789997, -0.052946221083402634, 0.0444154366850853, 0.008943727239966393, 0.019241059198975563, -0.03595568612217903, -0.012328031472861767, -0.02452894113957882, 0.03279603645205498, -0.014652584679424763, 0.0004571707977447659, 0.0383283905684948, 0.08476120978593826, 0.038770660758018494, -0.0005641364841721952, 0.012863718904554844, -0.07837529480457306, 0.012828600592911243, 0.017830919474363327, -0.015522832982242107, -0.03162357956171036, -0.036969754844903946, 0.01997237466275692, -0.005835445132106543, 0.0002132725203409791, 0.00550698721781373, -0.004788583610206842, 0.024582456797361374, -0.00012291091843508184, 0.020692814141511917, -4.3039166484959424e-05, 0.02308695763349533, -0.0320698618888855, -0.022110287100076675, 0.009809519164264202, -0.015153222717344761, -0.0047020879574120045, -0.025256728753447533, -0.0022235240321606398, 0.03901924565434456, -0.04081115126609802, -0.008353509940207005, 0.03582862764596939, -0.005574747454375029, -0.059680767357349396, -0.022990845143795013, 0.05063652619719505, 0.021203162148594856, 0.035883959382772446, 0.01282994169741869, -0.015058581717312336, 0.07177269458770752, -0.04124896228313446, -0.005817510187625885, -0.0062251016497612, 0.09383376687765121, -0.08054083585739136, 0.015647802501916885, -0.04257864132523537, 0.0686333104968071, 0.022665422409772873, -0.003193650860339403, -0.07468482851982117, 0.02502225525677204, -0.07945992797613144, 0.017783762887120247, -0.03340540826320648, 0.011141810566186905, -0.04612677916884422, 0.0201744195073843, -0.0852399691939354, -0.010669015347957611, -0.03079591877758503, 0.028458978980779648, -0.000528671604115516, 0.0388391837477684, -0.0009701951057650149, -0.015328388661146164, 0.023009728640317917, -0.01386967021971941, -0.008696314878761768, 0.09185249358415604, 0.0091159762814641, 0.021785562857985497, -0.027549097314476967, 0.008595976047217846, 0.03823503106832504, 0.017480013892054558, 0.04315800964832306, -0.023751812055706978, -0.010298914276063442, -0.06416524946689606, -0.014956229366362095, -0.0014385979156941175, -0.00721023790538311, -0.01303795538842678, 0.011816177517175674, 0.04091491177678108, 0.08507107198238373, -0.016331162303686142, 0.05409306287765503, -0.03148699924349785, -0.028595605865120888, 0.0665159523487091, 0.022775964811444283, 0.05177345126867294, -0.026938829571008682, 0.02166542410850525, 0.057304296642541885, 0.027249839156866074, 0.002089919988065958, 0.041894931346178055, -0.027614543214440346, -0.07695085555315018, 0.028845172375440598, 0.07652207463979721, 0.014765172265470028, -0.012056687846779823, -0.10233362019062042, 0.023894578218460083, 0.04293539747595787, 0.02459724433720112, 0.03871751204133034, -0.009708656929433346, 0.014595244079828262, -0.008089609444141388, 0.0040893093682825565, -0.03657882660627365, -0.024931160733103752, 0.08238127827644348, 0.04457470029592514, -0.010702704079449177, -0.06502632051706314, -0.018393751233816147, -0.035324957221746445, -0.07015643268823624, 0.03992270678281784, -0.026652518659830093, -0.00013389080413617194, 0.0005359531496651471, 0.02364419959485531, 0.003294995753094554, -0.06692832708358765, -0.045120175927877426, -0.0019163774559274316, 0.08804183453321457, 0.01307318639010191, -0.02056790143251419, 0.03137074038386345, 0.0033537012059241533, 0.00904708169400692, -0.049341920763254166, -0.03688247874379158, -0.059433627873659134, -0.053112588822841644, -0.017550239339470863, -0.0019460693001747131, -0.05369085446000099, 0.030076105147600174, 0.016851436346769333, -0.021779805421829224, -0.014342419803142548, -0.009926711209118366, 0.05401250347495079, 0.005858398973941803, 0.006299550179392099, -0.05091239511966705, 0.09661075472831726, -0.04077210649847984, 0.004449560306966305, -0.019048145040869713, -0.08047869801521301, 0.055917888879776, 0.014876343309879303, 0.030240491032600403, -0.042289186269044876, 0.025909438729286194, 0.007453594822436571, -0.02634337730705738, 0.02434758096933365, -0.016128839924931526, 0.0019147670827805996, -0.032775551080703735, -0.022353263571858406, 0.026587553322315216, 0.0011221464956179261, 0.042130790650844574, 0.033209528774023056, -0.019049022346735, -0.04885156825184822, 0.01569056324660778, -0.0161771010607481, -0.012068300507962704, 0.020987562835216522, -0.02902081236243248, 0.023097718134522438, -0.039645738899707794, 0.002449053805321455, 0.029034214094281197, -0.013682007789611816, -0.02087218686938286, -0.001321876421570778, 0.026316357776522636, -0.04658430069684982, -0.09131598472595215, -0.0012722787214443088, -0.0007682505529373884, -0.013765347190201283, 0.012151332572102547, 0.04108329489827156, -0.005688758566975594, 0.02105339802801609, 0.0028948113322257996, 0.03003797121345997, -0.006759669631719589, -0.008663102984428406, 0.006622841116040945, 0.017464127391576767, -0.017433954402804375, 0.016117705032229424, 0.0002162268356187269, -0.018862545490264893, 0.019642531871795654, 0.011794455349445343, -0.012825780548155308, 0.03448706865310669, 0.06624026596546173, 0.007592715322971344, -0.011674323119223118, -0.0201448742300272, 0.0016345412004739046, 0.01859896443784237, -0.026412300765514374, -0.008071922697126865, -0.004856553860008717, -0.03176780045032501, -0.0025751774664968252, 0.040319349616765976, -0.012317630462348461, -0.06004273146390915, 0.0022603149991482496, -0.02646208368241787, -0.00977018941193819, 0.10935483872890472, 0.023298021405935287, 0.042651597410440445, 0.023032108321785927, 0.03794274479150772, -0.03812096640467644, 0.05647982656955719, -0.08861745893955231, -0.03913350775837898, -0.004278955515474081, 0.008731056936085224, 0.06331274658441544, 0.08724428713321686, -0.06037725508213043, 0.032011039555072784, -0.058076631277799606, 0.030788619071245193, -0.014911027625203133, -0.04815947264432907, -0.023485150188207626, 0.013657977804541588, -0.06530507653951645, -0.024613628163933754, -0.041831593960523605, -0.012917022220790386, 0.04309142753481865, 0.09952415525913239, 0.012239747680723667, 0.0367879793047905, 0.007234178949147463, 0.04910415783524513, 0.034616902470588684, 0.03700118139386177, 0.036733802407979965, -0.05498407036066055, -0.019112730398774147, -0.029112908989191055, -0.033980417996644974, 0.0269315205514431, -0.007303233258426189, -0.019758740440011024, 0.004847004543989897, 0.02577020972967148, 0.0011480983812361956, -0.0292969923466444, -0.022320620715618134, 0.07103142887353897, -0.0034729796461760998, -0.009500831365585327, -0.06877782195806503, 0.012503680773079395, 0.032257478684186935, -0.01555885374546051, 0.035319503396749496, 0.019451478496193886, -0.04343036934733391, -0.003211656119674444, -0.011866230517625809, -0.01819217950105667, 0.02306266501545906, -0.00821365974843502, -0.010353307239711285, -0.044539473950862885, 0.048458509147167206, 0.005513358395546675, 0.08308698236942291, -0.014520314522087574, -0.016254115849733353, 0.01380881480872631, 0.0077892872504889965, -0.0528918020427227, -0.009477020241320133, 0.04231807217001915, 0.011585652828216553, -0.017661670222878456, -0.039951518177986145, -0.0017175410175696015, -0.01628199964761734, -0.043794918805360794, 1.9369386791368015e-05, -0.0041297608986496925, -0.034917913377285004, -0.026459725573658943, 0.049180466681718826, -0.03500933572649956, 0.011757249012589455, -0.02465430088341236, 0.02349701151251793, -0.02278805524110794, -0.025604814291000366, 0.017050711438059807, -0.035130102187395096, -0.0008509363979101181, 0.009446199983358383, -0.02618779055774212, -0.016177181154489517, 0.03142474964261055, -0.022792791947722435, -0.05391676723957062, -0.005998841021209955, 0.03888319060206413, 0.0308273546397686, -0.024521538987755775, 0.04123725742101669, 0.03449301794171333, 0.03626091778278351, 0.019219420850276947, -0.033324696123600006, -0.00025046474183909595, 0.057688143104314804, 0.03535791113972664, -0.03843776881694794, -0.032816726714372635, -0.025301510468125343, -0.029963400214910507, 0.004558613058179617, 0.05727077275514603, -0.02073764055967331, -0.005509511101990938, 0.006869136355817318, 0.011799278669059277, 0.04455806314945221, 0.0583316870033741, -0.012979074381291866, 0.006040597800165415, 0.026065340265631676, -0.049356188625097275, 0.00751268956810236, 0.02426019497215748, -0.029152195900678635, 0.023738577961921692, -0.01519553642719984, -0.05867673084139824, -0.01582433469593525, -0.07011420279741287, -0.005288171581923962, -0.026859186589717865, -0.08540823310613632, -0.02868887223303318, -0.023039795458316803, 0.0052734422497451305, -0.012938829138875008, -0.08338119834661484, 0.007192990742623806, -0.023456497117877007, 0.08372245728969574, -0.011389967985451221, -0.025728536769747734, 0.03341474384069443, 0.06822092831134796, 0.0037476203870028257, -0.04225587099790573, -0.019894130527973175, 0.013165379874408245, -0.02008705586194992, 0.044420067220926285, 0.030025692656636238, -0.02888169139623642, -0.0194022785872221, -0.049791496247053146, -0.02672993205487728, -0.022959569469094276, 0.04073236510157585, 0.04699478670954704, 0.00780224334448576, 0.023828895762562752, -0.019276242703199387, -0.005926098674535751, 0.0585966557264328, -0.04282625764608383, 0.04346286132931709, -0.02168111689388752, 0.0884394720196724, 0.05691337585449219, -0.02498811110854149, 0.028323356062173843, -0.05266127362847328, 0.0035364541690796614, 0.0019847266376018524, 0.008881895802915096, -0.02804938703775406, 0.010103408247232437, 0.016057705506682396, -0.08106963336467743, 0.01795961707830429, 0.04217352718114853, 0.0014261903706938028, -0.025277936831116676, -0.009371409192681313, 0.033685408532619476, 0.01556418463587761, -0.037654146552085876, 0.0181695818901062, 0.04837977886199951, 0.03598187863826752, -0.001200062339194119, -0.06728966534137726, 0.06344150751829147, -0.03647095710039139, -0.0018649754347279668, 0.011347044259309769, 0.028250638395547867, 0.02410617098212242, 0.011435574851930141, 0.01964891143143177, 0.016698043793439865, -0.0045118373818695545, 0.012055855244398117, 0.010056738741695881, 0.029463764280080795, 0.0061956713907420635, 0.05458185449242592, 0.02089911513030529, -0.03157007321715355, -0.024171726778149605, -0.03667059913277626, -0.048755571246147156, 0.0007694684900343418, 0.021876508370041847, 0.09597074240446091, 0.003373742802068591, -0.05351104959845543, -0.058054473251104355, 0.018599728122353554, 0.006976572796702385, -0.01712702587246895, -0.017562326043844223, 0.005126739852130413, 0.02546202391386032, 0.053606655448675156, 0.026991721242666245, 0.005119386129081249, 0.032660190016031265, 0.020684650167822838, 0.069789357483387, -0.00024005422892514616, 0.025410374626517296, 0.004855573642998934, 0.007356131449341774, -0.004015728831291199, 0.04783535376191139, -0.008128517307341099, -0.04621489346027374, 0.0236258115619421, -0.004638721235096455, 0.023090112954378128, 0.04951778054237366, 0.0003433989768382162, -0.05149313062429428, 0.01835394836962223, -0.039474938064813614, 0.015596739016473293, -0.0020844517275691032, -0.04481068626046181, -0.023735450580716133, 0.01597130484879017, -0.0010111687006428838, 0.014116736128926277, -4.901004025246718e-33, 0.050033558160066605, 0.00028399215079844, 0.019039804115891457, 0.018251383677124977, -0.05604281276464462, 0.005719100125133991, -0.021947011351585388, -0.04956495761871338, 0.00988087523728609, 0.022839099168777466, -0.0181568656116724, -0.0024700204376131296, 0.006571561098098755, -0.0469309538602829, 0.041962429881095886, -0.043392881751060486, 0.0234834011644125, -0.04694611579179764, -0.037009090185165405, 0.023143334314227104, 0.006097509991377592, -0.0019156860653311014, 0.033400293439626694, 0.055879827588796616, -0.029405320063233376, 0.013223716989159584, -0.03790085390210152, 0.05501920357346535, 0.009400187991559505, 0.019529130309820175, -0.014559642411768436, -0.06306499242782593, -0.028914034366607666, -0.09487207233905792, 0.004429107066243887, -0.0436757430434227, -0.03578140214085579, -0.032648880034685135, -0.07756228744983673, -0.0038205741439014673, -0.0587485171854496, -0.022703154012560844, 0.008390485309064388, 0.023849939927458763, -0.061665479093790054, 0.03459755331277847, -0.012686219997704029, 0.03539512678980827, -0.018267765641212463, -0.08438608795404434, 0.004319644067436457, 0.014122606255114079, 0.011363237164914608, -0.018555885180830956, 0.031770192086696625, 0.05501324683427811, -0.010287393815815449, -0.044954389333724976, -0.03315334394574165, 0.001004053745418787, -0.009141652844846249, 0.0040029422380030155, -0.01924002543091774, 0.08699921518564224, 0.10551462322473526, 0.007459345273673534, 0.03589785844087601, -0.003540214616805315, -0.007676652632653713, 0.030127644538879395, -0.021302105858922005, -0.0005716259474866092, 0.018115565180778503, -0.026238683611154556, 0.01723499409854412, -0.023927414789795876, -0.02131136879324913, -0.001809597248211503, 0.027887774631381035, 0.074457548558712, 0.007526191882789135, 0.03639652207493782, 0.007873148657381535, -0.0003066732606384903, -0.008958281949162483, 0.011665359139442444, 0.03033193200826645, -0.011341088451445103, 0.0016581870149821043, 0.043567389249801636, 0.014514234848320484, -0.013509965501725674, 0.020052360370755196, 0.002971164882183075, 0.05773961916565895, -0.0024333626497536898, 0.004786472301930189, 0.047340888530015945, -0.0003410572826396674, 0.014247787185013294, -0.06555934995412827, -0.04000825062394142, -0.03358869254589081, -0.04521181806921959, -0.01774045266211033, 0.025608599185943604, -0.03934728726744652, 0.049795858561992645, -0.009505280293524265, 0.017376337200403214, -0.02929360419511795, 0.004380695056170225, 0.04947692155838013, -0.020488014444708824, 0.066704660654068, -0.04785614833235741, -0.0027911625802516937, 0.028084052726626396, -0.00693945586681366, 0.00037420689477585256, -0.007783351931720972, 0.004320951644331217, -0.020971832796931267, 0.06497731059789658, -0.025127729400992393, -0.008675976656377316, -0.07704660296440125, 0.031703125685453415, -0.07230343669652939, -0.06546515971422195, 0.018708808347582817, 0.021228047087788582, 2.6887295234701014e-07, -0.0869716927409172, -0.004951111972332001, 0.04515857249498367, 0.00010575795749900863, 0.031524527817964554, 0.02884824201464653, -0.018823472782969475, 0.008946888148784637, 0.015526085160672665, 0.0347447507083416, 0.05272384732961655, 0.031126949936151505, -0.01948072947561741, -0.028264200314879417, -0.06714026629924774, -0.03662567958235741, -0.014276860281825066, 0.025982700288295746, -0.011305496096611023, -0.05907505750656128, -0.057015545666217804, 0.022389262914657593, 0.037301771342754364, 0.02272132597863674, 0.05359581112861633, -0.008734564296901226, 0.03703984245657921, 0.033505335450172424, 0.010622753761708736, -0.007954922504723072, 0.04930391535162926, -0.008187573403120041, -0.03145184367895126, -0.0747668519616127, -0.02282373607158661, 0.056993816047906876, -0.053749896585941315, 0.042432185262441635, -0.011032359674572945, -0.04135744646191597, -0.05608076974749565, 0.041487276554107666, 0.05491331219673157, -0.009110108017921448, 0.026954613626003265, -0.0748908668756485, 0.016929866746068, -0.007470616605132818, -0.005000015255063772, -0.01285680290311575, 0.013988200575113297, 0.021273203194141388, -0.031738653779029846, -0.01844053529202938, -0.018261631950736046, 0.05701855942606926, 0.005185623653233051, -0.034335896372795105, 0.022240785881876945, 0.051696229726076126, 0.029254335910081863, -0.024512171745300293, -0.015442402102053165, 0.03582252189517021, 0.0318482369184494, -0.04858313500881195, -0.03470783308148384, 1.8780085753708133e-34, -0.02041430026292801, -0.04303703084588051, 0.004850721452385187, 0.006619875319302082, -0.02228010818362236, -0.0007569359731860459, 0.017196116968989372, 0.017975999042391777, 0.1088654026389122, 0.026586713269352913, -0.039730504155159], [-0.056119829416275024, 0.004070463590323925, -0.00810107309371233, -0.057916417717933655, -0.01801663637161255, 0.013189977966248989, -0.013766703195869923, 0.010134114883840084, -0.0669793114066124, 0.0694354847073555, -0.009308619424700737, -0.023326892405748367, 0.018644623458385468, 0.01605956070125103, 0.019885577261447906, -0.009801189415156841, 0.0405200757086277, -0.0740460753440857, -0.02583947218954563, -0.03536833077669144, -0.04107652232050896, -0.0467359721660614, -0.02477450668811798, 0.04431719332933426, 0.010152729228138924, -0.029223607853055, 0.00846900139003992, 0.010284104384481907, 0.031108906492590904, -0.07932496070861816, -0.012197890318930149, -0.024241596460342407, 0.018748639151453972, 0.024763604626059532, 2.4250052774732467e-06, -0.00924533698707819, -0.012729472480714321, -0.02418270707130432, 0.005720448214560747, -0.0013882111525163054, -0.002644249703735113, -0.006221385672688484, 0.01379091665148735, 0.022806208580732346, 0.027608202770352364, -0.020988987758755684, 0.0005647523212246597, 0.08784552663564682, -0.05526835471391678, 0.03416489064693451, 0.019786855205893517, 0.06018036976456642, -0.032432444393634796, -0.038670606911182404, 0.02624424546957016, -0.03865635767579079, 0.03153758868575096, -0.0007026400999166071, 0.005453045945614576, -0.037251152098178864, -0.057641882449388504, -0.017510520294308662, 0.021797992289066315, 0.001819341559894383, -0.06769255548715591, 0.03311031311750412, 0.07460426539182663, 0.010974244214594364, -0.003503411542624235, -0.006619899533689022, -0.013017464429140091, 0.011775078251957893, -0.03193671256303787, -0.033229224383831024, 0.01687614433467388, 0.0844452753663063, -0.01899121142923832, 0.0018720589578151703, -0.0278929453343153, 0.06836386024951935, -0.015032435767352581, 0.08959125727415085, 0.03452432528138161, 0.012293871492147446, 0.039386194199323654, 0.06851551681756973, -0.039823465049266815, -0.04151573032140732, 0.0342513807117939, 0.07604065537452698, -0.028152579441666603, -0.012310037389397621, -0.005877952557057142, 0.0149860680103302, -0.016126282513141632, 0.007804714143276215, -0.023318370804190636, 0.06644027680158615, 0.007765794172883034, 0.04444340243935585, 0.04033568874001503, 0.030198736116290092, 0.016942378133535385, 0.00481713330373168, 0.017576707527041435, 0.026628902181982994, -0.059360798448324203, 0.009247533977031708, 0.003709186566993594, 0.035605255514383316, -0.012481292709708214, 0.023455599322915077, 0.0002958663972094655, 0.03558281809091568, -0.04370443522930145, -0.01247700210660696, -0.07151129096746445, -0.0027817583177238703, -0.06942056119441986, 0.030085066333413124, -0.0768406018614769, -0.00227624480612576, -0.024261023849248886, 0.028482535853981972, -0.09583213925361633, 0.00396298011764884, -0.021844973787665367, 0.030397506430745125, 0.011855261400341988, 0.023550547659397125, -0.006966788321733475, -0.044710539281368256, 0.020640568807721138, -0.0072590745985507965, -0.01908402144908905, 0.06584537029266357, 0.01104086171835661, -0.001627763151191175, 0.02227386087179184, 0.005872838664799929, 0.016855234280228615, -0.008850670419633389, 0.019812313839793205, -0.028176957741379738, 0.0006572993006557226, -0.005315990187227726, -0.008594210259616375, -0.02964101731777191, -0.008218993432819843, -0.024475540965795517, -0.03095167875289917, 0.008342333137989044, 0.0009470235672779381, -0.019345493987202644, -0.05294543504714966, 0.0020883174147456884, -0.010790104046463966, 0.05944778025150299, -0.028232276439666748, 0.01271001622080803, -0.005195396486669779, 0.05664624273777008, 0.04465906322002411, -0.015324256382882595, 0.023728882893919945, 0.04456193000078201, -0.015892131254076958, -0.0485553964972496, -0.016085054725408554, 0.003859694115817547, 0.04274367541074753, 0.005408493336290121, -0.06285908073186874, 0.06939511746168137, 0.024173356592655182, -0.010557646863162518, -0.01849553920328617, -0.03735015168786049, -0.02193359285593033, -0.01965106464922428, 0.038902804255485535, 0.017016209661960602, -0.04409682750701904, 0.0075466143898665905, -0.002211824059486389, -0.015157897025346756, -0.02795054018497467, -0.0011286167427897453, -0.027950188145041466, -0.06068956106901169, 0.013438698835670948, -0.001784427324309945, -0.041334863752126694, 0.045598048716783524, 0.007914498448371887, -0.0012194926384836435, -0.018912019208073616, -0.046807821840047836, 0.014854551292955875, 0.05107007920742035, 0.034551214426755905, -0.005877497605979443, -0.01467440277338028, -0.023131221532821655, 0.045901812613010406, -0.01021558791399002, -0.026319866999983788, -0.036568064242601395, -0.030667878687381744, -0.0215244572609663, 0.02953655645251274, 0.0029723632615059614, 0.041902001947164536, 0.010878841392695904, -0.02538960985839367, 0.0053748502396047115, -0.015454704873263836, 0.029974710196256638, 0.018752913922071457, 0.05884856358170509, 0.010582033544778824, -0.004375926684588194, -0.037136487662792206, -0.004971429239958525, -0.0134853171184659, -0.0752255842089653, 0.0028021906036883593, -0.0041750152595341206, -0.00383643782697618, -0.028295449912548065, -0.022500446066260338, -0.010445640422403812, -0.03670634329319, 0.05098221078515053, 0.0010869551915675402, -0.027110204100608826, -0.08751535415649414, -0.045178577303886414, 0.016731129959225655, -0.000660040182992816, 0.016011258587241173, -0.00995319988578558, 0.001475388533435762, -0.05539139360189438, 0.0029209183994680643, -0.0013332789530977607, 0.023409826681017876, 0.006779557093977928, -0.006082725711166859, -0.016750715672969818, 0.03794460371136665, 0.023341737687587738, -0.015016372315585613, -0.013717791996896267, -0.03760988637804985, -0.018790263682603836, 0.03841278329491615, -0.02758285030722618, -0.06329406797885895, -0.031731922179460526, 0.0006941843894310296, -0.012090962380170822, -0.004733221139758825, 0.03821833059191704, -0.01045067049562931, 0.01467162650078535, 0.03340597078204155, 0.023707089945673943, 0.010028106160461903, 0.014371708035469055, 0.02217276394367218, -0.00905721727758646, 0.0014294828288257122, -0.05523146316409111, 0.008034517988562584, 0.04460969939827919, 0.005531368311494589, 0.026821032166481018, 0.016594748944044113, 0.010012926533818245, 0.04533679038286209, 0.0179672259837389, -0.008290913887321949, -0.041502345353364944, 0.01841815933585167, -0.002102714264765382, 0.01985195092856884, -0.039213627576828, -0.007435191422700882, 0.05579417943954468, -0.004750889725983143, 0.03599483147263527, -0.029765639454126358, -0.01946580410003662, -0.008594955317676067, -0.016327645629644394, 0.012211665511131287, 0.05213534086942673, -0.020361946895718575, 0.058688972145318985, 0.059531666338443756, 0.05127068608999252, -0.02220200002193451, 0.030408602207899094, -0.0728900358080864, -0.06393734365701675, -0.001720980741083622, -0.01789376139640808, 0.017728472128510475, 0.05585775896906853, -0.05333755910396576, -0.015645740553736687, -0.002706427127122879, 0.01571096107363701, -0.01454051211476326, -0.036980729550123215, -0.008714784868061543, -0.008489183150231838, -0.0446249358355999, 0.04292779415845871, -0.057707253843545914, -0.019435200840234756, 0.020256327465176582, 0.040452755987644196, 0.013492584228515625, 0.03803262487053871, 0.036914363503456116, 0.010081310756504536, -0.010183156467974186, 0.06118404492735863, -0.010055922903120518, -0.006019992288202047, 0.008327067829668522, -0.027148237451910973, -0.0522543266415596, 0.020541468635201454, -0.0011240431340411305, 0.007355035748332739, -0.004902783781290054, 0.013443922623991966, -0.005461363587528467, -0.0699530839920044, -0.018276406452059746, 0.00922386720776558, 0.020869432017207146, -0.013875721022486687, -0.0647946149110794, 0.010821890085935593, 0.05124562606215477, -0.07949091494083405, 0.07553042471408844, 0.01148750726133585, -0.06136525422334671, -0.03739286959171295, -0.01588207669556141, -0.06527929753065109, 0.010267512872815132, -0.06649385392665863, 0.0341072604060173, -0.08620830625295639, 0.056333668529987335, -0.0006156611489132047, 0.07006200402975082, 0.005305208265781403, -0.01826358400285244, 0.0440654493868351, -0.021131714805960655, 0.007519384380429983, -0.060631491243839264, 0.029227394610643387, 0.08310212939977646, -0.005372979678213596, -0.017982861027121544, 0.015416872687637806, -0.06673280894756317, -0.01978966034948826, 0.018151037395000458, -0.002807074459269643, 0.06465491652488708, 0.008110915310680866, 0.06252098828554153, 0.040772709995508194, -0.01361625362187624, -0.004737103823572397, 0.07730717957019806, -0.0331849567592144, -0.03245377540588379, 0.04800368845462799, -0.07197727262973785, 0.009073479101061821, 0.0028721678536385298, -0.04190785065293312, -0.029025478288531303, 0.04563223570585251, 0.025202475488185883, 0.004568638280034065, 0.0016727411421015859, 0.03500622883439064, -0.04209337383508682, 0.014602533541619778, 0.0716654360294342, 0.05787841975688934, -0.02345990017056465, -0.023679763078689575, -0.05128033459186554, -0.004739623516798019, -0.010282238945364952, 0.006672066170722246, -0.05112794414162636, 0.02118898183107376, 0.029023174196481705, 0.0005109935300424695, 0.02988084778189659, 0.05342010408639908, 0.049569036811590195, 0.034636177122592926, 0.01253623515367508, -0.039446376264095306, -0.030499843880534172, 0.057748034596443176, -0.03982102498412132, -0.02748578041791916, 0.02757718227803707, -0.04169025644659996, -0.01813814416527748, 0.00289446790702641, -0.01733235828578472, 0.03455821052193642, -0.031783849000930786, -0.025154337286949158, -0.008868214674293995, -0.06040872633457184, -0.00038047131965868175, 0.008154338225722313, -0.03278208523988724, 0.025187509134411812, 0.03422870859503746, -0.03096151165664196, -0.004140925128012896, -0.0007395623251795769, -0.006690591108053923, -0.06185213103890419, 0.10938391089439392, -0.00693949731066823, 0.0067909792996943, -0.06300579011440277, 0.07188352942466736, 0.02862662635743618, 0.0053174677304923534, 0.0016567049315199256, 0.01376347616314888, 0.06671268492937088, 0.03915543481707573, 0.008736529387533665, -0.009245909750461578, -0.034867431968450546, -0.06669717282056808, 0.006140860263258219, -0.024490756914019585, 0.026153599843382835, 0.042740918695926666, 0.025649864226579666, 0.03504322096705437, 0.01797417365014553, -0.02095145918428898, 0.12645390629768372, 0.0007104533724486828, 0.06562246382236481, 0.008868801407516003, 0.06076082959771156, 0.03760938718914986, 0.0073030791245400906, 0.01677904836833477, -0.024415867403149605, 0.018020128831267357, 0.03219732642173767, 0.0011488047894090414, -0.010912454687058926, 0.014932217076420784, -0.01676817238330841, -0.028530508279800415, 0.06943453103303909, -0.01935500279068947, 0.029395733028650284, 0.034881897270679474, -0.03674011677503586, 0.025728795677423477, 0.025300752371549606, 0.006142752710729837, 0.029842529445886612, 0.07819727063179016, 0.07025889307260513, -0.02531410939991474, 0.02377701736986637, 0.07407716661691666, 0.016356339678168297, 0.04393965005874634, 0.042301543056964874, 0.013596974313259125, 0.005817928817123175, 0.039783887565135956, 0.034325581043958664, -0.03147399052977562, -0.0003508872468955815, -0.006220541428774595, 0.004393677692860365, 0.044716328382492065, 0.015592485666275024, 0.0030735251493752003, 0.004705572035163641, -0.03823523223400116, -0.04702489823102951, -0.08990917354822159, -0.044312454760074615, -0.0020006122067570686, 0.016238486394286156, 0.011762289330363274, -0.03340829163789749, 0.0033311520237475634, -0.07737448811531067, 0.01733233965933323, -0.014701779000461102, -0.01909385435283184, -0.04115765541791916, -0.00803971104323864, 0.04134782776236534, 0.00624547153711319, 0.02307944744825363, 0.002982261823490262, 0.040514469146728516, 0.06931472569704056, 0.05475715920329094, -0.07315743714570999, 0.005782994907349348, 0.007072144653648138, -0.027883920818567276, 0.03126808628439903, 0.04085025191307068, -0.012178795412182808, -0.0061381543055176735, -0.011368137784302235, -0.035516612231731415, -0.04373396933078766, 0.024270979687571526, 0.02842635288834572, -0.06327410787343979, -0.006008668337017298, -0.03781386837363243, -0.02443043328821659, 0.0066797202453017235, -0.07138904929161072, -0.000322155246976763, -0.026993278414011, 0.02631431259214878, -0.006868227384984493, -6.1338360517662395e-33, 0.023537587374448776, -0.031009715050458908, -0.0038836400490254164, 9.703862451715395e-05, -0.03527495265007019, -0.01916525326669216, -0.012860161252319813, -0.006095588207244873, 0.021493256092071533, -0.006877191364765167, -0.060032520443201065, -0.007607186678797007, 0.02296159230172634, 0.008188081905245781, -0.006644563749432564, -0.04611889272928238, 0.008490217849612236, -0.0022731369826942682, -0.020461903885006905, -0.02518284320831299, 0.013526855036616325, 0.03348619118332863, 0.058070629835128784, -0.03478642553091049, -0.031078912317752838, 0.027896085754036903, -0.04337332025170326, 0.006656705401837826, -0.008763318881392479, 0.019006449729204178, 0.009584604762494564, -0.0212219450622797, -0.04433721303939819, -0.06506656110286713, -0.01209618803113699, -0.029039675369858742, 0.02713094837963581, -0.04274313524365425, -0.03971925005316734, 0.03506387025117874, -0.024632496759295464, 0.012047898955643177, 0.025542957708239555, 0.008689592592418194, -0.06858031451702118, 0.03852827101945877, -0.040812693536281586, -0.023829208686947823, -0.03270343318581581, -0.02375921979546547, -0.010589469224214554, 0.012115963734686375, 0.051330018788576126, -0.0037983490619808435, 0.015520949847996235, 0.12020591646432877, 0.00480318907648325, -0.03414582833647728, 0.02546239085495472, -0.005234117154031992, 0.037586648017168045, -0.0056440201587975025, 0.01333175040781498, 0.09188953042030334, 0.04371223971247673, 0.017580315470695496, 0.04201039671897888, -0.005415802821516991, -0.03445558249950409, -0.056814514100551605, -0.043358802795410156, -0.004507926758378744, -0.04052863270044327, 0.0746476873755455, 0.08947959542274475, -0.023376205936074257, -0.002528653247281909, 0.021205661818385124, 0.03067415952682495, 0.03134696185588837, 0.008598992601037025, 0.019133105874061584, -0.054698504507541656, -0.04075205326080322, 0.005496399477124214, -0.03689444437623024, 0.0455239862203598, 0.0016155934426933527, -0.02486233040690422, 0.014652066864073277, 0.008873417973518372, 0.06760521233081818, -0.01639939658343792, -0.00037984398659318686, -0.011287655681371689, -0.026638830080628395, -0.008580077439546585, 0.00258710328489542, -0.024536127224564552, -0.02778714708983898, -0.06067810207605362, -0.06921865046024323, -0.05276026204228401, -0.023341067135334015, 0.005668352358043194, 0.029135698452591896, 0.01526609156280756, 0.009076668880879879, -0.03787890449166298, 0.001714623998850584, -0.022022413089871407, 0.036890558898448944, 0.08609464019536972, -0.0968402624130249, 0.09448793530464172, -0.08356529474258423, -0.006124625913798809, 0.013078784570097923, 0.027943238615989685, 0.02243351936340332, -0.013507690280675888, -0.027494508773088455, -0.06959223747253418, 0.039487093687057495, -0.012198273092508316, -0.014935947023332119, -0.04438287392258644, 0.012697635218501091, -0.07720852643251419, -0.08144833147525787, -0.0021294725593179464, 0.00613419571891427, 3.208131147403037e-07, -0.028876034542918205, -0.005945162381976843, 0.025826696306467056, 0.0009287919383496046, 0.04719347879290581, 0.06264407187700272, -0.014297141693532467, -0.011731468141078949, -0.016864124685525894, 0.0037948708049952984, 0.024702997878193855, 0.013179915957152843, 0.02389976941049099, 0.02884366549551487, 0.027173247188329697, -0.06063712760806084, -0.013409901410341263, 0.03899864852428436, 0.018637273460626602, 0.0023610806092619896, -0.02506592869758606, -0.0037089905235916376, -0.001385324401780963, -0.02745823748409748, 0.03288004547357559, -0.002645164728164673, 0.08547915518283844, -0.03357022628188133, 0.05060853809118271, -0.02862516976892948, 0.002020834479480982, 0.02371407300233841, -0.008913276717066765, -0.015605843625962734, 0.006010028533637524, 0.04247743636369705, -0.00847062561661005, -0.018687715753912926, 0.0039053575601428747, -0.026553239673376083, -0.047467924654483795, 0.0048480890691280365, 0.03813689202070236, -0.05297001823782921, 0.08836127817630768, -0.06432041525840759, 0.0401272252202034, -0.009690826758742332, 0.025558797642588615, -0.005964858457446098, 0.01638304442167282, 0.009969955310225487, -0.010785331018269062, -0.006566629279404879, -0.01505469623953104, 0.001808668253943324, -0.01215128693729639, 0.010130262933671474, 0.017855525016784668, 0.051026351749897, -0.012684758752584457, -0.05678874999284744, -0.012576770968735218, 0.06599968671798706, -0.017325695604085922, -0.05944015085697174, -0.07865235209465027, 2.954837165008359e-34, -0.005692309234291315, -0.05673280730843544, 0.06242978945374489, -0.09416067600250244, -0.027008820325136185, -0.005330794490873814, -0.027806522324681282, 0.025819772854447365, 0.06252776831388474, 0.050792887806892395, -0.026026904582977295], [-0.05276346579194069, 4.5482673158403486e-05, -0.027323473244905472, 0.011823449283838272, 0.014224875718355179, -0.02177630178630352, -0.020776165649294853, 0.025731174275279045, -0.035872552543878555, -0.007062368560582399, -0.04871335253119469, 0.000771133229136467, -0.026169510558247566, 0.02325722575187683, 0.008677396923303604, -0.030285662040114403, -0.001563534839078784, -0.010833500884473324, -0.012382294051349163, -0.018074117600917816, -0.025981472805142403, -0.04925176501274109, -0.004819911904633045, 0.036148518323898315, 0.03486265614628792, -0.00474602309986949, 6.840785772510571e-06, 0.018710626289248466, 0.018300190567970276, -0.09556464105844498, -0.023906873539090157, -0.01094526331871748, 0.030713459476828575, 0.07842128723859787, 2.155716856577783e-06, -0.025763362646102905, 0.0699576884508133, -0.006671791430562735, 0.04050721600651741, 0.09448845684528351, 0.008008910343050957, 0.003623276948928833, -0.0012385104782879353, 0.0008360871579498053, -0.03519299253821373, 0.014465950429439545, 0.003338039852678776, 0.031157342717051506, 0.03634779900312424, 0.02584817446768284, 0.025282347574830055, -0.02072008140385151, -0.015644913539290428, -0.014538025483489037, -0.0075071025639772415, -0.10465187579393387, 0.01732567884027958, 0.025706179440021515, -0.005771022755652666, -0.05750538036227226, -0.0165995005518198, -0.025275206193327904, 0.00980372168123722, 0.0020684378687292337, -0.03335864841938019, 0.04099893569946289, 0.031618084758520126, 0.026992179453372955, -0.016302302479743958, 0.027778973802924156, -0.0991756021976471, 0.02374217100441456, -0.0007490227581001818, -0.023720314726233482, 0.020769152790308, -0.034721218049526215, 0.00018792810442391783, -0.04520227015018463, 0.005942183081060648, -0.025447551161050797, -0.04463573917746544, 0.03275756910443306, 0.025596322491765022, -0.03150317445397377, -0.03438780829310417, 0.04444758966565132, -0.005296546500176191, -0.03389337286353111, -0.005082832649350166, 0.05090777575969696, 0.02014717273414135, -0.10082133859395981, -0.032960131764411926, -0.025563888251781464, -0.05263102427124977, -0.019187001511454582, 0.004732503090053797, 0.010532068088650703, -0.03484158217906952, 0.0038076103664934635, 0.08492662012577057, 0.01959400437772274, 0.05613922327756882, -0.027097558602690697, -0.06777531653642654, 0.06223533675074577, -0.05282997339963913, 0.03498199209570885, 0.0008141043363139033, 0.050399865955114365, -0.040672093629837036, 0.0016781690064817667, -0.011135615408420563, 0.024524057283997536, -0.0040650684386491776, -0.014710793271660805, -0.08059629797935486, 0.0018485409673303366, -0.02177525870501995, -0.01425054669380188, 0.019449183717370033, 0.04162409156560898, 0.01265170332044363, 0.045984815806150436, -0.060760825872421265, -0.043548163026571274, -0.09049763530492783, 0.016018608585000038, 0.061641935259103775, -0.006234522443264723, -0.013028831221163273, -0.007192691322416067, -0.024219170212745667, -0.010222119279205799, -0.00144576421007514, 0.08919573575258255, 0.030735816806554794, 0.024024609476327896, 0.027553921565413475, 0.05348420515656471, -0.018089534714818, -0.05689384415745735, -0.03497789427638054, -0.04231853410601616, -0.019863134250044823, -0.05143580585718155, -0.030405860394239426, 0.008816265501081944, 0.016079695895314217, -0.0015930745285004377, -0.02053968422114849, 0.021924378350377083, 0.01955597661435604, -0.00969152431935072, -0.0020204302854835987, -0.023117246106266975, -0.06339384615421295, 0.08927012979984283, 0.015454311855137348, 0.01960808038711548, 0.0025512641295790672, 0.02650512009859085, 0.02045607753098011, -0.0021434458903968334, 0.0042145573534071445, 0.014756790362298489, -0.0012407810427248478, -0.03379349410533905, 0.013172803446650505, 0.11587639898061752, 0.011244744062423706, -0.003097216133028269, -0.06599529087543488, -0.010187912732362747, 0.04821351170539856, 0.02099689468741417, 0.03833399713039398, -0.0024490621872246265, 0.03899705037474632, -0.018698984757065773, 0.0027299481444060802, 0.00309965037740767, -0.014171023853123188, 0.06520383059978485, 0.05692274868488312, 0.04102081432938576, -0.04082946851849556, 0.035388652235269547, -0.03213074058294296, -0.07991893589496613, 0.0004237212997395545, -0.00040943355998024344, -0.011585372500121593, -0.01304761040955782, 0.03336270526051521, 0.013416086323559284, -0.053573548793792725, -0.02096506953239441, -0.013820439577102661, 0.04220594838261604, -0.02537424862384796, -0.05805414542555809, -0.008763027377426624, -0.04465721175074577, -0.027389677241444588, -0.043445296585559845, -0.01063578948378563, -0.008075112476944923, -0.07640594989061356, -0.016650235280394554, -0.01925710402429104, 0.04241757467389107, 0.02338034100830555, -0.018104812130331993, -0.003260982222855091, -0.04510154947638512, 0.0404852069914341, 0.087489053606987, 0.004331459291279316, 0.05783519148826599, 0.0061454433016479015, 0.04372059926390648, -0.0004748636274598539, 0.026143541559576988, -0.011978275142610073, 0.008612959645688534, 0.06456092745065689, -0.04726878181099892, 0.006626877933740616, 0.028602905571460724, -0.016223371028900146, -0.025885438546538353, -0.006080924533307552, 0.03253279626369476, -0.02128531225025654, 0.0012066398048773408, -0.018606361001729965, 0.01033070683479309, 0.013790316879749298, 0.0068697757087647915, 0.07294480502605438, 0.055370647460222244, 0.008116751909255981, 0.018322542309761047, 0.02013568952679634, 0.04909246414899826, 0.0043782140128314495, 0.0056132120080292225, -0.027500852942466736, -0.012483600527048111, -0.010889003053307533, 0.0062263887375593185, 0.020251087844371796, 0.0061293700709939, -0.042561862617731094, 0.049734871834516525, -0.03958945721387863, 0.0005062666605226696, -0.08069245517253876, 0.024649571627378464, 0.05237909406423569, 0.01144391018897295, -0.026811491698026657, 0.0014726057415828109, 0.025096870958805084, -0.006929914932698011, -0.0036096496041864157, 0.03345073014497757, 0.04304773360490799, -0.07074939459562302, 0.0212125051766634, 0.015259304083883762, -0.054787393659353256, -0.026581479236483574, 0.025856435298919678, 0.002519490197300911, 0.02493196539580822, 0.0452469140291214, -0.028129007667303085, 0.0528942234814167, 0.02879808284342289, -0.01711825653910637, 8.763352525420487e-05, -0.015434161759912968, 0.012865360826253891, 0.02928910404443741, -0.01628059707581997, 0.009825215674936771, -0.02283053658902645, 0.016209904104471207, -0.02454499341547489, -9.233055607182905e-05, -0.008955084718763828, -0.0608399361371994, 0.042989179491996765, 0.04676191881299019, -0.04395406320691109, 0.08704736083745956, -0.01957731321454048, -0.022110523656010628, 0.02967698499560356, 0.00028631227905862033, -0.0014678987208753824, 0.0479874424636364, -0.007475447375327349, -0.07375489920377731, -0.04322345182299614, -0.007944374345242977, 0.007082748226821423, 0.1175825372338295, -0.037225231528282166, -0.024574492126703262, -0.03841120004653931, 0.041297655552625656, -0.042214248329401016, -0.06471214443445206, -0.016692591831088066, -0.011769052594900131, -0.0430220328271389, -0.005788027308881283, -0.014815491624176502, 0.03506788611412048, 0.04124666377902031, 0.004633451346307993, 0.01506798341870308, 0.010741415433585644, 0.007142251823097467, 0.013144908472895622, -0.019192928448319435, 0.011811479926109314, 0.008880039677023888, -0.05574636906385422, -0.019415931776165962, -0.020165150985121727, -0.03331797569990158, 0.014209815301001072, 0.0064003667794167995, -0.00039766848203726113, 0.02925071306526661, -0.010305004194378853, -0.005304600112140179, -0.04838430508971214, -0.001524516032077372, 0.06091533228754997, -0.01641474850475788, -0.0011621505254879594, -0.03375563025474548, -0.023408865556120872, -0.03198999911546707, -0.03357389196753502, -0.0015094404807314277, 0.025113174691796303, -0.03856080397963524, -0.02074917033314705, 0.0011991560459136963, -0.005396496970206499, 0.0025298488326370716, 0.023776015266776085, 0.03768051415681839, -0.03347501531243324, 0.12866917252540588, 0.02412840910255909, 0.05349761247634888, -0.01375568937510252, 0.019634032621979713, -0.0048997849225997925, -0.03847990557551384, -0.034286607056856155, -0.035777460783720016, 0.08909539133310318, 0.09409726411104202, -0.04267223924398422, -0.0016104793176054955, 0.025463376194238663, -0.030671987682580948, 0.018174750730395317, 0.005134650971740484, -0.008195784874260426, 0.02042759209871292, -0.007286758627742529, 0.003537027398124337, -0.07880550622940063, -0.00894889235496521, 0.005797477439045906, 0.07380932569503784, -0.0073407599702477455, -0.021523118019104004, 0.04834610968828201, -0.05664661526679993, 0.015751974657177925, -0.004202449228614569, -0.06606719642877579, -0.016845885664224625, 0.012438051402568817, -0.013627900741994381, -0.07476803660392761, 0.032775167375802994, 0.018718888983130455, 0.041291244328022, -0.021084528416395187, 0.003114374354481697, 0.004717182833701372, 0.021701417863368988, -0.0020480139646679163, -0.08565225452184677, 0.00741642015054822, 0.014925841242074966, 0.040509358048439026, -0.018633553758263588, -0.044466905295848846, -0.03675948828458786, -0.05070876702666283, 0.04959665983915329, 0.04774728789925575, 0.02211667411029339, -0.026759333908557892, 0.01747940480709076, -0.005797803867608309, 0.046541567891836166, 0.027365870773792267, 0.017941661179065704, 0.027769772335886955, 0.00404221098870039, 0.03186837211251259, -0.06300108879804611, -0.007615414448082447, -0.009934823028743267, 0.008391816169023514, 0.012929573655128479, -0.006815511733293533, 0.004895017948001623, -0.04522199556231499, -0.03095582127571106, 0.004053505603224039, -0.08258718997240067, -0.04545574262738228, -0.050941649824380875, 0.022350825369358063, -0.03951472043991089, -0.05537082627415657, -0.037780631333589554, -0.07367328554391861, 0.07049714773893356, -0.02089007943868637, 0.005877246148884296, -0.060473233461380005, 0.03981902450323105, 0.04291732981801033, -0.03923657909035683, 0.046984247863292694, 0.02333042025566101, -0.017592888325452805, 0.013057089410722256, 0.0186234712600708, -0.028588516637682915, -0.027856117114424706, -0.03793807327747345, 0.021047664806246758, 0.009285292588174343, 0.03926251828670502, 0.03940054029226303, 0.017211252823472023, 0.03367144986987114, 0.018170760944485664, 0.043539803475141525, 0.027045052498579025, 0.04990414157509804, 0.029707035049796104, 0.006174546200782061, 0.05645948275923729, -0.0038047577254474163, 0.00839762669056654, 0.04584164917469025, -0.021136518567800522, 0.01948653906583786, 0.011880024336278439, 0.015785422176122665, -0.05548284575343132, -0.007558391895145178, -0.027581840753555298, -0.024852409958839417, 0.12558415532112122, -0.0017274398123845458, 0.0024455622769892216, 0.010099676437675953, 0.00993775948882103, -0.06963995844125748, 0.030859297141432762, -0.06111331656575203, -0.028306951746344566, 0.04908783733844757, -0.018700720742344856, -0.0013303549494594336, 0.007840505801141262, 0.05688244849443436, -0.05914176255464554, -0.036778051406145096, -0.002397196600213647, 0.03611627221107483, 0.07308906316757202, -0.040334105491638184, -0.004935557022690773, 0.03321237862110138, 0.0005687780794687569, 0.019772516563534737, 0.049250733107328415, -0.005913564935326576, -0.007974463514983654, 0.014318505302071571, -0.018335793167352676, 0.023199502378702164, -0.004582826513797045, -0.03954940289258957, 0.002139310585334897, 0.00768804457038641, -0.004872697871178389, 0.12987761199474335, 0.012179900892078876, -0.02339470200240612, -0.015022729523479939, -0.005751600954681635, 0.008273283950984478, -0.026788897812366486, -0.008820443414151669, 0.016435014083981514, -0.008571171201765537, 0.0044680931605398655, -0.03412964195013046, -0.002583565656095743, 0.02074948139488697, 0.03413264825940132, 0.08136732876300812, -0.05480264499783516, 0.022521980106830597, 0.001510897185653448, -0.031047681346535683, 0.02399148792028427, 0.07747480273246765, -0.02198920026421547, -0.04740352928638458, -0.05372840538620949, 0.015669066458940506, -0.03207479044795036, 0.05756204202771187, 0.017105253413319588, -0.03247285261750221, 0.017147323116660118, -0.021846596151590347, -0.00833509024232626, -0.02985852211713791, -0.026000037789344788, -0.034019798040390015, -0.00878165103495121, -0.014549239538609982, 0.00845947302877903, -5.446279855078683e-33, 0.01307452842593193, 0.039869897067546844, 0.024824216961860657, -0.018351133912801743, -0.04729481413960457, 0.004381906241178513, -0.002379173180088401, -0.023243684321641922, -0.017279626801609993, 0.007377547677606344, -0.024765463545918465, -0.012991228140890598, 0.016418786719441414, -0.027646590024232864, 0.010103244334459305, -0.06630843132734299, 0.0375058650970459, -0.05106925219297409, -0.034663617610931396, -0.01175148505717516, -0.004037525039166212, 0.03812985494732857, -0.00983169674873352, 0.03433701768517494, -0.013312824070453644, -0.0015655744355171919, -0.017224466428160667, -0.01320456899702549, -0.01730159856379032, 0.00864808727055788, 0.0064530097879469395, -0.02980399876832962, -0.017803894355893135, -0.06645827740430832, -0.007359762210398912, -0.05754494667053223, -0.01383056491613388, -0.014494966715574265, -0.08100759983062744, 0.0014469427987933159, -0.035727228969335556, -0.051361795514822006, 0.05414494872093201, 0.03978170454502106, -0.10552191734313965, -0.018381888046860695, 0.006261820439249277, 0.01124813687056303, -0.014628976583480835, -0.046325743198394775, 0.02429039031267166, 0.032199062407016754, 0.04326330125331879, 0.014493506401777267, 0.013293949887156487, -0.0073450892232358456, 0.0012511631939560175, 0.005610282067209482, -0.059368863701820374, 0.009713047184050083, -0.014365410432219505, 0.013600131496787071, 0.016324611380696297, 0.05714410915970802, 0.07492997497320175, 0.009554279036819935, 0.09481330215930939, -0.01592615433037281, -0.029854128137230873, -0.03162185475230217, -0.0037034719716757536, 0.008248765952885151, 0.07670861482620239, 0.027133267372846603, 0.07702647149562836, -0.06863650679588318, -0.025148626416921616, 0.06842625141143799, 0.005840649828314781, 0.04120193421840668, -0.028270298615098, 0.03181913495063782, -0.05238231271505356, 0.020818231627345085, 0.0025197335053235292, 0.0040029194205999374, 0.0043288348242640495, -0.07363669574260712, 0.009121582843363285, 0.012171125039458275, 0.02286314219236374, -0.01473233662545681, 0.0007236087694764137, -0.010290667414665222, 0.05415785685181618, -0.007715041749179363, -0.00047014994197525084, 0.047209903597831726, -0.024255802854895592, -0.03138174116611481, -0.03443945571780205, -0.03708915412425995, 0.003681028261780739, 0.0388147234916687, 0.0026279620360583067, -0.021387854591012, -0.05233318358659744, 0.0052101342007517815, -0.002150530694052577, 0.02863958291709423, 0.00931556522846222, 0.026870321482419968, 0.06477876752614975, -0.048260126262903214, 0.024657052010297775, -0.04752776399254799, 0.021399635821580887, 0.00881822220981121, -0.026574959978461266, -0.05527976155281067, -0.025587746873497963, 0.0514129176735878, 0.020846838131546974, 0.03106878511607647, 0.0024668886326253414, 0.016169270500540733, -0.03828664496541023, 0.022203076630830765, -0.08883495628833771, -0.041543539613485336, -0.05792675167322159, -0.01011294312775135, 2.8832806719947257e-07, -4.150231688981876e-05, 0.036850787699222565, -0.006470588501542807, 0.014175612479448318, -0.009670538827776909, 0.06085497513413429, -0.05966823175549507, 0.019921908155083656, 0.009618868120014668, 0.026784459128975868, 0.03647615760564804, 0.03013933263719082, -0.01536960992962122, 0.0029196159448474646, -0.0374918133020401, -0.026390284299850464, -0.04455395042896271, 0.06920632719993591, -0.005501577164977789, -0.009095381014049053, -0.02183542773127556, 0.032799333333969116, 0.014776481315493584, 0.024959169328212738, 0.05154874920845032, -0.04596185311675072, 0.013622133061289787, 0.022365223616361618, 0.04356857389211655, 0.0063671329990029335, 0.04280584305524826, 0.05665648728609085, -0.027299193665385246, -0.04222433269023895, -0.017380068078637123, 0.05013523995876312, 0.02227305807173252, 0.017664160579442978, -0.01827937364578247, -0.025790320709347725, -0.023168599233031273, 0.031212270259857178, 0.04499973729252815, -0.0520053468644619, 0.058306898921728134, -0.026392914354801178, -0.014670049771666527, -0.034931380301713943, -0.05614493787288666, -0.016267310827970505, 0.03603527694940567, 0.0013928324915468693, -0.0012351878685876727, 0.018063431605696678, -0.03978535905480385, 0.039170365780591965, 0.02972155436873436, -0.036815397441387177, 0.00654797675088048, 0.03456518054008484, -0.02657664567232132, -0.0334206186234951, -0.04292532056570053, 0.04197968915104866, 0.03712983801960945, -0.03322720527648926, -0.03205832839012146, 2.1230137405058333e-34, -0.028095725923776627, -0.026626665145158768, 0.0026149095501750708, -0.048095740377902985, -0.0015117332804948092, 0.004727017600089312, 0.03662234917283058, -0.0005139785935170949, 0.029661940410733223, 0.030139120295643806, 0.0028596629854291677], [-0.021530289202928543, 0.04898805171251297, -0.014925939962267876, -0.020630642771720886, 0.0023565341252833605, -0.00031681687687523663, -0.0031596024055033922, 0.0005632670945487916, -0.022081950679421425, 0.019389422610402107, -0.042398851364851, -0.021926097571849823, -0.04528858885169029, 0.020714331418275833, 0.015581862069666386, -0.03871575742959976, 0.017252204939723015, -0.03988796845078468, 0.010676538571715355, -0.036389924585819244, -0.023052910342812538, -0.06905783712863922, -0.01041776966303587, 0.024129630997776985, 0.01617703214287758, -0.003730997908860445, -0.030661912634968758, -0.009325049817562103, 0.00870033074170351, -0.07310269773006439, 0.007214262615889311, -0.01946830004453659, 0.04435397684574127, 0.07367052137851715, 2.2580145468964474e-06, -0.02835865505039692, -0.005064775701612234, -0.0327235572040081, 0.036605678498744965, 0.023883171379566193, -0.0042211804538965225, 0.04916765168309212, -0.007487967144697905, 0.018132830038666725, -0.011187119409441948, -0.048097044229507446, 0.05184142291545868, 0.05569147318601608, 0.046077910810709, 0.0005280099576339126, 0.02665271982550621, -0.0172096136957407, 0.004329266492277384, -0.03149591013789177, -0.00505972420796752, -0.10819116234779358, 0.040914423763751984, 0.023738251999020576, 0.012023687362670898, -0.036112044006586075, -0.009539319202303886, -0.006086356472223997, 0.023890003561973572, 0.0157467108219862, -0.021066831424832344, 0.01536854449659586, -0.01558203436434269, 0.009283164516091347, -0.026039335876703262, 0.015428347513079643, -0.08230294287204742, 0.01636412926018238, 0.02484005317091942, -0.025980375707149506, -0.007090480532497168, -0.030778223648667336, -0.012807256542146206, 0.02097715623676777, -0.035707153379917145, -0.006654231809079647, -0.015854790806770325, 0.03592275083065033, 0.029028138145804405, -0.0056445179507136345, -0.03622237592935562, 0.022563865408301353, -0.03571394830942154, -0.03575799614191055, 0.041956715285778046, 0.052023570984601974, -0.04324803501367569, -0.015757638961076736, -0.016297219321131706, -0.013619055971503258, -0.06101585924625397, 0.0006074724951758981, 0.032152533531188965, -0.010987435467541218, -0.026499155908823013, -0.023844126611948013, 0.047249771654605865, -0.01547951065003872, 0.027068519964814186, 0.026044685393571854, -0.015980646014213562, 0.06538522243499756, -0.06146426126360893, 0.005017621908336878, 0.004379733465611935, 0.03786635026335716, -0.040099598467350006, 0.006442399229854345, -0.042270973324775696, 0.053398605436086655, 0.030242616310715675, -0.011380111798644066, -0.04657881334424019, -0.0019084600498899817, -0.03679375350475311, 0.003586825681850314, -0.007416531443595886, 0.040621720254421234, -0.027156276628375053, 0.04728797823190689, -0.050018344074487686, -0.028100283816456795, -0.09087897092103958, 0.034342702478170395, 0.02700207754969597, 0.020312510430812836, -0.00161628273781389, -0.005111457780003548, -0.038540069013834, -0.03531033918261528, -0.007913119159638882, 0.12034386396408081, 0.005360106471925974, 0.036753471940755844, -0.030374128371477127, 0.02353830635547638, 0.012787248007953167, 0.004286064766347408, 0.025629093870520592, -0.05146056413650513, 0.003777814097702503, -0.06548946350812912, -0.0016592844622209668, 0.011991489678621292, 0.004425134044140577, -0.013590812683105469, -0.013366264291107655, 0.03805099055171013, 0.022360578179359436, -0.02515980415046215, 0.06172984465956688, 0.017920151352882385, -0.047444041818380356, 0.07212067395448685, 0.06159864738583565, 0.04087761789560318, 0.013813738711178303, 0.024345284327864647, 0.02977881021797657, 0.008133111521601677, 0.009465902112424374, -0.019667701795697212, 0.04484563320875168, -0.04143925756216049, 0.020719198510050774, 0.12973569333553314, 0.0228497963398695, 0.013954268768429756, -0.08237355947494507, -0.009192964993417263, 0.033944182097911835, -0.004025168251246214, 0.0582331083714962, 0.01372456829994917, 0.018161648884415627, -0.003623851342126727, -0.014413287863135338, -0.0023985079023987055, -0.015491217374801636, 0.04518086090683937, 0.0481630377471447, 0.017745988443493843, -0.004808624740689993, 0.029020680114626884, -0.032689668238162994, -0.05808836966753006, 0.04040684178471565, -0.007020596880465746, -0.016811011359095573, -0.015349515713751316, 0.018608471378684044, 0.003120630979537964, -0.06277201324701309, -0.11638695001602173, 0.0033270535059273243, 0.07083986699581146, -0.0357767678797245, -0.06779829412698746, 0.012478945776820183, -0.04366513714194298, 0.016736900433897972, -0.04571276158094406, -0.007446196395903826, -0.042540043592453, -0.04614424332976341, -0.038035862147808075, 0.028247814625501633, -0.01494357455521822, 0.029324909672141075, -0.03709568455815315, -0.04810291901230812, -0.026867270469665527, -0.01559709943830967, 0.10464585572481155, 0.028747010976076126, 0.02153123915195465, -0.04366563260555267, 0.032993465662002563, 0.02518739551305771, -0.004945602733641863, -0.04892594367265701, -0.07666225731372833, 0.07375053316354752, -0.008760624565184116, 0.025359248742461205, -0.03119117021560669, -0.011171367950737476, -0.00389654072932899, -0.0146650280803442, 0.010724849067628384, -0.01409870944917202, -0.07082656770944595, -0.04059501737356186, -0.0032092526089400053, 0.0030697558540850878, -0.06074732169508934, 0.04577392339706421, 0.02678290195763111, 0.0024651787243783474, -0.015308516100049019, 0.015402151271700859, -0.003515371587127447, -0.026499848812818527, -0.0030132278334349394, -0.054755646735429764, -0.004521607421338558, 0.015805337578058243, -0.03297247365117073, 0.05499131605029106, -0.0027430211193859577, -0.039613377302885056, -0.00786106288433075, -0.04966520145535469, -0.016818707808852196, -0.11841440945863724, 0.02406957373023033, 0.0028662660624831915, 0.014279956929385662, -0.0004161662654951215, 0.035814300179481506, 0.003481905208900571, 0.012716219760477543, -0.01536859106272459, -0.0032215407118201256, 0.02271050028502941, -0.047779422253370285, 0.002379470970481634, -0.011208119802176952, -0.02969181537628174, 0.007709507364779711, -0.017121577635407448, -0.017820917069911957, -0.009687203913927078, -0.005280082114040852, -0.007355933077633381, 0.0510956235229969, 0.044492561370134354, 0.019957367330789566, -0.012470210902392864, -0.042652398347854614, 0.0008441336103715003, 0.0029468892607837915, -0.010339362546801567, -0.008928384631872177, -0.01181569043546915, -0.028013359755277634, -0.007442620117217302, 0.001189999165944755, -0.0236895103007555, 0.0024127059150487185, 0.008473312482237816, -0.02281828783452511, -0.03841089829802513, 0.07747960835695267, 0.009691822342574596, 0.020998340100049973, 0.04267406836152077, 0.024981124326586723, -0.03379359096288681, 0.054973054677248, 0.0014637877466157079, -0.05699430778622627, 0.005295493174344301, -0.01211098674684763, 0.01580096036195755, 0.08615674078464508, -0.035571951419115067, 0.013093221932649612, -0.026526734232902527, 0.03946816548705101, 0.016688840463757515, -0.09677941352128983, -0.05196802318096161, -0.005078263580799103, -0.014304391108453274, -0.005411501508206129, -0.016829947009682655, 0.002888148184865713, 0.11170276254415512, 0.04342233017086983, 0.0029326078947633505, -0.001007134560495615, -0.02658715844154358, 0.007052921690046787, -0.005945561453700066, -0.00842701829969883, 0.026676081120967865, -0.03282211348414421, -0.033358506858348846, -0.056726373732089996, -0.049597613513469696, 0.007545909844338894, -0.019294701516628265, -0.009845812804996967, -0.003976300358772278, -0.018699511885643005, -0.00579274445772171, -0.07442717999219894, 0.0007874901057220995, 0.019702119752764702, -0.010169001296162605, -0.007674813270568848, -0.009734543971717358, -0.016310369595885277, -0.029543248936533928, -0.016106704249978065, 0.05070903152227402, 0.024183865636587143, -0.02459513582289219, -0.03209003433585167, 0.00885291863232851, 0.023987287655472755, 0.03525668382644653, 0.007125319913029671, 0.05952197313308716, -0.06458869576454163, 0.07101087272167206, -0.026266857981681824, 0.06244096904993057, -0.013480287045240402, 0.005707733333110809, 0.04354051500558853, -0.011616080068051815, -0.05084998533129692, -0.04623416066169739, 0.04493540897965431, 0.01400770153850317, -0.03040660358965397, -0.04464871063828468, -0.008649179711937904, -0.02064860239624977, -0.01391752902418375, 0.002826050855219364, -0.015729520469903946, 0.0366591140627861, -0.016103196889162064, 0.0528072826564312, -0.0847284272313118, -0.03460344672203064, -0.011369050480425358, 0.08611743152141571, -0.012386738322675228, -0.05933334305882454, 0.024307018145918846, -0.0508917011320591, 0.0076332841999828815, -0.006213046610355377, -0.06287198513746262, -0.03538166359066963, -0.004499850329011679, -0.05171746388077736, -0.07304378598928452, 0.011789041571319103, 0.041115280240774155, 0.011497863568365574, -0.017064470797777176, 0.03415331244468689, 0.03083551488816738, 0.06553075462579727, 0.0015391367487609386, -0.06286584585905075, -0.0011625700863078237, 0.0628160834312439, -0.0032043750397861004, -0.012625305913388729, -0.0034216877538710833, -0.05624422803521156, -0.02321779727935791, 0.01959204114973545, 0.05800217390060425, 0.008556046523153782, -0.033842314034700394, 0.018765350803732872, 0.016189299523830414, 0.031906601041555405, 0.04821854829788208, 0.02991594932973385, 0.04067876562476158, 0.032191671431064606, -0.021156826987862587, -0.05381391569972038, -0.01158320251852274, -0.03171180188655853, 0.030196532607078552, 0.009574750438332558, -0.02639225497841835, -0.005488131195306778, -0.06977993994951248, -0.03220045566558838, -0.01019743550568819, -0.05947894603013992, 0.017410285770893097, -0.03961670771241188, -0.009784193709492683, 0.021337484940886497, -0.0634775385260582, -0.013388953171670437, -0.03678251430392265, 0.07358283549547195, -0.011677307076752186, -0.03754623234272003, -0.012431073933839798, 0.06668000668287277, 0.0201189573854208, -0.05290594324469566, 0.012410481460392475, -0.0013773319078609347, -0.04068741947412491, 0.06187592074275017, 0.009111924096941948, -0.04954172670841217, -0.01992470771074295, -0.005584607366472483, -0.0038847639225423336, -0.017878783866763115, 0.04777064174413681, 0.07895782589912415, 0.01415348518639803, 0.009532415308058262, 0.004659757949411869, -0.03507102280855179, 0.040413998067379, 0.044888950884342194, 0.07632988691329956, 0.0005395792541094124, 0.0543791726231575, 0.004153619520366192, -0.006997930817306042, 0.018383679911494255, -0.06107568368315697, 0.005977559369057417, -0.026078110560774803, 0.03609639033675194, -0.05923962965607643, 0.020547278225421906, -0.046820953488349915, -0.05088885501027107, 0.0695047378540039, -0.0020774672739207745, -0.02019183710217476, -0.01452169381082058, 0.032393332570791245, 0.03286781534552574, 0.0007081676740199327, -0.04323272779583931, 0.04120840132236481, 0.04074922949075699, 0.008187893778085709, 0.00020543498976621777, 0.023993011564016342, 0.11128289997577667, -0.023158209398388863, -0.017787400633096695, 0.007776911370456219, 0.05177322030067444, 0.05784260109066963, -0.023243578150868416, 0.00750199519097805, 0.01581907644867897, 0.020802179351449013, 0.03211544081568718, 0.06049933657050133, 0.009179085493087769, 0.01640191115438938, 0.032327134162187576, 0.007117096334695816, -6.772563938284293e-05, -0.0347265750169754, -0.0569634884595871, -0.06843135505914688, 0.010091559961438179, 0.019618267193436623, 0.10169093310832977, 0.005534139461815357, -0.04863521456718445, -0.03220844641327858, 0.008949031122028828, 0.0001391629921272397, -0.056475602090358734, -0.017289327457547188, 0.007027786690741777, 0.014094940386712551, 0.04483433812856674, -0.004820091649889946, 0.009060785174369812, 0.0415315181016922, 0.03497260436415672, 0.06899669766426086, -0.05553934723138809, 0.012066701427102089, -0.0020355747547000647, -0.04038412868976593, 0.08642161637544632, 0.08551514148712158, -0.014985865913331509, -0.05821061506867409, 0.002551572397351265, 0.020229242742061615, -0.015391162596642971, 0.06955108046531677, 0.042671095579862595, -0.03427340090274811, 0.038161322474479675, -0.03330995887517929, -0.0035810668487101793, 0.01766786351799965, -0.03610410913825035, -0.028582077473402023, -0.02166479267179966, -0.0036942146252840757, -0.01188761368393898, -5.603884627507166e-33, 0.01076706126332283, 0.005206660367548466, 0.012910807505249977, -0.05315867066383362, -0.0251457616686821, -0.015960104763507843, 0.0032861833460628986, -0.030454590916633606, 0.0012280514929443598, 0.017054246738553047, -0.02004343457520008, -0.013015231117606163, 0.01341747771948576, -0.005124053452163935, 0.013955743052065372, -0.029192280024290085, 0.01819087751209736, -0.017795398831367493, -0.03536386787891388, 0.01199876144528389, 0.024781107902526855, 0.048901427537202835, 0.019812924787402153, 0.006646015215665102, -0.038288265466690063, 0.031560108065605164, 0.003692308673635125, -0.0044672186486423016, 0.004632012918591499, 0.03825468569993973, 0.008860036730766296, -0.026915349066257477, -0.03140055015683174, -0.06376253813505173, -1.673941733315587e-05, -0.0346604622900486, -0.02118484489619732, -0.02406053990125656, -0.05198565125465393, 0.004222881980240345, 0.006598940584808588, -0.03277230262756348, 0.04412367567420006, 0.035670775920152664, -0.06379994004964828, 0.01251280028373003, 0.017973249778151512, 0.03198925405740738, -0.0283559188246727, -0.08006833493709564, 0.04933740943670273, 0.013433719053864479, 0.0017979340627789497, 0.0008775648893788457, 0.038102954626083374, 0.01536639966070652, -0.010637949220836163, 0.01949465088546276, -0.020678460597991943, 0.018981998786330223, -0.003689585719257593, -0.002242006827145815, 0.013059621676802635, 0.07526908069849014, 0.06364636868238449, 0.03854861110448837, 0.05088045448064804, -0.0054349517449736595, 0.010698474943637848, -0.012815956026315689, -0.01673966832458973, -0.007656935136765242, 0.041602376848459244, 0.03429829329252243, 0.07549909502267838, 0.016223253682255745, -0.04311937838792801, 0.05615584924817085, 0.009766250848770142, 0.049734532833099365, 0.0019168138969689608, 0.030332084745168686, -0.03300149738788605, 0.007225155830383301, -0.005567499436438084, -0.01968894526362419, 0.004528570920228958, -0.012697351165115833, 0.007507104892283678, 0.008475232869386673, 0.018834080547094345, 0.013037169352173805, 0.01012382097542286, -0.030069490894675255, 0.027079006657004356, 0.008122391067445278, 0.01821686513721943, 0.043604884296655655, -0.005801565945148468, 0.005030367057770491, -0.04268113151192665, -0.04547978937625885, 0.003326158504933119, -0.00720525486394763, 0.010051305405795574, 0.006864862982183695, -0.038082100450992584, 0.0001180367253255099, -0.00923303421586752, 0.027136335149407387, 0.015123977325856686, 0.013979039154946804, 0.04294976592063904, -0.037515684962272644, 0.032955996692180634, -0.07226308435201645, 0.02224055863916874, 0.02132822759449482, -0.004023364279419184, -0.020500892773270607, 0.01316371001303196, -0.013197710737586021, -0.029101964086294174, 0.033784352242946625, -0.002949559362605214, 0.01767054572701454, -0.08005750179290771, 0.0087314797565341, -0.06960839778184891, -0.08125335723161697, -0.02288554050028324, 0.018999574705958366, 2.911249055159715e-07, -0.061544910073280334, 0.04867413640022278, 0.051934994757175446, -0.005143498070538044, 0.00041870641871355474, 0.056579601019620895, -0.04394305497407913, 0.009866162203252316, -0.02302086353302002, 0.031128590926527977, 0.06556832045316696, 0.021722830832004547, -0.010757043026387691, 0.0029051348101347685, -0.058172788470983505, -0.004936838056892157, -0.05617297440767288, 0.09096142649650574, -0.015175219625234604, -0.03504681959748268, -0.022729303687810898, 0.04965706542134285, -0.008428631350398064, -0.004785993602126837, 0.082277812063694, -0.024112273007631302, 0.028438471257686615, 0.015748364850878716, 0.04883899912238121, 0.0003146482340525836, 0.04533080756664276, 0.03761376813054085, -0.03736526891589165, -0.03353480249643326, -0.0053030820563435555, 0.048079416155815125, 0.011480103246867657, 0.03469434008002281, 0.037868648767471313, -0.051860325038433075, -0.011703791096806526, 0.014780370518565178, 0.06666768342256546, -0.026971008628606796, 0.03627985715866089, -0.025688139721751213, -0.001992274774238467, -0.012065589427947998, -0.014754327945411205, -0.025659121572971344, 0.030361207202076912, -0.01364858541637659, 0.002350620925426483, -0.03896865248680115, -0.026622237637639046, 0.013429824262857437, 0.029357511550188065, -0.044812947511672974, 0.033309414982795715, 0.0768471211194992, 0.02138642780482769, -0.045720916241407394, -0.047838762402534485, 0.04248054325580597, 0.04269257187843323, -0.0615558847784996, -0.043198112398386, 2.3758143617322244e-34, 0.022422712296247482, -0.0586949922144413, 0.0031054348219186068, -0.01955108530819416, -0.011375792324543, 0.003436069702729583, 0.01663924753665924, 0.010805368423461914, 0.06752856075763702, 0.01783214509487152, -0.023399321362376213], [-0.05590800195932388, -0.018253611400723457, -0.0284926425665617, -0.007795252371579409, -0.004382930230349302, -0.009067066945135593, -0.04167887941002846, 0.019164996221661568, -0.03752436488866806, 0.04882456362247467, 0.025210363790392876, 0.06635590642690659, 0.01510770246386528, -0.00997775699943304, -0.021169524639844894, 0.03308534622192383, 0.03206852823495865, -0.0488690510392189, -0.03215903788805008, -0.036212626844644547, -0.04654013738036156, -0.08726409077644348, -0.017239857465028763, 0.028385447338223457, 0.05340786278247833, -0.021942198276519775, -0.03881334885954857, -0.00216828566044569, 0.019979998469352722, -0.04397917166352272, 0.009820179082453251, -0.026682524010539055, 0.020179500803351402, 0.021991657093167305, 1.6821722965687513e-06, -0.0343128964304924, -0.012335502542555332, -0.041896648705005646, 0.023274939507246017, 0.025042125955224037, -0.029050732031464577, 0.03940160945057869, -0.051826611161231995, -0.031072650104761124, 0.01695866510272026, 0.05169982090592384, -0.013791288249194622, 0.0038790793623775244, -0.03228962421417236, 0.011707116849720478, 0.015089105814695358, 0.008624979294836521, -0.04916192591190338, -0.07168831676244736, 0.005447824019938707, -0.09082087874412537, 0.04317304491996765, 0.012665173970162868, 0.015376301482319832, -0.031868547201156616, -0.03350147604942322, 0.0030793172772973776, 0.04513613134622574, 0.004957563243806362, -0.0058767045848071575, 0.02895914949476719, 0.050732407718896866, 0.05717790499329567, 0.0034633639734238386, -0.006435752846300602, -0.0015356841031461954, -0.05879107490181923, -0.012796454131603241, -0.009239796549081802, -0.03817228600382805, 0.013659973628818989, 0.02724384143948555, -0.00420087855309248, -0.01150751207023859, 0.02578338421881199, 0.017548423260450363, 0.07977394759654999, 0.002944061066955328, 0.010425366461277008, 0.03712141513824463, 0.06368670612573624, 0.015754876658320427, -0.06019015237689018, 0.044942013919353485, 0.04398144409060478, -0.03315495699644089, -0.01346183754503727, -0.03578037768602371, 0.028715351596474648, -0.02474362589418888, 0.017467666417360306, -0.005794387776404619, 0.0008317012689076364, -0.05717069283127785, 0.011119337752461433, 0.014827154576778412, -0.0354178249835968, 0.06908389180898666, 0.016856662929058075, 0.05203450098633766, 0.016200557351112366, -0.05418084189295769, 0.012214216403663158, 0.002177376765757799, 0.08549872040748596, 0.02086060307919979, -0.005829972680658102, 0.02033299393951893, -0.0005184906767681241, 0.024534573778510094, 0.021052759140729904, -0.05918465927243233, 0.020305199548602104, -0.02158576250076294, 0.041842956095933914, 0.002943861298263073, 0.02592756412923336, -0.07658478617668152, -0.014314587228000164, -0.07935705780982971, -0.021660324186086655, -0.014611074700951576, 0.010688870213925838, -0.0007655523950234056, 0.04302193969488144, 0.0347677543759346, 0.02602900192141533, -0.010700557380914688, -0.02121688425540924, -0.043332356959581375, 0.026994269341230392, -0.036816395819187164, 0.04164864495396614, 0.020705904811620712, 0.003916166722774506, -0.012689933180809021, -0.0054253097623586655, -0.0477813221514225, -0.06709682196378708, 0.009304594248533249, -0.01074261125177145, 0.021404879167675972, 0.007360404822975397, -0.003857246832922101, -0.03760874643921852, 0.027353966608643532, 0.01593949645757675, 0.08742307871580124, -0.019168132916092873, 0.04081336036324501, 0.018625393509864807, 0.00921960361301899, 0.05473173037171364, -0.030536601319909096, 0.04865967482328415, 0.030013076961040497, 0.05969446524977684, 0.040174685418605804, 0.007464258465915918, 0.0038380653131753206, 0.0072433240711688995, -0.015270432457327843, -0.03702855855226517, 0.011333734728395939, 0.002576718805357814, -0.04613811895251274, -0.0225792545825243, -0.041964635252952576, -0.014017265290021896, -0.001530419453047216, 0.006294365972280502, -0.019013069570064545, -0.09234505891799927, -0.06090772524476051, 0.011944166384637356, 0.025692956522107124, 0.018826503306627274, -0.04869008809328079, 0.0033277927432209253, 0.05006140097975731, 0.02017100341618061, 0.023769130930304527, -0.026989413425326347, -0.004928478039801121, -0.009664117358624935, -0.030190065503120422, -0.0763324722647667, -0.10498037934303284, -0.058920834213495255, 0.02557479217648506, 0.005772358272224665, -0.028901202604174614, 0.01228216104209423, 0.00541725754737854, 0.0814228281378746, 0.02302979677915573, -0.011113043874502182, 0.019351541996002197, -0.08639218658208847, 0.06963251531124115, -0.041730329394340515, 0.017382973805069923, 0.0021931370720267296, -0.0396910235285759, -0.03071299009025097, 0.0006248871213756502, -0.0009031763183884323, 0.06080210953950882, 0.029100319370627403, 0.0016805450432002544, 0.040562305599451065, -0.005705487448722124, 0.006661546882241964, 0.0003112927661277354, 0.005840524565428495, 0.01571064256131649, 0.036345917731523514, -0.028523115441203117, -0.035625506192445755, 0.02070462517440319, -0.0899665430188179, -0.025123782455921173, 0.016077876091003418, 0.04066277667880058, -0.0210818350315094, -0.06701634079217911, 0.00277720601297915, -0.03641119971871376, 0.04160798341035843, -0.00472704041749239, 0.026395972818136215, 0.03812302276492119, -0.012103176675736904, 0.029955683276057243, -0.03411264717578888, 0.03926519677042961, 0.017095647752285004, 0.0014297496527433395, 0.0006685068365186453, 0.009473891928792, 0.04698791354894638, -0.05188867077231407, 0.018434444442391396, 0.001314416527748108, 0.024293769150972366, 0.027478216215968132, 0.00822178740054369, 0.01925511658191681, 0.03094724752008915, 0.022855298593640327, -0.04194921255111694, 0.0006252498715184629, 0.01423335075378418, -0.10320402681827545, 0.014527685940265656, -0.043321333825588226, -0.04236423596739769, -0.04334690794348717, 0.002274330472573638, 0.07391411811113358, 0.08023221790790558, -0.012651847675442696, -0.013718600384891033, -0.015820210799574852, 0.019691767171025276, -0.002339832717552781, 0.010115467943251133, -0.03471791744232178, -0.013453139923512936, 0.04229748621582985, -0.0005470592877827585, -0.033985696732997894, -0.01811952516436577, -0.050013672560453415, 0.014566328376531601, 0.0035522726830095053, -0.013492796570062637, -0.005125388503074646, -0.010831056162714958, 0.003979249857366085, -0.022189917042851448, -0.016745949164032936, -0.03326140716671944, -0.005423408001661301, 0.02001894824206829, 0.002585434587672353, 0.056181445717811584, -0.009851302020251751, -0.015309920534491539, 0.03113689459860325, -0.038872309029102325, -0.044871389865875244, 0.06878957897424698, 0.01328841969370842, 0.02549482136964798, -0.039461083710193634, 0.07891622930765152, 0.006231564097106457, 0.012275462970137596, -0.023609846830368042, -0.008326376788318157, -0.03464822471141815, -0.021088730543851852, 0.08879974484443665, 0.06247905641794205, 0.0033599662128835917, -0.014183124527335167, 0.03178524598479271, -0.012753079645335674, -0.020110126584768295, -0.06897367537021637, -0.015803098678588867, 0.03543631732463837, -0.0668196976184845, -0.007231892552226782, -0.040729131549596786, -0.026234570890665054, 0.020250339061021805, 0.014776155352592468, 0.022678490728139877, -0.029896646738052368, 0.013721856288611889, 0.0110561428591609, -0.05455783009529114, 0.0707087442278862, 0.016112998127937317, -0.07164153456687927, 0.05219172313809395, 0.006995426956564188, 0.007218963000923395, 0.004310042597353458, -0.030493421480059624, -0.010292028076946735, 0.027616262435913086, 0.031804319471120834, -0.036794453859329224, -0.041245099157094955, 0.04024345427751541, -0.0026034617330878973, -0.006368616130203009, -0.08502000570297241, -0.012990685179829597, 0.02211202122271061, 0.07687938213348389, -0.04999510198831558, 0.05250127986073494, 0.010209831409156322, -0.04828450083732605, -0.028102710843086243, -0.058027882128953934, -0.04678535461425781, -0.013466112315654755, -0.033530380576848984, -0.04520728811621666, -0.0615411140024662, 0.023143025115132332, -0.011544798500835896, 0.05044625326991081, 0.07852353900671005, 0.016891270875930786, -0.007395305670797825, 0.03021494299173355, -0.006993499584496021, -0.0005409670411609113, 0.012718708254396915, 0.06896945834159851, 0.04876935854554176, 0.015876149758696556, 0.002943496685475111, -0.010302768088877201, -0.004493769258260727, 0.009964562021195889, -0.006069357972592115, 0.005017719231545925, -0.008958263322710991, 0.041692040860652924, -0.005329109262675047, 0.010258295573294163, -0.044996730983257294, 0.055816467851400375, -0.02169298566877842, 0.014733420684933662, 0.0029413083102554083, -0.007446060422807932, -0.043171659111976624, 0.003022218821570277, 0.011986945755779743, -0.03425091505050659, 0.07077200710773468, -0.022419637069106102, -0.042832355946302414, -0.027514401823282242, 0.025232652202248573, 0.03148941695690155, -0.0010987124405801296, 0.03644673898816109, 0.037837762385606766, 0.009083790704607964, -0.007142598275095224, -0.039502911269664764, 0.023398401215672493, 0.038775451481342316, 0.03644628822803497, -0.06755336374044418, -0.008503297343850136, 0.015860090032219887, -0.003068925580009818, 0.05745009332895279, 0.036604952067136765, -0.009017460979521275, -0.0026717069558799267, -0.035730089992284775, -0.016973067075014114, -0.03150869905948639, 0.04393816366791725, -0.03634629398584366, 0.01891891099512577, 0.0193942878395319, -0.009385393932461739, 0.021877799183130264, 0.01621948555111885, -0.02856631763279438, 0.010362607426941395, -0.04234318435192108, -0.056701261550188065, 0.00539441267028451, -0.022385580465197563, -0.04454793408513069, 0.024314817041158676, -0.03282104432582855, 0.007797392550855875, -0.01317087933421135, 0.006470903288573027, -0.03893205150961876, -0.022474130615592003, 0.010349232703447342, -0.06291362643241882, 0.14990901947021484, 0.003309538820758462, -0.020021386444568634, -0.008260340429842472, 0.043900053948163986, 0.023912711068987846, -0.0667102187871933, -0.03239976242184639, 0.0015966887585818768, 0.02321099303662777, 0.03865792602300644, -0.03508894518017769, -0.015782909467816353, -0.0670236349105835, -0.06378553807735443, -0.034905388951301575, -0.06134645640850067, 0.030255230143666267, 0.03477274253964424, 0.024083716794848442, 0.005941697861999273, -0.037323057651519775, -0.0210482906550169, 0.03390498086810112, -0.02567162737250328, 0.08798462152481079, 0.04841875657439232, 0.07339228689670563, 0.01228166464716196, 0.016665620729327202, 0.01384266559034586, 0.017986154183745384, -0.030202116817235947, -0.0438108816742897, 0.004806568846106529, 0.002511640079319477, -0.07373304665088654, -0.05668500438332558, -0.020194798707962036, 0.016354285180568695, 0.023410776630043983, 0.005960987880825996, -0.024413559585809708, 0.029664404690265656, 0.03474559634923935, 0.042525514960289, 0.02681802585721016, 0.04958989471197128, 0.06562985479831696, 0.022184593603014946, 0.012582274153828621, 0.0033142580650746822, 0.040024612098932266, -0.06460358202457428, 0.07369106262922287, 0.005340805742889643, 0.04537803679704666, 0.015696991235017776, -0.03822111338376999, -0.0009721869719214737, 0.013015818782150745, 0.04163632169365883, 0.012482436373829842, -0.01315609086304903, 0.03909057751297951, 0.006077336613088846, -0.010719694197177887, -0.021600650623440742, 0.024978768080472946, -0.06431756168603897, -0.022952748462557793, -0.060053419321775436, -0.030645467340946198, -0.017089275643229485, 0.025935523211956024, 0.03212154284119606, -0.009654405526816845, -0.04854367673397064, -0.010998602956533432, -0.013384475372731686, -0.008917344734072685, -0.007878008298575878, 0.05621115863323212, 0.004478994756937027, 0.029741503298282623, 0.008349275216460228, 0.02851267158985138, 0.014217814430594444, -0.012205862440168858, 0.012324590235948563, -0.07676617056131363, 0.036819469183683395, -0.004479896277189255, 0.033248286694288254, -0.03527968004345894, 0.005312968976795673, 0.02418767474591732, -0.021226493641734123, 0.004508784506469965, -0.02525993064045906, 0.0014504790306091309, 0.06073528155684471, 0.024196049198508263, -0.03451817110180855, -0.044491469860076904, -0.00958454329520464, -0.045506104826927185, 0.06475375592708588, -0.03239427134394646, -0.031094780191779137, 0.007355976849794388, -0.04537573456764221, 0.008400483056902885, -5.663563006330475e-33, -0.0038680480793118477, 0.007130263838917017, -0.038869909942150116, -0.02855837531387806, 0.028790844604372978, -0.038142818957567215, 0.005959724076092243, -0.03570789471268654, 0.03189259395003319, -0.021576402708888054, 0.010411777533590794, -0.007592968642711639, 0.02477349154651165, -0.006437456700950861, -0.019380496814846992, -0.05423709750175476, 0.0007477200124412775, -0.015120713971555233, -0.026255249977111816, -0.04236267879605293, 0.022719748318195343, -0.007214596960693598, 0.024573814123868942, 0.017092064023017883, 0.012367548421025276, 0.031776152551174164, -0.045911289751529694, 0.05301380902528763, -0.013935757800936699, 0.056633077561855316, -0.01549396850168705, -0.023790430277585983, -0.039921484887599945, -0.09013071656227112, 0.04176563769578934, -0.06836143136024475, 0.04809678718447685, -0.07535140961408615, 0.0032300082966685295, 0.02684503234922886, 0.01760978065431118, 0.00419400492683053, 0.006551182363182306, -0.011951620690524578, -0.0675504058599472, 0.017534589394927025, 0.0008730810950510204, 0.038196075707674026, 0.03592758998274803, -0.053488489240407944, 0.005259846802800894, -0.012739392928779125, 0.030581243336200714, 0.03909854590892792, -0.009476330131292343, 0.012425500899553299, 0.02495812065899372, -0.01862190291285515, 0.022354694083333015, -0.026312798261642456, 0.05854853615164757, 0.08978407829999924, 0.025366095826029778, -0.0027580242604017258, 0.059111934155225754, 0.03701077029109001, 0.05043121427297592, -0.0067232572473585606, -0.020607320591807365, -0.0514591783285141, -0.04720454663038254, -0.01824486255645752, -0.051680248230695724, 0.044707223773002625, -0.028401728719472885, -0.04050464928150177, -0.02656528912484646, 0.010828893631696701, 0.026140693575143814, -0.03037244826555252, 0.002169728511944413, 0.007008921355009079, -0.09553272277116776, 0.02194315381348133, -0.019317694008350372, -0.016611473634839058, 0.027299458160996437, 0.004190800711512566, -0.00962749682366848, 0.04163999482989311, 0.031621988862752914, 0.021221976727247238, -0.0233946293592453, -0.01197506207972765, 0.008525492623448372, -0.06290808320045471, 0.04702817648649216, -0.005428854841738939, -0.012779046781361103, -0.03534652665257454, -0.01819252036511898, -0.011708302423357964, -0.045289069414138794, -0.023474039509892464, 0.01709054969251156, 0.029501749202609062, -0.008854367770254612, -0.010359484702348709, 0.017280984669923782, 0.021966606378555298, 0.009531449526548386, -0.03715325519442558, 0.06410203874111176, -0.02581840753555298, 0.10339657962322235, -0.0518915057182312, -0.07565227895975113, 0.05229784548282623, 0.024283472448587418, 0.002363806124776602, 0.02692202478647232, 0.035499900579452515, -0.030422374606132507, 0.04772572964429855, -0.018970945850014687, 0.014902341179549694, 0.007037575356662273, 0.02453499846160412, -0.06879989802837372, -0.026486316695809364, -0.019975541159510612, 0.0012395113008096814, 2.5829194783000275e-07, 0.00515304459258914, -0.0251984354108572, 0.014971807599067688, 0.0732254907488823, 0.06350672245025635, 0.062118254601955414, -0.05604768171906471, 0.028263237327337265, 0.02580067329108715, -0.02956276759505272, -0.02502923086285591, 0.014389080926775932, -0.006519329268485308, -0.037651196122169495, 0.022941764444112778, -0.044249698519706726, 0.0390329509973526, 0.08924071490764618, 0.02819005958735943, -0.03328540921211243, 0.00665304996073246, 0.040709059685468674, 0.06771599501371384, -0.007047007791697979, -0.0028871323447674513, 0.0033222055062651634, 0.015559496358036995, 0.015296069905161858, -0.0065000057220458984, 0.026304000988602638, 0.0007724189781583846, -0.04899792745709419, -0.03237943351268768, -0.03899158164858818, 0.015213634818792343, 0.02405109442770481, 0.013893664814531803, 0.016901247203350067, 0.04597456008195877, -0.014664449729025364, -0.027364563196897507, 0.026260992512106895, 0.008161919191479683, -0.020528562366962433, 0.05329884588718414, -0.03715680539608002, 0.0022758778650313616, 0.03351673483848572, 0.02073233388364315, 0.049958959221839905, -0.006520917639136314, 0.07015715539455414, -0.022551381960511208, -0.01607973501086235, -0.056835971772670746, 0.05889296904206276, 0.012142352759838104, 0.03156362101435661, -0.002016838174313307, 0.04033801704645157, 0.01626279577612877, -0.08083269000053406, 0.01911184750497341, 0.028503697365522385, 0.039864856749773026, -0.07897292822599411, -0.039401207119226456, 2.1138304204787746e-34, -0.007573731709271669, 0.000737641763407737, 0.05085986480116844, -0.04135601595044136, 0.008312015794217587, -0.00035529935848899186, -0.004075502511113882, -0.018191823735833168, 0.043786536902189255, 0.06729286164045334, -0.01331151369959116], [-0.01751711219549179, 0.0008949381299316883, -0.030039502307772636, 0.023612847551703453, -0.020613914355635643, -0.015755336731672287, -0.003903692588210106, 0.015306017361581326, -0.0646728128194809, 0.009708328172564507, -0.03982706740498543, -0.0010766913183033466, -0.03176954388618469, 0.03665197268128395, 0.02266070246696472, -0.018700942397117615, 0.027861539274454117, 0.0152817377820611, -0.013449913822114468, -0.018720922991633415, -0.024301541969180107, -0.08039811998605728, -0.004917871672660112, 0.026103103533387184, -0.014388551004230976, 0.02692052163183689, -0.07684856653213501, 0.005713518708944321, 0.02418859302997589, -0.08394287526607513, 0.03917346149682999, -0.015001257881522179, 0.05772091820836067, 0.08561732620000839, 2.1116709376656218e-06, -0.03627653047442436, 0.01601872220635414, -0.02860417403280735, 0.04056277126073837, 0.03032016009092331, 0.00988346803933382, 0.054776839911937714, -0.06285315752029419, 0.010807452723383904, -0.03008643351495266, -0.03374218940734863, 0.03028259612619877, 0.012131113559007645, 0.04208148643374443, 0.04899780824780464, -0.0016237753443419933, -0.06700923293828964, 0.011163297109305859, -0.015048511326313019, 0.0006564006907865405, -0.051026903092861176, 0.030095694586634636, 0.0039941370487213135, 0.005082929972559214, -0.02584374137222767, -0.03156175836920738, 0.036108002066612244, 0.056992337107658386, -0.02765779383480549, -0.005670495331287384, 0.019854098558425903, -0.017505237832665443, -0.0351589061319828, -0.02063862793147564, -0.01753287948668003, -0.07617105543613434, 0.00599757581949234, 0.0028598320204764605, 0.008891839534044266, -0.023102767765522003, -0.08163778483867645, 0.002579760504886508, 0.015165366232395172, -0.03013906627893448, -0.023593543097376823, -0.0717930719256401, 0.034469589591026306, 0.01144719123840332, -0.012349464930593967, 0.002232967410236597, -0.008476714603602886, 0.010121031664311886, -0.005923915188759565, 0.030390292406082153, 0.004267449956387281, 0.03534764423966408, -0.010887161828577518, -0.005316589958965778, 0.004643579013645649, -0.028130127117037773, 0.011171212419867516, 0.003933771979063749, 0.01158408634364605, -0.016313957050442696, -0.03762055188417435, 0.06619134545326233, 0.002224656753242016, 0.034032803028821945, 0.01930965855717659, -0.05243026837706566, 0.03957439213991165, -0.10232618451118469, 0.009085363708436489, 0.0021365657448768616, 0.07049795240163803, -0.0836082175374031, -0.005467140581458807, -0.033628642559051514, 0.08141780644655228, 0.03290875628590584, -0.012202628888189793, -0.04485078901052475, -0.010618181899189949, -0.015132876113057137, 0.014628313481807709, -0.0764329731464386, 0.017744990065693855, -0.013072408735752106, 0.020653339102864265, -0.06585169583559036, 0.0041493303142488, -0.02753365971148014, -0.018055979162454605, 0.04837486147880554, 0.005673740990459919, -0.007140202447772026, -0.007475478574633598, -0.05333264172077179, 0.01227868627756834, 0.01803644932806492, 0.09119141101837158, 0.0051694526337087154, -0.033522944897413254, -0.07591954618692398, 0.043406032025814056, 0.03189541772007942, 0.010983224958181381, 0.05771136283874512, -0.015574664808809757, -0.019464785233139992, -0.04996412619948387, -0.003295812290161848, 0.039905447512865067, 0.004165126476436853, -0.025014135986566544, 0.02260807529091835, 0.06622485816478729, 0.020675059407949448, -0.048826687037944794, 0.049207232892513275, 0.013515393249690533, -0.03667444735765457, 0.06240428611636162, 0.009724806994199753, 0.018222400918602943, -0.036681920289993286, 0.0035749743692576885, -2.472065170877613e-05, -0.006070000119507313, 0.0008060258696787059, 0.01891500875353813, -0.036966171115636826, -0.053172044456005096, -0.020035788416862488, 0.1150297224521637, 0.035654373466968536, 0.0026488848961889744, -0.058130376040935516, -0.014332422986626625, 0.011859931983053684, 0.016271168366074562, 0.028298942372202873, 0.021968865767121315, 0.007722842041403055, -0.01385902613401413, 0.03988316282629967, 3.9116839616326615e-05, -0.0025351247750222683, 0.08477656543254852, 0.031498536467552185, -0.017735259607434273, -0.03781815245747566, 0.045436326414346695, -0.06351012736558914, -0.057871777564287186, 0.0364539809525013, -0.008794290944933891, -0.010159839875996113, -0.021504109725356102, 0.030117271468043327, 0.015309068374335766, -0.07460632920265198, -0.05739870294928551, 0.012629365548491478, 0.019796736538410187, 0.005126140080392361, -0.05368358641862869, 0.051438573747873306, 0.006828954443335533, 0.04447849839925766, -0.02488991804420948, -0.011661938391625881, -0.032514847815036774, -0.029965579509735107, 0.03687511011958122, 0.01939939148724079, 0.0016495048766955733, 0.016188692301511765, -0.03357016667723656, -0.02696525678038597, -0.05312284454703331, -0.024083303287625313, 0.10515855997800827, 0.008069893345236778, -0.011983047239482403, -0.01432171929627657, 0.07932047545909882, -5.9457801398821175e-05, -0.00019807895296253264, -0.015955636277794838, -0.02105611003935337, 0.00025665920111350715, -0.0058773052878677845, 0.03605547919869423, 0.009634310379624367, -0.0164776723831892, -0.016896896064281464, 0.032159071415662766, 0.00399109348654747, -0.0018899290589615703, -0.028479641303420067, -0.04140240699052811, -0.005324459634721279, 0.022574804723262787, -0.04691263288259506, 0.06041550636291504, 0.04613359645009041, -0.001667878939770162, -0.038829255849123, 0.029244352132081985, -0.008858142420649529, 0.021693481132388115, -0.007845992222428322, 0.018864456564188004, 0.003162527456879616, -0.004105889704078436, -0.002584723522886634, 0.014113005250692368, -0.00044598380918614566, -0.017674459144473076, -0.016167238354682922, 0.008146670646965504, 0.027480080723762512, -0.10897155851125717, 0.013416705653071404, 0.028197145089507103, -0.018135201185941696, -0.02525286003947258, 0.01165241003036499, -0.02389998733997345, -0.00692964531481266, 0.022432440891861916, 0.038168564438819885, -0.01914818584918976, -0.0162526685744524, 0.00565746147185564, 0.020519977435469627, -0.04701971262693405, 0.01984441466629505, -0.016215801239013672, 0.021773723885416985, 0.06796032190322876, -0.027175765484571457, -0.024373266845941544, 0.053822990506887436, 0.006426108069717884, -0.04793812707066536, -0.022680552676320076, -0.05454593524336815, 0.02324157953262329, 0.012374947778880596, 0.026176370680332184, -0.018831199035048485, -0.05094917491078377, -0.0509520098567009, 0.03679024055600166, 0.024034488946199417, -0.005607139319181442, -0.07101580500602722, 0.017785876989364624, 0.026454873383045197, -0.07476566731929779, 0.11100190132856369, 0.02227628231048584, 0.04760637506842613, 0.02109602652490139, 0.04055529460310936, 0.018639227375388145, 0.03526398167014122, -0.036875054240226746, -0.031173260882496834, -0.031085478141903877, 0.023703256621956825, -0.0011822665110230446, 0.10594889521598816, -0.03968051075935364, 0.03631910681724548, -0.08383599668741226, -0.02336980029940605, 0.023047221824526787, -0.07683701068162918, -0.025848524644970894, -0.009802151471376419, -0.05854182317852974, -0.038786210119724274, -0.023907262831926346, 0.006154133938252926, 0.032326843589544296, 0.026844171807169914, -0.0041599576361477375, -0.007257401943206787, 0.025981267914175987, 0.04846193641424179, 0.02990940771996975, 0.029361773282289505, 0.0195161122828722, -0.01679718680679798, -0.0012053773971274495, -0.008481482975184917, 0.0042646704241633415, -0.004106059204787016, 0.005063704214990139, -0.042657189071178436, -0.022055648267269135, -0.028253519907593727, -0.02613145112991333, -0.017070626839995384, -0.035275403410196304, 0.01982554793357849, 0.0021601980552077293, 0.027627011761069298, 0.010408611036837101, -0.030160313472151756, -0.0323440283536911, -0.04158557951450348, 0.047116804867982864, 0.0464789979159832, -0.02211504802107811, -0.019097479060292244, -0.060052528977394104, 0.017390906810760498, 0.02369462139904499, -0.0023689917288720608, 0.009779898449778557, -0.002280487446114421, 0.09936147928237915, 0.009572785347700119, 0.04109109938144684, 0.026549674570560455, -0.08886291831731796, 0.030807791277766228, -0.008533257059752941, -0.033162038773298264, -0.009317521937191486, 0.04562268778681755, 0.07058203220367432, -0.039037272334098816, -0.018585113808512688, -0.02971394918859005, -0.01780976913869381, -0.07657256722450256, -0.015251646749675274, 0.038586702197790146, -0.024149462580680847, 0.00780102564021945, 0.025281686335802078, 0.011778956279158592, -0.012006732635200024, -0.015430825762450695, -0.012821047566831112, 0.005622221622616053, 0.014279562048614025, 0.0470377579331398, -0.06002185493707657, 0.04245524853467941, 0.010697613470256329, -0.0513700433075428, -0.008469336666166782, -0.02708645910024643, -0.06894215196371078, -0.022774100303649902, 0.015436516143381596, 0.06344173848628998, 0.01954035647213459, -0.03199189901351929, 0.05615780130028725, 0.09920667856931686, 0.020942939445376396, 0.018526924774050713, -0.04553375020623207, 0.017682982608675957, 0.0749039500951767, 0.012370828539133072, -0.04729555547237396, -0.03381822258234024, 0.0066149672493338585, -0.012034127488732338, 0.05421369895339012, 0.02465994283556938, 0.06365938484668732, 0.0018041259609162807, 0.00776220066472888, 0.019091999158263206, 0.02864278107881546, 0.0003132256679236889, 0.02580263651907444, 0.005652561318129301, 0.009796014055609703, -0.02819833904504776, 0.010962745174765587, 0.009528456255793571, -0.018082115799188614, -0.00912396889179945, 0.03260532766580582, -0.007546140346676111, -0.00035060729715041816, -0.027213381603360176, -0.0025151625741273165, 0.016650531440973282, -0.07839619368314743, -0.00036451820051297545, -0.023904521018266678, 0.012327495962381363, -0.0019774427637457848, -0.03424171358346939, 0.016187794506549835, -0.028801480308175087, 0.050085216760635376, 0.024152614176273346, -0.07192843407392502, 5.762890577898361e-05, 0.03767707198858261, -0.025118131190538406, -0.027113959193229675, 0.0341043584048748, 0.03138069808483124, -0.06476237624883652, 0.015771297737956047, 0.0470844991505146, -0.04999923333525658, 0.004105850122869015, 0.009255259297788143, -0.05041728541254997, -0.010157025419175625, 0.06114315986633301, 0.07021080702543259, 0.012844323180615902, 0.02908387780189514, 0.013920415192842484, 0.006303579080849886, 0.025317978113889694, -0.022441139444708824, 0.03408277779817581, 0.017529791221022606, 0.04358888417482376, -0.005863285157829523, -0.028656573966145515, 0.008898249827325344, -0.0337032824754715, 0.007334826979786158, -0.006006528623402119, -0.01622907817363739, -0.014975369907915592, 0.01740187592804432, 0.012798752635717392, -0.06878560781478882, 0.09026558697223663, -0.020538505166769028, -0.0026968768797814846, 0.01621745154261589, 0.05484765022993088, 0.00861476082354784, 0.06153364107012749, -0.06021500751376152, -0.0224087443202734, 0.038256119936704636, -0.013431557454168797, 0.04053766280412674, -0.006399449892342091, 0.0760418102145195, -0.02795976772904396, 0.04012872278690338, 0.026525840163230896, 0.02702338993549347, 0.026523537933826447, -0.04704398661851883, 0.0078496178612113, 0.04134733974933624, 0.025786636397242546, -0.022265490144491196, 0.060580067336559296, 0.014529971405863762, 0.03114158846437931, 0.029564758762717247, -0.0117331026121974, -0.02108226902782917, 0.03427819162607193, -0.044012464582920074, -0.0023358892649412155, -0.017554625868797302, 0.01765485107898712, 0.09024489670991898, -0.03393560275435448, -0.01882350631058216, -0.038861602544784546, -0.0031574685126543045, -0.0012686534319072962, -0.04160666838288307, -0.023707572370767593, 0.04097782447934151, 0.039119526743888855, 0.032511502504348755, 0.017027366906404495, -0.008181542158126831, 0.02035454846918583, 0.018383627757430077, 0.05983254685997963, -0.0072099496610462666, 0.055321477353572845, -0.006369448266923428, 0.0011996692046523094, 0.02731073647737503, 0.07057654857635498, -0.026284826919436455, -0.06146840378642082, 0.027109431102871895, -0.009429216384887695, -0.007893284782767296, -0.0015330553287640214, 0.05360295623540878, -0.022540152072906494, 0.0558948740363121, -0.013229436241090298, 0.016685474663972855, -0.04785991832613945, 0.004739012103527784, -0.05629829689860344, 0.010831945575773716, 0.01786247082054615, 0.0023743989877402782, -5.865060003134693e-33, 0.010882952250540257, 0.025304315611720085, 0.036823805421590805, -0.027444934472441673, -0.009871267713606358, 0.002269600983709097, -0.016942692920565605, -0.047260452061891556, -0.01056432444602251, -0.017114076763391495, 0.00935334898531437, 0.005554105620831251, 0.018696649000048637, 0.005224625580012798, 0.005956449545919895, -0.0312805250287056, 0.03509821370244026, -0.08075621724128723, 0.006983826868236065, 0.002786774653941393, 0.012817704118788242, 0.0369817353785038, 0.010776807554066181, 0.0021955715492367744, -0.053627077490091324, 0.04673133045434952, -0.01629154570400715, 0.005406930577009916, -0.03555894270539284, 0.015841247513890266, -0.018737729638814926, -0.06673423200845718, -0.05109792575240135, -0.08118551224470139, 0.002689316403120756, -0.07230424135923386, -0.05731797590851784, -0.006256321910768747, -0.06621680408716202, 0.004134682007133961, -0.036125924438238144, -0.025610165670514107, 0.024801582098007202, 0.004806437063962221, -0.0765233039855957, -0.004193935543298721, 0.006741059012711048, -0.009253556840121746, -0.023946838453412056, -0.010166522115468979, -0.016389628872275352, 0.00762803852558136, 0.016253018751740456, -0.036205705255270004, 0.06445478647947311, -0.03920908644795418, -0.03134435787796974, 0.003940380644053221, -0.02053113654255867, 0.01830916479229927, -0.020517293363809586, 0.023913556709885597, -0.005061169620603323, 0.05848086252808571, 0.07964757084846497, 0.046028975397348404, 0.08651473373174667, 0.0105440029874444, -0.02678825333714485, -0.009080975316464901, -0.012257068417966366, -0.029816895723342896, 0.04569357633590698, -0.03452707827091217, 0.06914062798023224, -0.06465469300746918, 0.005152925383299589, -0.005226386245340109, 0.05311793088912964, 0.06260991841554642, 0.0011760428315028548, 0.038884807378053665, -0.01699705608189106, 0.010881680063903332, 0.0015452636871486902, -0.03864559903740883, 0.02013782039284706, -0.04122377932071686, 0.01762169413268566, 0.009630712680518627, 0.055834922939538956, -0.012827908620238304, -0.0022198574151843786, -0.024707667529582977, 0.10763809084892273, -0.018216164782643318, 0.00609950814396143, 0.0566706657409668, 0.03666697070002556, -0.021425927057862282, -0.018857359886169434, -0.014257186092436314, 0.010102595202624798, -0.010242201387882233, -0.02049395814538002, 0.016535161063075066, -0.027284210547804832, 0.09259821474552155, 0.009738985449075699, 0.012892636470496655, -0.03325243294239044, 0.006654422730207443, 0.07203589379787445, -0.0701516792178154, 0.01985466107726097, -0.026330377906560898, -0.004880486521869898, 0.014214538969099522, -0.009861046448349953, 0.018420403823256493, 0.004594285972416401, 0.0026650256477296352, -0.026257866993546486, 0.06397799402475357, -0.023515164852142334, -0.004479159135371447, -0.07959984242916107, 0.042033229023218155, -0.05544562637805939, -0.07462675869464874, -0.03027927875518799, 0.028224246576428413, 2.8759879455719783e-07, -0.0953870564699173, 0.0018883391749113798, 0.049735791981220245, -0.0014363793889060616, 0.018657812848687172, -0.004131076391786337, -0.04339029639959335, -0.01473852526396513, 0.0043892282992601395, -0.025623686611652374, 0.0287588182836771, 0.03789356350898743, 0.01803596317768097, -0.02298400178551674, -0.03684328868985176, -0.03956323489546776, 0.003782762913033366, 0.013838502578437328, 0.011891299858689308, -0.017917541787028313, -0.008415269665420055, 0.05958577245473862, 0.0474528893828392, -0.0013760020956397057, 0.0679466649889946, -0.02409602887928486, 0.014073386788368225, 0.03637554123997688, 0.03927953168749809, -0.031468894332647324, -0.009554577991366386, 0.016153814271092415, -0.03655559569597244, 0.0001101548841688782, -0.024093056097626686, -0.0033192806877195835, -0.0324658527970314, 0.02923036739230156, -0.029639575630426407, -0.04143446311354637, 0.014064846560359001, 0.0686643198132515, 0.031539831310510635, 0.005672719329595566, 0.01767086423933506, -0.010192624293267727, -0.029069282114505768, 0.011912360787391663, -0.019129032269120216, 0.002694298978894949, -0.01392381638288498, -0.0180291086435318, 0.023736143484711647, -0.027026813477277756, -0.05875283107161522, 0.023210076615214348, -0.009812263771891594, -0.04055442661046982, 0.04585861414670944, 0.018193187192082405, -0.010272880084812641, -0.03596477583050728, -0.08807055652141571, 0.0022973923478275537, 0.04438474029302597, -0.03265136107802391, -0.04845475032925606, 2.288366076814441e-34, 0.009816333651542664, -0.058278441429138184, 0.01801229827105999, -0.02017729915678501, -0.024692673236131668, 0.013355079106986523, 0.029652299359440804, 0.01334469486027956, 0.021896112710237503, -0.014139614999294281, -0.034570664167404175]], node_map={'3f4a67f5-5bc7-4af1-8a9e-4e5e024876de': Node(page_content='Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 0, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3f4a67f5-5bc7-4af1-8a9e-4e5e024876de', wins=21), 'bec2012e-c935-4494-84e6-4c83a1304cb3': Node(page_content='Estimates for the optimal action values can be learned\\nusing Q-learning (Watkins, 1989), a form of temporal dif-\\nference learning (Sutton, 1988). Most interesting problems\\nare too large to learn all action values in all states sepa-\\nrately. Instead, we can learn a parameterized value function\\nQ(s,a;θt). The standard Q-learning update for the param-\\neters after taking action Atin stateStand observing the\\nimmediate reward Rt+1and resulting state St+1is then\\nθt+1=θt+α(YQ\\nt−Q(St,At;θt))∇θtQ(St,At;θt).(1)\\nwhereαis a scalar step size and the target YQ\\ntis deﬁned as\\nYQ\\nt≡Rt+1+γmax\\naQ(St+1,a;θt). (2)\\nThis update resembles stochastic gradient descent, updating\\nthe current value Q(St,At;θt)towards a target value YQ\\nt.\\nDeep Q Networks\\nA deep Q network (DQN) is a multi-layered neural network\\nthat for a given state soutputs a vector of action values\\nQ(s,·;θ), where θare the parameters of the network. For\\nann-dimensional state space and an action space contain-\\ningmactions, the neural network is a function from Rnto\\nRm. Two important ingredients of the DQN algorithm as\\nproposed by Mnih et al. (2015) are the use of a target net-\\nwork, and the use of experience replay. The target network,\\nwith parameters θ−, is the same as the online network ex-\\ncept that its parameters are copied every τsteps from the\\nonline network, so that then θ−\\nt=θt, and kept ﬁxed on all\\nother steps. The target used by DQN is then\\nYDQN\\nt≡Rt+1+γmax\\naQ(St+1,a;θ−\\nt). (3)\\nFor the experience replay (Lin, 1992), observed transitions\\nare stored for some time and sampled uniformly from this\\nmemory bank to update the network. Both the target network\\nand the experience replay dramatically improve the perfor-\\nmance of the algorithm (Mnih et al., 2015).\\nDouble Q-learning\\nThe max operator in standard Q-learning and DQN, in (2)\\nand (3), uses the same values both to select and to evalu-\\nate an action. This makes it more likely to select overesti-\\nmated values, resulting in overoptimistic value estimates. To\\nprevent this, we can decouple the selection from the evalua-\\ntion. This is the idea behind Double Q-learning (van Hasselt,\\n2010).\\nIn the original Double Q-learning algorithm, two value\\nfunctions are learned by assigning each experience ran-\\ndomly to update one of the two value functions, such that\\nthere are two sets of weights, θandθ′. For each update, one\\nset of weights is used to determine the greedy policy and the\\nother to determine its value. For a clear comparison, we can\\nﬁrst untangle the selection and evaluation in Q-learning and\\nrewrite its target (2) as\\nYQ\\nt=Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θt).The Double Q-learning error can then be written as\\nYDoubleQ\\nt≡Rt+1+γQ(St+1,argmax\\naQ(St+1,a;θt);θ′\\nt).\\n(4)\\nNotice that the selection of the action, in the argmax , is\\nstill due to the online weights θt. This means that, as in Q-\\nlearning, we are still estimating the value of the greedy pol-\\nicy according to the current values, as deﬁned by θt. How-\\never, we use the second set of weights θ′\\ntto fairly evaluate\\nthe value of this policy. This second set of weights can be\\nupdated symmetrically by switching the roles of θandθ′.\\nOveroptimism due to estimation errors\\nQ-learning’s overestimations were ﬁrst investigated by\\nThrun and Schwartz (1993), who showed that if the action\\nvalues contain random errors uniformly distributed in an in', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 1, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='bec2012e-c935-4494-84e6-4c83a1304cb3', wins=19), '58b9d738-ba0c-403d-8f3a-2a5f4971ba3f': Node(page_content=' this\\ncauses higher estimation errors for unseen states, resulting\\nin higher overestimations. This is important because ﬂexi-\\nble parametric function approximators are often employed\\nin reinforcement learning (see, e.g., Tesauro 1995; Sallans\\nand Hinton 2004; Riedmiller 2005; Mnih et al. 2015).\\nIn contrast to van Hasselt (2010) we did not use a sta-\\ntistical argument to ﬁnd overestimations, the process to ob-\\ntain Figure 2 is fully deterministic. In contrast to Thrun and\\nSchwartz (1993), we did not rely on inﬂexible function ap-\\nproximation with irreducible asymptotic errors; the bottom\\nrow shows that a function that is ﬂexible enough to cover all\\nsamples leads to high overestimations. This indicates that\\nthe overestimations can occur quite generally.\\nIn the examples above, overestimations occur even when\\nassuming we have samples of the true action value at cer-\\ntain states. The value estimates can further deteriorate if we\\nbootstrap off of action values that are already overoptimistic,\\nsince this causes overestimations to propagate throughout\\nour estimates. Although uniformly overestimating values\\nmight not hurt the resulting policy, in practice overestima-\\ntion errors will differ for different states and actions. Over-\\nestimation combined with bootstrapping then has the perni-\\ncious effect of propagating the wrong relative information\\nabout which states are more valuable than others, directly\\naffecting the quality of the learned policies.\\nThe overestimations should not be confused with opti-\\nmism in the face of uncertainty (Sutton, 1990; Agrawal,\\n1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and\\nTennenholtz, 2003; Szita and L ˝orincz, 2008; Strehl et al.,\\n2009), where an exploration bonus is given to states or\\nactions with uncertain values. Conversely, the overestima-\\ntions discussed here occur only after updating, resulting in\\noveroptimism in the face of apparent certainty. This was al-\\nready observed by Thrun and Schwartz (1993), who noted\\nthat, in contrast to optimism in the face of uncertainty, these\\noverestimations actually can impede learning an optimal\\npolicy. We will see this negative effect on policy quality con-\\nﬁrmed later in the experiments as well: when we reduce the\\noverestimations using Double Q-learning, the policies im-\\nprove.\\n2We arbitrarily used the samples of action ai+5(fori≤5)\\norai−5(fori > 5) as the second set of samples for the double\\nestimator of action ai.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 2, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='58b9d738-ba0c-403d-8f3a-2a5f4971ba3f', wins=16), '3a3d16f1-c204-4708-a36f-2299250ba53a': Node(page_content='DQN Double DQN Double DQN (tuned)\\nMedian 47.5% 88.4% 116.7%\\nMean 122.0% 273.1% 475.2%\\nTable 2: Summary of normalized performance up to 30 minutes\\nof play on 49 games with human starts. Results for DQN are from\\nNair et al. (2015).\\nto allow for a controlled experiment focused just on re-\\nducing overestimations. The learned policies are evaluated\\nfor 5 mins of emulator time (18,000 frames) with an ϵ-\\ngreedy policy where ϵ= 0.05. The scores are averaged over\\n100 episodes. The only difference between Double DQN\\nand DQN is the target, using YDoubleDQN\\nt rather thanYDQN.\\nThis evaluation is somewhat adversarial, as the used hyper-\\nparameters were tuned for DQN but not for Double DQN.\\nTo obtain summary statistics across games, we normalize\\nthe score for each game as follows:\\nscore normalized =score agent−score random\\nscore human−score random. (5)\\nThe ‘random’ and ‘human’ scores are the same as used by\\nMnih et al. (2015), and are given in the appendix.\\nTable 1, under no ops , shows that on the whole Double\\nDQN clearly improves over DQN. A detailed comparison\\n(in appendix) shows that there are several games in which\\nDouble DQN greatly improves upon DQN. Noteworthy ex-\\namples include Road Runner (from 233% to 617%), Asterix\\n(from 70% to 180%), Zaxxon (from 54% to 111%), and\\nDouble Dunk (from 17% to 397%).\\nThe Gorila algorithm (Nair et al., 2015), which is a mas-\\nsively distributed version of DQN, is not included in the ta-\\nble because the architecture and infrastructure is sufﬁciently\\ndifferent to make a direct comparison unclear. For complete-\\nness, we note that Gorila obtained median and mean normal-\\nized scores of 96% and 495%, respectively.\\nRobustness to Human starts\\nOne concern with the previous evaluation is that in deter-\\nministic games with a unique starting point the learner could\\npotentially learn to remember sequences of actions with-\\nout much need to generalize. While successful, the solution\\nwould not be particularly robust. By testing the agents from\\nvarious starting points, we can test whether the found so-\\nlutions generalize well, and as such provide a challenging\\ntestbed for the learned polices (Nair et al., 2015).\\nWe obtained 100 starting points sampled for each game\\nfrom a human expert’s trajectory, as proposed by Nair et al.\\n(2015). We start an evaluation episode from each of these\\nstarting points and run the emulator for up to 108,000 frames\\n(30 mins at 60Hz including the trajectory before the starting\\npoint). Each agent is only evaluated on the rewards accumu-\\nlated after the starting point.\\nFor this evaluation we include a tuned version of Double\\nDQN. Some tuning is appropriate because the hyperparame-\\nters were tuned for DQN, which is a different algorithm. For\\nthe tuned version of Double DQN, we increased the num-\\nber of frames between each two copies of the target network\\nfrom 10,000 to 30,000, to reduce overestimations further be-\\ncause immediately after each switch DQN and Double DQN\\n0% 100% 200% 300% 400% 500%1000%1500%2000%2500%5000%7500%\\nNormalized score\\nHuman\\n∗∗Solaris∗∗Private EyeGravitarVentureMontezuma’s RevengeAsteroids∗∗Pitfall∗∗Ms. PacmanAmidar∗∗Yars Revenge∗∗AlienCentipedeBowling∗∗Skiing∗∗FrostbiteChopper CommandSeaquest∗∗Berzerk∗∗H.E.R.O.TutankhamIce HockeyBattle ZoneRiver Raid∗∗Surround∗∗Q*BertTennisFishing DerbyZaxxonPongFreewayBeam RiderBank HeistTime PilotName This GameWizard of WorKung-Fu MasterEnduroJames BondSpace InvadersUp and Down∗∗Phoenix∗∗∗∗Defender∗∗AsterixKangarooCrazy ClimberKrullRoad RunnerStar GunnerBoxingGopherRobotankDouble DunkAssaultBreakoutDemon AttackAtlantisVideo', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 5, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='3a3d16f1-c204-4708-a36f-2299250ba53a', wins=17), 'abb026b9-5154-4765-b3ba-0680e0f9af45': Node(page_content='human, or even surpassing these.\\nDouble DQN appears more robust to this more challeng-\\ning evaluation, suggesting that appropriate generalizations\\noccur and that the found solutions do not exploit the deter-\\nminism of the environments. This is appealing, as it indi-\\ncates progress towards ﬁnding general solutions rather than\\na deterministic sequence of steps that would be less robust.\\nDiscussion\\nThis paper has ﬁve contributions. First, we have shown why\\nQ-learning can be overoptimistic in large-scale problems,\\neven if these are deterministic, due to the inherent estima-\\ntion errors of learning. Second, by analyzing the value es-\\ntimates on Atari games we have shown that these overesti-\\nmations are more common and severe in practice than pre-\\nviously acknowledged. Third, we have shown that Double\\nQ-learning can be used at scale to successfully reduce this\\noveroptimism, resulting in more stable and reliable learning.\\nFourth, we have proposed a speciﬁc implementation called\\nDouble DQN, that uses the existing architecture and deep\\nneural network of the DQN algorithm without requiring ad-\\nditional networks or parameters. Finally, we have shown that\\nDouble DQN ﬁnds better policies, obtaining new state-of-\\nthe-art results on the Atari 2600 domain.\\nAcknowledgments\\nWe would like to thank Tom Schaul, V olodymyr Mnih, Marc\\nBellemare, Thomas Degris, Georg Ostrovski, and Richard\\nSutton for helpful comments, and everyone at Google Deep-\\nMind for a constructive research environment.\\nReferences\\nR. Agrawal. Sample mean based index policies with O(log n) re-\\ngret for the multi-armed bandit problem. Advances in Applied\\nProbability , pages 1054–1078, 1995.\\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the\\nmultiarmed bandit problem. Machine learning , 47(2-3):235–\\n256, 2002.\\nL. Baird. Residual algorithms: Reinforcement learning with func-\\ntion approximation. In Machine Learning: Proceedings of the\\nTwelfth International Conference , pages 30–37, 1995.\\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The ar-\\ncade learning environment: An evaluation platform for general\\nagents. J. Artif. Intell. Res. (JAIR) , 47:253–279, 2013.\\nR. I. Brafman and M. Tennenholtz. R-max-a general polynomial\\ntime algorithm for near-optimal reinforcement learning. The\\nJournal of Machine Learning Research , 3:213–231, 2003.\\nK. Fukushima. Neocognitron: A hierarchical neural network ca-\\npable of visual pattern recognition. Neural networks , 1(2):119–\\n130, 1988.\\nL. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement\\nlearning: A survey. Journal of Artiﬁcial Intelligence Research ,\\n4:237–285, 1996.\\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based\\nlearning applied to document recognition. Proceedings of the\\nIEEE , 86(11):2278–2324, 1998.\\nL. Lin. Self-improving reactive agents based on reinforcement\\nlearning, planning and teaching. Machine learning , 8(3):293–\\n321, 1992.H. R. Maei. Gradient temporal-difference learning algorithms .\\nPhD thesis, University of Alberta, 2011.\\nV . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-\\nvski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,\\nD. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-\\nlevel control through deep reinforcement learning. Nature , 518\\n(7540):529–533, 2015.\\nA. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.\\nMaria, V . Panneershelvam, M. Suleyman, C. Beattie, S. Pe-', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 6, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='abb026b9-5154-4765-b3ba-0680e0f9af45', wins=22), '205d470b-7014-478d-8130-1ec0f0be88a6': Node(page_content='. Consider a state sin which all the true optimal action\\nvalues are equal at Q∗(s,a) =V∗(s). Suppose that the estimation\\nerrorsQt(s,a)−Q∗(s,a)are independently distributed uniformly\\nrandomly in [−1,1]. Then,\\nE[\\nmax\\naQt(s,a)−V∗(s)]\\n=m−1\\nm+ 1\\nProof. Deﬁneϵa=Qt(s,a)−Q∗(s,a); this is a uniform ran-\\ndom variable in [−1,1]. The probability that max aQt(s,a)≤x\\nfor somexis equal to the probability that ϵa≤xfor allasimul-\\ntaneously. Because the estimation errors are independent, we can\\nderive\\nP(max\\naϵa≤x) =P(X1≤x∧X2≤x∧...∧Xm≤x)\\n=m∏\\na=1P(ϵa≤x).\\nThe function P(ϵa≤x)is the cumulative distribution function\\n(CDF) ofϵa, which here is simply deﬁned as\\nP(ϵa≤x) =\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1\\n1+x\\n2ifx∈(−1,1)\\n1 ifx≥1\\nThis implies that\\nP(max\\naϵa≤x) =m∏\\na=1P(ϵa≤x)\\n=\\uf8f1\\n\\uf8f2\\n\\uf8f30 ifx≤−1(1+x\\n2)mifx∈(−1,1)\\n1 ifx≥1\\nThis gives us the CDF of the random variable max aϵa. Its expec-\\ntation can be written as an integral\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx,\\nwherefmaxis the probability density function of this variable, de-\\nﬁned as the derivative of the CDF: fmax(x) =d\\ndxP(max aϵa≤\\nx), so that for x∈[−1,1]we havefmax(x) =m\\n2(1+x\\n2)m−1.\\nEvaluating the integral yields\\nE[\\nmax\\naϵa]\\n=∫1\\n−1xfmax(x)dx\\n=[(x+ 1\\n2)mmx−1\\nm+ 1]1\\n−1\\n=m−1\\nm+ 1.\\nExperimental Details for the Atari 2600\\nDomain\\nWe selected the 49 games to match the list used by Mnih et al.\\n(2015), see Tables below for the full list. Each agent step is com-\\nposed of four frames (the last selected action is repeated during\\nthese frames) and reward values (obtained from the Arcade Learn-\\ning Environment (Bellemare et al., 2013)) are clipped between -1\\nand 1.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 7, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='205d470b-7014-478d-8130-1ec0f0be88a6', wins=19), '9d2b58d5-997a-4b85-81c9-50ffb321603e': Node(page_content='Network Architecture\\nThe convolution network used in the experiment is exactly the one\\nproposed by proposed by Mnih et al. (2015), we only provide de-\\ntails here for completeness. The input to the network is a 84x84x4\\ntensor containing a rescaled, and gray-scale, version of the last four\\nframes. The ﬁrst convolution layer convolves the input with 32 ﬁl-\\nters of size 8 (stride 4), the second layer has 64 layers of size 4\\n(stride 2), the ﬁnal convolution layer has 64 ﬁlters of size 3 (stride\\n1). This is followed by a fully-connected hidden layer of 512 units.\\nAll these layers are separated by Rectiﬁer Linear Units (ReLu). Fi-\\nnally, a fully-connected linear layer projects to the output of the\\nnetwork, i.e., the Q-values. The optimization employed to train the\\nnetwork is RMSProp (with momentum parameter 0.95).\\nHyper-parameters\\nIn all experiments, the discount was set to γ= 0.99, and the learn-\\ning rate toα= 0.00025 . The number of steps between target net-\\nwork updates was τ= 10,000. Training is done over 50M steps\\n(i.e., 200M frames). The agent is evaluated every 1M steps, and\\nthe best policy across these evaluations is kept as the output of the\\nlearning process. The size of the experience replay memory is 1M\\ntuples. The memory gets sampled to update the network every 4\\nsteps with minibatches of size 32. The simple exploration policy\\nused is anϵ-greedy policy with the ϵdecreasing linearly from 1 to\\n0.1over 1M steps.\\nSupplementary Results in the Atari 2600\\nDomain\\nThe Tables below provide further detailed results for our experi-\\nments in the Atari domain.', metadata={'source': 'pdf\\\\Double DQN.pdf', 'page': 8, 'filename': 'pdf\\\\Double DQN.pdf'}, doc_id='9d2b58d5-997a-4b85-81c9-50ffb321603e', wins=13)}, run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), node_filter=NodeFilter(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), threshold=7.5, context_scoring_prompt=Prompt(name='score_context', instruction='Given a context, complete the two following tasks and output answer valid json format\\n1.Evaluate the provided context and assign a numerical score between 0 and 10 based on the following criteria:\\n    - Award a high score to context that thoroughly delves into and explains concepts.\\n    - Assign a lower score to context that contains excessive references, acknowledgments, personal information, or other non-essential elements.', examples=[{'context': 'Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.', 'output': {'score': 6.0}}], input_keys=['context'], output_key='output', output_type='json', language='english')), question_filter=QuestionFilter(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=15, max_wait=90, max_workers=16, exception_types=<class 'Exception'>)), filter_question_prompt=Prompt(name='filter_question', instruction='\\nAsses the given question for clarity and answerability given enough domain knowledge, consider the following criteria:\\n1.Independence: Can the question be understood and answered without needing additional context or access to external references not provided within the question itself? Questions should be self-contained, meaning they do not rely on specific documents, tables, or prior knowledge not shared within the question.\\n2.Clear Intent: Is it clear what type of answer or information the question seeks? The question should convey its purpose without ambiguity, allowing for a direct and relevant response.\\nBased on these criteria, assign a verdict of \"1\" if a question is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. Assign \"0\" if it fails to meet one or more of these criteria due to vagueness, reliance on external references, or ambiguity in intent.\\nProvide feedback and a verdict in JSON format, including suggestions for improvement if the question is deemed unclear. Highlight aspects of the question that contribute to its clarity or lack thereof, and offer advice on how it could be reframed or detailed for better understanding and answerability.\\n', examples=[{'question': 'What is the discovery about space?', 'output': {'feedback': \"The question is too vague and broad, asking for a 'discovery about space' without specifying any particular aspect, time frame, or context of interest. This could refer to a wide range of topics, from the discovery of new celestial bodies to advancements in space travel technology. To improve clarity and answerability, the question could specify the type of discovery (e.g., astronomical, technological), the time frame (e.g., recent, historical), or the context (e.g., within a specific research study or space mission).\", 'verdict': '0'}}, {'question': \"How does ALMA-13B-R perform compared to other translation models in the WMT'23 study, based on the results in context1 and context2?\", 'output': {'feedback': \"This question asks for a comparison of the ALMA-13B-R model's performance against other translation models within the WMT'23 study, specifically referring to results in 'context1' and 'context2'. While it clearly specifies the model of interest (ALMA-13B-R) and the study (WMT'23), it assumes access to and understanding of 'context1' and 'context2' without explaining what these contexts entail. This makes the question unclear for those not familiar with the WMT'23 study or these specific contexts. To improve clarity and answerability for a broader audience, the question could benefit from defining or describing 'context1' and 'context2' or explaining the criteria used for comparison in these contexts.\", 'verdict': '0'}}, {'question': 'How do KIWI-XXL and XCOMET compare to the gold standard references in Table 1 in terms of evaluation scores, translation model performance, and success rate in surpassing the references?', 'output': {'feedback': \"The question requests a comparison between KIWI-XXL and XCOMET models and gold standard references in 'Table 1', focusing on evaluation scores, translation model performance, and success rates in surpassing the references. It specifies the models and criteria for comparison, making the intent clear. However, the question assumes access to 'Table 1' without providing its content or context, making it unclear for those without direct access to the source material. To be clearer and more answerable for a general audience, the question could include a brief description of the content or key findings of 'Table 1', or alternatively, frame the question in a way that does not rely on specific, unpublished documents.\", 'verdict': '0'}}, {'question': 'What is the configuration of UL2 training objective in OpenMoE and why is it a better choice for pre-training?', 'output': {'feedback': 'The question asks for the configuration of the UL2 training objective within the OpenMoE framework and the rationale behind its suitability for pre-training. It is clear in specifying the topic of interest (UL2 training objective, OpenMoE) and seeks detailed information on both the configuration and the reasons for its effectiveness in pre-training. However, the question might be challenging for those unfamiliar with the specific terminology or the context of OpenMoE and UL2. For broader clarity and answerability, it would be helpful if the question included a brief explanation or context about OpenMoE and the UL2 training objective, or clarified the aspects of pre-training effectiveness it refers to (e.g., efficiency, accuracy, generalization).', 'verdict': '1'}}, {'question': 'What is the detailed configuration of the UL2 training objective in OpenMoE, based on the provided context?', 'output': {'feedback': \"The question seeks detailed information on the UL2 training objective's configuration within the OpenMoE framework, mentioning 'the provided context' without actually including or describing this context within the query. This makes the question unclear for those who do not have access to the unspecified context. For the question to be clear and answerable, it needs to either include the relevant context directly within the question or be framed in a way that does not require external information. Detailing the specific aspects of the configuration of interest (e.g., loss functions, data augmentation techniques) could also help clarify the query.\", 'verdict': '0'}}], input_keys=['question'], output_key='output', output_type='json', language='english')), question_answer_prompt=Prompt(name='answer_formulate', instruction=\"Answer the question using the information from the given context. Output verdict as '1' if answer is present '-1' if answer is not present in the context.\", examples=[{'context': 'Climate change is significantly influenced by human activities, notably the emission of greenhouse gases from burning fossil fuels. The increased greenhouse gas concentration in the atmosphere traps more heat, leading to global warming and changes in weather patterns.', 'question': 'How do human activities contribute to climate change?', 'answer': {'answer': 'Human activities contribute to climate change primarily through the emission of greenhouse gases from burning fossil fuels. These emissions increase the concentration of greenhouse gases in the atmosphere, which traps more heat and leads to global warming and altered weather patterns.', 'verdict': '1'}}, {'context': 'The concept of artificial intelligence (AI) has evolved over time, but it fundamentally refers to machines designed to mimic human cognitive functions. AI can learn, reason, perceive, and, in some instances, react like humans, making it pivotal in fields ranging from healthcare to autonomous vehicles.', 'question': 'What are the key capabilities of artificial intelligence?', 'answer': {'answer': 'Artificial intelligence is designed to mimic human cognitive functions, with key capabilities including learning, reasoning, perception, and reacting to the environment in a manner similar to humans. These capabilities make AI pivotal in various fields, including healthcare and autonomous driving.', 'verdict': '1'}}, {'context': 'The novel \"Pride and Prejudice\" by Jane Austen revolves around the character Elizabeth Bennet and her family. The story is set in the 19th century in rural England and deals with issues of marriage, morality, and misconceptions.', 'question': \"What year was 'Pride and Prejudice' published?\", 'answer': {'answer': 'The answer to given question is not present in context', 'verdict': '-1'}}], input_keys=['context', 'question'], output_key='answer', output_type='json', language='english'), find_relevant_context_prompt=Prompt(name='find_relevant_context', instruction='Given a question and set of contexts, find the most relevant contexts to answer the question.', examples=[{'question': 'What is the capital of France?', 'contexts': ['1. France is a country in Western Europe. It has several cities, including Paris, Lyon, and Marseille. Paris is not only known for its cultural landmarks like the Eiffel Tower and the Louvre Museum but also as the administrative center.', '2. The capital of France is Paris. It is also the most populous city in France, with a population of over 2 million people. Paris is known for its cultural landmarks like the Eiffel Tower and the Louvre Museum.', '3. Paris is the capital of France. It is also the most populous city in France, with a population of over 2 million people. Paris is known for its cultural landmarks like the Eiffel Tower and the Louvre Museum.'], 'output': {'relevant_contexts': [1, 2]}}, {'question': 'How does caffeine affect the body and what are its common sources?', 'contexts': ['1. Caffeine is a central nervous system stimulant. It can temporarily ward off drowsiness and restore alertness. It primarily affects the brain, where it alters the function of neurotransmitters.', '2. Regular physical activity is essential for maintaining good health. It can help control weight, combat health conditions, boost energy, and promote better sleep.', '3. Common sources of caffeine include coffee, tea, cola, and energy drinks. These beverages are consumed worldwide and are known for providing a quick boost of energy.'], 'output': {'relevant_contexts': [1, 2]}}], input_keys=['question', 'contexts'], output_key='output', output_type='json', language='english'), rewrite_invalid_question_prompt=Prompt(name='rewrite_question', instruction='Given a context, question and feedback, rewrite the question to improve its clarity and answerability based on the feedback provided.', examples=[{'context': \"The Eiffel Tower was constructed using iron and was originally intended as a temporary exhibit for the 1889 World's Fair held in Paris. Despite its initial temporary purpose, the Eiffel Tower quickly became a symbol of Parisian ingenuity and an iconic landmark of the city, attracting millions of visitors each year. The tower's design, created by Gustave Eiffel, was initially met with criticism from some French artists and intellectuals, but it has since been celebrated as a masterpiece of structural engineering and architectural design.\", 'question': 'Who created the design for the Tower?', 'feedback': \"The question asks about the creator of the design for 'the Tower', but it does not specify which tower it refers to. There are many towers worldwide, and without specifying the exact tower, the question is unclear and unanswerable. To improve the question, it should include the name or a clear description of the specific tower in question.\", 'output': 'Who created the design for the Eiffel Tower?'}, {'context': \"'Exploring Zero-Shot Learning in Neural Networks' was published by Smith and Lee in 2021, focusing on the application of zero-shot learning techniques in artificial intelligence.\", 'question': 'What datasets were used for the zero-shot evaluations in this study?', 'feedback': \"The question asks about the datasets used for zero-shot evaluations in 'this study', without specifying or providing any details about the study in question. This makes the question unclear for those who do not have access to or knowledge of the specific study. To improve clarity and answerability, the question should specify the study it refers to, or provide enough context about the study for the question to be understood and answered independently.\", 'output': 'What datasets were used for the zero-shot evaluations Exploring Zero-Shot Learning in Neural Networks paper?'}], input_keys=['context', 'question', 'feedback'], output_key='output', output_type='string', language='english'), max_tries=5, is_async=True, compress_question_prompt=Prompt(name='compress_question', instruction='Rewrite the following question to make it more indirect and shorter while retaining the essence of the original question.\\n    The goal is to create a question that conveys the same meaning but in a less direct manner. The rewritten question should shorter so use abbreviation wherever possible.', examples=[{'question': 'What is the distance between the Earth and the Moon?', 'output': 'How far is the Moon from Earth?'}, {'question': 'What ingredients are required to bake a chocolate cake?', 'output': \"What's needed for a chocolate cake?\"}], input_keys=['question'], output_key='output', output_type='string', language='english'), multi_context_question_prompt=Prompt(name='multi_context_question', instruction=\"\\n    The task is to rewrite and complicate the given question in a way that answering it requires information derived from both context1 and context2. \\n    Follow the rules given below while rewriting the question.\\n        1. The rewritten question should not be very long. Use abbreviation wherever possible.\\n        2. The rewritten question must be reasonable and must be understood and responded by humans.\\n        3. The rewritten question must be fully answerable from information present in context1 and context2. \\n        4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\\n        5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\", examples=[{'question': 'What process turns plants green?', 'context1': 'Chlorophyll is the pigment that gives plants their green color and helps them photosynthesize.', 'context2': 'Photosynthesis in plants typically occurs in the leaves where chloroplasts are concentrated.', 'output': 'In which plant structures does the pigment responsible for their verdancy facilitate energy production?'}, {'question': 'How do you calculate the area of a rectangle?', 'context1': \"The area of a shape is calculated based on the shape's dimensions. For rectangles, this involves multiplying the length and width.\", 'context2': 'Rectangles have four sides with opposite sides being equal in length. They are a type of quadrilateral.', 'output': \"What multiplication involving equal opposites yields a quadrilateral's area?\"}], input_keys=['question', 'context1', 'context2'], output_key='output', output_type='string', language='english'))\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-8b-8192\")\n",
    "critic_llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-70b-8192\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import download_loader\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# SemanticScholarReader = download_loader(\"SemanticScholarReader\")\n",
    "# loader = SemanticScholarReader()\n",
    "# query_space = \"large language models\"\n",
    "# documents = loader.load_data(query=query_space, limit=100)\n",
    "\n",
    "# # generator with openai models\n",
    "# generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm,\n",
    "#     critic_llm,\n",
    "#     embeddings\n",
    "# )\n",
    "\n",
    "\n",
    "# distributions = {\n",
    "#     simple: 0.5,\n",
    "#     multi_context: 0.4,\n",
    "#     reasoning: 0.1\n",
    "# }\n",
    "\n",
    "# # generate testset\n",
    "# testset = generator.generate_with_llamaindex_docs(documents, 100,distributions)\n",
    "# testset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>Overestimations in Q-learning can negatively a...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>Overestimations in Q-learning can negatively a...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>Overestimations in Q-learning can negatively a...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>The potential problem with the Q-learning algo...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here is a question that can be fully answered ...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>The potential problem with the Q-learning algo...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here is a rewritten version of the question th...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>nan</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here is a rewritten version of the question th...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>The key limitation of Q-learning that Double Q...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Here is a rewritten version of the question th...</td>\n",
       "      <td>[Deep Reinforcement Learning with Double Q-lea...</td>\n",
       "      <td>nan</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Here is a question that can be fully answered ...   \n",
       "1  Here is a question that can be fully answered ...   \n",
       "2  Here is a question that can be fully answered ...   \n",
       "3  Here is a question that can be fully answered ...   \n",
       "4  Here is a question that can be fully answered ...   \n",
       "5  Here is a rewritten version of the question th...   \n",
       "6  Here is a rewritten version of the question th...   \n",
       "7  Here is a rewritten version of the question th...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "1  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "2  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "3  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "4  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "5  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "6  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "7  [Deep Reinforcement Learning with Double Q-lea...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Overestimations in Q-learning can negatively a...         simple   \n",
       "1  Overestimations in Q-learning can negatively a...         simple   \n",
       "2  Overestimations in Q-learning can negatively a...         simple   \n",
       "3  The potential problem with the Q-learning algo...         simple   \n",
       "4  The potential problem with the Q-learning algo...         simple   \n",
       "5                                                nan      reasoning   \n",
       "6  The key limitation of Q-learning that Double Q...      reasoning   \n",
       "7                                                nan      reasoning   \n",
       "\n",
       "   episode_done  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  \n",
       "5          True  \n",
       "6          True  \n",
       "7          True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestDataset(test_data=[DataRow(question='Here is a question that can be fully answered from the given context:\\n\\n\"What is the potential impact of overestimations in Q-learning on the quality of the resulting policy?\"\\n\\nThis question is relevant to the topic of overestimations in Q-learning and their impact on policy performance, and can be answered by referencing the text, which discusses the potential negative effects of overestimations on policy performance, as well as the benefits of reducing overestimations through the use of algorithms like Double Q-learning.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='Overestimations in Q-learning can negatively affect the quality of the resulting policy, leading to suboptimal policies, even asymptotically. However, reducing overestimations through algorithms like Double Q-learning can lead to more accurate value estimates and better policy performance.', evolution_type='simple'), DataRow(question='Here is a question that can be fully answered from the given context:\\n\\n\"What is the potential impact of overestimations in Q-learning on the quality of the resulting policy?\"\\n\\nThis question is relevant to the topic of \"Overestimations in Q-learning and their impact on policy performance\" and can be answered by referencing the text, which discusses the potential negative effects of overestimations on policy performance and provides examples of how overestimations can lead to suboptimal policies.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='Overestimations in Q-learning can negatively affect the quality of the resulting policy. If the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might lead to suboptimal policies, even asymptotically.', evolution_type='simple'), DataRow(question='Here is a question that can be fully answered from the given context:\\n\\n\"What is the potential impact of overestimations in Q-learning on the quality of the resulting policy?\"\\n\\nThis question is relevant to the topic of overestimations in Q-learning and their impact on policy performance, and can be answered by referencing the text, which discusses the potential negative effects of overestimations on policy performance, as well as the benefits of reducing overestimations through the use of algorithms like Double Q-learning.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='Overestimations in Q-learning can negatively affect the quality of the resulting policy, leading to suboptimal policies, even asymptotically. However, reducing overestimations through algorithms like Double Q-learning can lead to more accurate value estimates and better policy performance.', evolution_type='simple'), DataRow(question='Here is a question that can be fully answered from the given context:\\n\\nWhat is the potential problem with the Q-learning algorithm that the Double Q-learning algorithm aims to address?\\n\\nThis question can be answered by referring to the context, which states that the Q-learning algorithm is known to overestimate action values under certain conditions, and that the Double Q-learning algorithm is designed to reduce these overestimations.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='The potential problem with the Q-learning algorithm that the Double Q-learning algorithm aims to address is that it overestimates action values under certain conditions.', evolution_type='simple'), DataRow(question='Here is a question that can be fully answered from the given context:\\n\\nWhat is the potential problem with the Q-learning algorithm that the Double Q-learning algorithm aims to address?\\n\\nThis question can be answered by referring to the context, which states that the Q-learning algorithm is known to overestimate action values under certain conditions, and that the Double Q-learning algorithm is designed to reduce these overestimations.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='The potential problem with the Q-learning algorithm that the Double Q-learning algorithm aims to address is that it overestimates action values under certain conditions.', evolution_type='simple'), DataRow(question='Here is a rewritten version of the question that requires multi-hop reasoning:\\n\\n\"What\\'s the key limitation of Q-learning that Double Q-learning addresses, and how does this improvement impact algorithm performance?\"\\n\\nThis rewritten question is shorter and more indirect than the original, while still conveying the same meaning. It requires the reader to make multiple logical connections and inferences, including understanding the limitation of Q-learning, the purpose of Double Q-learning, and the impact of this improvement on algorithm performance.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='nan', evolution_type='reasoning'), DataRow(question='Here is a rewritten version of the question that requires multi-hop reasoning:\\n\\n\"What\\'s the key limitation of Q-learning that Double Q-learning addresses, and how does this improvement impact algorithm performance?\"\\n\\nThis rewritten question is shorter and more indirect than the original, while still conveying the same meaning. It requires the reader to make multiple logical connections and inferences, including understanding the limitation of Q-learning, the purpose of Double Q-learning, and the impact of this improvement on algorithm performance.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='The key limitation of Q-learning that Double Q-learning addresses is that Q-learning is known to overestimate action values under certain conditions. Double Q-learning reduces these overestimations, leading to more accurate value estimates and improved algorithm performance.', evolution_type='reasoning'), DataRow(question='Here is a rewritten version of the question that requires multi-hop reasoning:\\n\\n\"What are the implications of inaccurate action value estimates on policy quality, and how does this relate to the effectiveness of Double Q-learning in improving policy performance?\"\\n\\nThis rewritten question still conveys the same meaning as the original, but in a more indirect and concise manner. It requires the reader to make similar logical connections and inferences as the original question, including:\\n\\n1. Understanding the potential consequences of inaccurate action value estimates on policy quality\\n2. Recognizing the relationship between inaccurate action value estimates and the performance of the Double Q-learning algorithm\\n3. Connecting the idea of inaccurate action value estimates to the effectiveness of the Double Q-learning algorithm in improving policy performance\\n\\nBy requiring the reader to make these connections, the rewritten question encourages critical thinking and a deeper understanding of the topic.', contexts=['Deep Reinforcement Learning with Double Q-learning\\nHado van Hasselt andArthur Guez andDavid Silver\\nGoogle DeepMind\\nAbstract\\nThe popular Q-learning algorithm is known to overestimate\\naction values under certain conditions. It was not previously\\nknown whether, in practice, such overestimations are com-\\nmon, whether they harm performance, and whether they can\\ngenerally be prevented. In this paper, we answer all these\\nquestions afﬁrmatively. In particular, we ﬁrst show that the\\nrecent DQN algorithm, which combines Q-learning with a\\ndeep neural network, suffers from substantial overestimations\\nin some games in the Atari 2600 domain. We then show that\\nthe idea behind the Double Q-learning algorithm, which was\\nintroduced in a tabular setting, can be generalized to work\\nwith large-scale function approximation. We propose a spe-\\nciﬁc adaptation to the DQN algorithm and show that the re-\\nsulting algorithm not only reduces the observed overestima-\\ntions, as hypothesized, but that this also leads to much better\\nperformance on several games.\\nThe goal of reinforcement learning (Sutton and Barto, 1998)\\nis to learn good policies for sequential decision problems,\\nby optimizing a cumulative future reward signal. Q-learning\\n(Watkins, 1989) is one of the most popular reinforcement\\nlearning algorithms, but it is known to sometimes learn un-\\nrealistically high action values because it includes a maxi-\\nmization step over estimated action values, which tends to\\nprefer overestimated to underestimated values.\\nIn previous work, overestimations have been attributed\\nto insufﬁciently ﬂexible function approximation (Thrun and\\nSchwartz, 1993) and noise (van Hasselt, 2010, 2011). In\\nthis paper, we unify these views and show overestimations\\ncan occur when the action values are inaccurate, irrespective\\nof the source of approximation error. Of course, imprecise\\nvalue estimates are the norm during learning, which indi-\\ncates that overestimations may be much more common than\\npreviously appreciated.\\nIt is an open question whether, if the overestimations\\ndo occur, this negatively affects performance in practice.\\nOveroptimistic value estimates are not necessarily a prob-\\nlem in and of themselves. If all values would be uniformly\\nhigher then the relative action preferences are preserved and\\nwe would not expect the resulting policy to be any worse.\\nFurthermore, it is known that sometimes it is good to be op-\\ntimistic: optimism in the face of uncertainty is a well-known\\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al., 1996). If, however,\\nthe overestimations are not uniform and not concentrated at\\nstates about which we wish to learn more, then they might\\nnegatively affect the quality of the resulting policy. Thrun\\nand Schwartz (1993) give speciﬁc examples in which this\\nleads to suboptimal policies, even asymptotically.\\nTo test whether overestimations occur in practice and at\\nscale, we investigate the performance of the recent DQN al-\\ngorithm (Mnih et al., 2015). DQN combines Q-learning with\\na ﬂexible deep neural network and was tested on a varied\\nand large set of deterministic Atari 2600 games, reaching\\nhuman-level performance on many games. In some ways,\\nthis setting is a best-case scenario for Q-learning, because\\nthe deep neural network provides ﬂexible function approx-\\nimation with the potential for a low asymptotic approxima-\\ntion error, and the determinism of the environments prevents\\nthe harmful effects of noise. Perhaps surprisingly, we show\\nthat even in this comparatively favorable setting DQN some-\\ntimes substantially overestimates the values of the actions.\\nWe show that the idea behind the Double Q-learning algo-\\nrithm (van Hasselt, 2010), which was ﬁrst proposed in a tab-\\nular setting, can be generalized to work with arbitrary func-\\ntion approximation, including deep neural networks. We use\\nthis to construct a new algorithm we call Double DQN. We\\nthen show that this algorithm not only yields more accurate\\nvalue estimates, but leads to much higher scores on several\\ngames. This demonstrates that the overestimations of DQN\\nwere indeed leading to poorer policies and that it is beneﬁ-\\ncial to reduce them. In addition, by improving upon DQN\\nwe obtain state-of-the-art'], ground_truth='nan', evolution_type='reasoning')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586efb9bc00b4f2bb4b1e887ca41bf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\metrics\\base.py\", line 93, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\metrics\\base.py\", line 89, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 182, in _ascore\n",
      "    answer_result = await self.llm.generate(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\llms\\base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\llms\\base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 421, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 411, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 632, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1261, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1074, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
      "    return self._retry_request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1074, in _retry_request\n",
      "    return self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[0;32m     19\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m---> 21\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m score\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32mc:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\evaluation.py:231\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    229\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    234\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    235\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    236\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    237\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\evaluation.py:213\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "import nest_asyncio\n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "score = evaluate(dataset,metrics=[faithfulness,answer_correctness])\n",
    "score.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
