{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-70b-8192\", temperature=0)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_retriever():\n",
    "    loader = PyPDFLoader('pdf\\Attention-is-all-you-need.pdf')\n",
    "    # loader = WebBaseLoader(\n",
    "    #     web_paths=(\"https://www.nature.com/articles/s41467-020-16278-6\",),\n",
    "    #     bs_kwargs=dict(\n",
    "    #         parse_only=bs4.SoupStrainer(\n",
    "    #             class_=(\"c-article-title\", \"c-article-section__content\")\n",
    "    #         )\n",
    "    #     ),\n",
    "    # )\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "    \n",
    "    db = Chroma.from_documents(documents=texts_chunks, embedding=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_history_aware_retriever(llm, retriever):\n",
    "    contextualize_q_system_prompt = (\n",
    "        'Taking into account the chat history and the latest user question that may be referencing the chat history,'\n",
    "        'generate a new question that can be understood without the chat history. DO NOT answer that question,'\n",
    "        'just reformulate it if needed and otherwise return it as is.'\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    return history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(llm, history_aware_retriever):\n",
    "    system_prompt = (\n",
    "        'You are a helpful assistant that answers questions.'\n",
    "        'Use the retrieved context to answer the question.'\n",
    "        'If you do not know the answer your reply should be \"I dont know.\"'\n",
    "        'Try to keep the answers short unless otherwise specifed by the question.'\n",
    "        '\\n\\n'\n",
    "        '{context}'\n",
    "        )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    qa_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "store = {}\n",
    "\n",
    "llm = load_llm()\n",
    "retriever = prepare_retriever()\n",
    "history_aware_retriever = generate_history_aware_retriever(llm, retriever)\n",
    "rag_chain = create_qa_chain(llm, history_aware_retriever)\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': \n",
    "        [\n",
    "            'What is self-attention?', \n",
    "            'How many identical layers does the encoder of the transformer have?'\n",
    "        ],\n",
    "    'answer': \n",
    "        [],\n",
    "    'contexts' :\n",
    "        [\n",
    "            ['Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.'], \n",
    "            ['The encoder is composed of a stack of N = 6 identical layers.']\n",
    "        ],\n",
    "}\n",
    "# dataset = Dataset.from_dict(data_samples)\n",
    "# score = evaluate(dataset,metrics=[faithfulness], llm=ChatOllama(model='mistral'), embeddings=OllamaEmbeddings(model='mistral'))\n",
    "# score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in data_samples['question']:\n",
    "    answer = conversational_rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },\n",
    "    )[\"answer\"]\n",
    "    data_samples['answer'].append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['What is self-attention?',\n",
       "  'How many identical layers does the encoder of the transformer have?'],\n",
       " 'answer': ['Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.',\n",
       "  'The encoder of the transformer has 6 identical layers.'],\n",
       " 'contexts': [['Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.'],\n",
       "  ['The encoder is composed of a stack of N = 6 identical layers.']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35025c1d392460682fe4851418a5cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is self-attention?</td>\n",
       "      <td>Self-attention is an attention mechanism relat...</td>\n",
       "      <td>[Self-attention, sometimes called intra-attent...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many identical layers does the encoder of ...</td>\n",
       "      <td>The encoder of the transformer has 6 identical...</td>\n",
       "      <td>[The encoder is composed of a stack of N = 6 i...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                            What is self-attention?   \n",
       "1  How many identical layers does the encoder of ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Self-attention is an attention mechanism relat...   \n",
       "1  The encoder of the transformer has 6 identical...   \n",
       "\n",
       "                                            contexts  faithfulness  \n",
       "0  [Self-attention, sometimes called intra-attent...           1.0  \n",
       "1  [The encoder is composed of a stack of N = 6 i...           0.5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness \n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "score = evaluate(dataset,metrics=[faithfulness], llm=ChatOllama(model='mistral'), embeddings=OllamaEmbeddings(model='mistral'))\n",
    "score.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
