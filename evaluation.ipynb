{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import SeleniumURLLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-70b-8192\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pdf files\n",
    "loader = DirectoryLoader(path='pdf', glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']\n",
    "\n",
    "# load the vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the provided context to answer the user's question.\n",
    "If you don't know the answer, respond with \"I do not know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d25153b9664beea0973b5c702814d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140f09b449a74a2491de8131af8c14d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 141, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 543, in _aevolve\n",
      "    result = await self._acomplex_evolution(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 387, in _acomplex_evolution\n",
      "    is_valid_question, feedback = await self.question_filter.filter(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\filters.py\", line 86, in filter\n",
      "    results = await self.llm.generate(prompt=prompt)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\llms\\base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\irvin\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\llms\\base.py\", line 177, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 570, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 530, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 715, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 268, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 357, in create\n",
      "    return await self._post(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1711, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1427, in request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1503, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1549, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1503, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1549, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\groq\\_base_client.py\", line 1518, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01hxzv8pbfe9v91j7eqgw9b9tb` on tokens per minute (TPM): Limit 6000, Used 5475, Requested 1809. Please try again in 12.839s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(\n\u001b[0;32m     10\u001b[0m     generator_llm,\n\u001b[0;32m     11\u001b[0m     critic_llm,\n\u001b[0;32m     12\u001b[0m     embeddings\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# generate testset\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43msimple\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_context\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\generator.py:179\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[1;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd_documents(\n\u001b[0;32m    176\u001b[0m     [Document\u001b[38;5;241m.\u001b[39mfrom_langchain_document(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\irvin\\Documents\\GitHub\\support-chatbot\\venv\\lib\\site-packages\\ragas\\testset\\generator.py:274\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[1;34m(self, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    272\u001b[0m     test_data_rows \u001b[38;5;241m=\u001b[39m exec\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_data_rows:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-8b-8192\")\n",
    "critic_llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-70b-8192\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtestset\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
