{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-8b-8192\", temperature=0)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_retriever():\n",
    "    # loader = PyPDFLoader('pdf\\Attention-is-all-you-need.pdf')\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(\"https://www.nature.com/articles/s41467-020-16278-6\",),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"c-article-title\", \"c-article-section__content\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "    \n",
    "    db = Chroma.from_documents(documents=texts_chunks, embedding=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_history_aware_retriever(llm, retriever):\n",
    "    contextualize_q_system_prompt = (\n",
    "        'Taking into account the chat history and the latest user question that may be referencing the chat history,'\n",
    "        'generate a new question that can be understood without the chat history. DO NOT answer that question,'\n",
    "        'just reformulate it if needed and otherwise return it as is.'\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    return history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(llm, history_aware_retriever):\n",
    "    system_prompt = (\n",
    "        'You are a helpful assistant that answers questions.'\n",
    "        'Use the retrieved context to answer the question.'\n",
    "        'If you do not know the answer your reply should be \"I dont know.\"'\n",
    "        'Try to keep the answers short unless otherwise specifed by the question.'\n",
    "        '\\n\\n'\n",
    "        '{context}'\n",
    "        )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    qa_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "llm = load_llm()\n",
    "retriever = prepare_retriever()\n",
    "history_aware_retriever = generate_history_aware_retriever(llm, retriever)\n",
    "rag_chain = create_qa_chain(llm, history_aware_retriever)\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the article, confirmation bias refers to the tendency of the brain to selectively accumulate new information that supports one's initial decision or choice, and to discount or ignore information that contradicts it. This bias is driven by confidence in one's initial decision, and is more pronounced when individuals are highly confident in their choice.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is confirmation bias?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the article, in the current task where new evidence is always helpful, this bias against incorporating conflicting post-decision evidence is normatively maladaptive. This means that it can lead to:\\n\\n* Ignoring or discounting important information that contradicts one\\'s initial decision\\n* Making decisions that are less accurate or informed\\n* Being \"blind\" to disconfirmatory evidence\\n\\nIn other scenarios where new evidence may be distracting or actively misleading, this confirmation bias might actually be helpful. However, in situations where new evidence is important for making accurate decisions, this bias can have negative consequences.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are some negative consequences of the above fact?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the article, MEG was recorded continuously at 600 samples/second using a whole-head 273-channel axial gradiometer system (CTF Omega, VSM MedTech).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How was MEG recorded?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was \"How was MEG recorded?\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What was my previous question?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is confirmation bias?'), AIMessage(content=\"According to the article, confirmation bias refers to the tendency of the brain to selectively accumulate new information that supports one's initial decision or choice, and to discount or ignore information that contradicts it. This bias is driven by confidence in one's initial decision, and is more pronounced when individuals are highly confident in their choice.\"), HumanMessage(content='What are some negative consequences of the above fact?'), AIMessage(content='According to the article, in the current task where new evidence is always helpful, this bias against incorporating conflicting post-decision evidence is normatively maladaptive. This means that it can lead to:\\n\\n* Ignoring or discounting important information that contradicts one\\'s initial decision\\n* Making decisions that are less accurate or informed\\n* Being \"blind\" to disconfirmatory evidence\\n\\nIn other scenarios where new evidence may be distracting or actively misleading, this confirmation bias might actually be helpful. However, in situations where new evidence is important for making accurate decisions, this bias can have negative consequences.'), HumanMessage(content='How was MEG recorded?'), AIMessage(content='According to the article, MEG was recorded continuously at 600 samples/second using a whole-head 273-channel axial gradiometer system (CTF Omega, VSM MedTech).'), HumanMessage(content='What was my previous question?'), AIMessage(content='Your previous question was \"How was MEG recorded?\"')])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
