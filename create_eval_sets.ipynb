{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model):\n",
    "    llm = ChatGroq(groq_api_key=api_key, model=model, temperature=0.2)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_retriever():\n",
    "    loader = PyPDFLoader('pdf\\Attention-is-all-you-need.pdf')\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True},)\n",
    "    \n",
    "    db = Chroma.from_documents(documents=texts_chunks, embedding=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_history_aware_retriever(llm, retriever):\n",
    "    contextualize_q_system_prompt = (\n",
    "        'Taking into account the chat history and the latest user question that may be referencing the chat history,'\n",
    "        'generate a new question that can be understood without the chat history. DO NOT answer that question,'\n",
    "        'just reformulate it if needed and otherwise return it as is.'\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    return history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(llm, history_aware_retriever):\n",
    "    system_prompt = (\n",
    "        'You are a helpful assistant that answers questions.'\n",
    "        'Use the retrieved context to answer the question.'\n",
    "        'If you do not know the answer your reply should be \"I dont know.\"'\n",
    "        'Try to keep the answers short unless otherwise specifed by the question.'\n",
    "        '\\n\\n'\n",
    "        '{context}'\n",
    "        )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    qa_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_eval_set(model, data):\n",
    "\n",
    "    store = {}\n",
    "    llm = load_llm(model)\n",
    "    retriever = prepare_retriever()\n",
    "    history_aware_retriever = generate_history_aware_retriever(llm, retriever)\n",
    "    rag_chain = create_qa_chain(llm, history_aware_retriever)\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "\n",
    "    data['answer'] = []\n",
    "\n",
    "    for question in data['question']:\n",
    "        answer = conversational_rag_chain.invoke(\n",
    "            {\"input\": question},\n",
    "            config={\n",
    "                \"configurable\": {\"session_id\": model}\n",
    "            },\n",
    "        )[\"answer\"]\n",
    "        data['answer'].append(answer)\n",
    "\n",
    "    with open(f'data\\{model}.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(data, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama3-70b-8192', 'llama3-8b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it', 'gemma2-9b-it']\n",
    "\n",
    "data_samples = {\n",
    "    'question': \n",
    "        [\n",
    "            'What is the primary architectural innovation introduced in the \"Attention is All You Need\" paper?', \n",
    "            'How does the Transformer model handle the sequential nature of input data without recurrence?',\n",
    "            'What is the main advantage of the Transformer model over RNNs?',\n",
    "            'What are multi-head attention mechanisms in the Transformer model?',\n",
    "            'How does the Transformer model utilize residual connections?',\n",
    "            'What optimization method is used to train the Transformer model?',\n",
    "            'How does the self-attention mechanism work in the Transformer model?',\n",
    "            'What regularization technique is applied to the Transformer model during training?',\n",
    "            'How is the final output of the Transformer model produced?',\n",
    "            'What datasets were used to evaluate the Transformer model?'\n",
    "\n",
    "        ],\n",
    "    'answer': \n",
    "        [],\n",
    "    'contexts' :\n",
    "        [\n",
    "            ['In this work, we propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.'], \n",
    "            ['To handle the sequential nature of the data, the model adds positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks.'],\n",
    "            ['The Transformer allows for significantly more parallelization, which allows training on much more data than is possible for RNNs, and reduces the training time considerably.'],\n",
    "            ['Instead of performing a single attention function with d_model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dv and dq dimensions, respectively.'],\n",
    "            ['We employ a residual connection around each of the two sub-layers, followed by layer normalization.'],\n",
    "            ['We use the Adam optimizer with β1=0.9, β2=0.98 and ε=10−9.'],\n",
    "            ['In the self-attention mechanism, each position in the sequence attends to all positions, which allows it to draw global dependencies between input and output.'],\n",
    "            ['We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.'],\n",
    "            [\"The decoder generates the output sequence one token at a time, with each generated token being conditioned on the previously generated tokens and the encoder's output.\"],\n",
    "            ['We evaluate our models on the WMT 2014 English-to-German and English-to-French translation tasks.']\n",
    "        ],\n",
    "    'ground_truth' :\n",
    "        [\n",
    "            'The Transformer architecture, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.',\n",
    "            'The Transformer model handles the sequential nature of input data without recurrence by adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
    "            'The main advantage of the Transformer model is significantly more parallelization, allowing for training on much more data and reducing the training time considerably.',\n",
    "            'Multi-head attention mechanisms involve linearly projecting the queries, keys, and values multiple times with different learned projections to allow the model to jointly attend to information from different representation subspaces.',\n",
    "            'The Transformer model employs a residual connection around each of the two sub-layers, followed by layer normalization.',\n",
    "            'The optimization method used to train the Transformer model is the Adam optimizer with β1=0.9, β2=0.98, and ε=10−9.',\n",
    "            'Each position in the sequence attends to all positions, allowing it to draw global dependencies between input and output.',\n",
    "            'Dropout is applied to the output of each sub-layer before it is added to the sub-layer input and normalized.',\n",
    "            \"The decoder generates the output sequence one token at a time, conditioned on previously generated tokens and the encoder's output.\",\n",
    "            'The datasets used to evaluate the Transformer model were the WMT 2014 English-to-German and English-to-French translation tasks.',\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    create_model_eval_set(model, data_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
